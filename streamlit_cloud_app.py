import streamlit as st
import pandas as pd
from pathlib import Path
import zipfile
import os
from PIL import Image
import io
import base64
import json
from datetime import datetime
import logging
import numpy as np
import hashlib
import random
import requests

# OpenAI package import attempt (Streamlit Cloud compatibility)
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    openai = None
    OPENAI_AVAILABLE = False
    st.warning("‚ö†Ô∏è Cannot load OpenAI package. Running in simulation mode.")

# Page configuration
st.set_page_config(
    page_title="Manufacturing Excel VLM System - Cloud",
    page_icon="üè≠",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Logging configuration
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# LLM API configuration
GPT_OSS_API_KEY = "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"
GPT_OSS_BASE_URL = "https://api.openai.com/v1"  # Change to actual API endpoint

# Get API key from environment variables (for security)
import os
if os.getenv("OPENAI_API_KEY"):
    GPT_OSS_API_KEY = os.getenv("OPENAI_API_KEY")

class LLMIntegration:
    """LLM Model Integration Class"""
    
    def __init__(self):
        self.gpt_oss_client = None
        self.qwen3_client = None
        self.initialize_llm_clients()
    
    def initialize_llm_clients(self):
        """Initialize LLM clients"""
        try:
            # Check OpenAI package availability
            if not OPENAI_AVAILABLE:
                logger.warning("‚ö†Ô∏è Cannot use OpenAI package. Switching to simulation mode.")
                self.gpt_oss_client = None
                return
            
            # API key validation
            if not GPT_OSS_API_KEY or GPT_OSS_API_KEY.startswith("sk-or-v1-"):
                logger.warning("‚ö†Ô∏è Invalid API key format. Switching to simulation mode.")
                self.gpt_oss_client = None
                return
            
            # Initialize GPT OSS 120B client
            self.gpt_oss_client = openai.OpenAI(
                api_key=GPT_OSS_API_KEY,
                base_url=GPT_OSS_BASE_URL
            )
            
            # Simple API test
            try:
                response = self.gpt_oss_client.models.list()
                logger.info("‚úÖ GPT OSS 120B client initialization and connection test completed")
            except Exception as test_error:
                logger.warning(f"‚ö†Ô∏è API connection test failed: {test_error}")
                self.gpt_oss_client = None
                
        except Exception as e:
            logger.error(f"‚ùå GPT OSS client initialization failed: {e}")
            self.gpt_oss_client = None
    
    def analyze_image_with_gpt_oss(self, image, prompt):
        """GPT OSS 120BÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù"""
        try:
            if not self.gpt_oss_client:
                return {"error": "GPT OSS ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Í∞Ä Ï¥àÍ∏∞ÌôîÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§."}
            
            # Ïù¥ÎØ∏ÏßÄÎ•º base64Î°ú Ïù∏ÏΩîÎî©
            img_buffer = io.BytesIO()
            image.save(img_buffer, format='PNG')
            img_base64 = base64.b64encode(img_buffer.getvalue()).decode()
            
            # GPT OSS Vision API Ìò∏Ï∂ú
            response = self.gpt_oss_client.chat.completions.create(
                model="gpt-4o",  # Change to actual model name
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{img_base64}"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=1000,
                temperature=0.3
            )
            
            return {
                "success": True,
                "analysis": response.choices[0].message.content,
                "model": "GPT OSS 120B"
            }
            
        except Exception as e:
            logger.error(f"GPT OSS image analysis failed: {e}")
            return {"error": str(e)}
    
    def generate_text_with_gpt_oss(self, prompt, context=""):
        """Generate text using GPT OSS 120B"""
        try:
            if not self.gpt_oss_client:
                return {"error": "GPT OSS client not initialized."}
            
            full_prompt = f"{context}\n\n{prompt}" if context else prompt
            
            response = self.gpt_oss_client.chat.completions.create(
                model="gpt-4o",  # Change to actual model name
                messages=[
                    {"role": "user", "content": full_prompt}
                ],
                max_tokens=1000,
                temperature=0.3
            )
            
            return {
                "success": True,
                "text": response.choices[0].message.content,
                "model": "GPT OSS 120B"
            }
            
        except Exception as e:
            logger.error(f"GPT OSS text generation failed: {e}")
            return {"error": str(e)}
    
    def analyze_image_with_qwen3(self, image, prompt):
        """Analyze image using Qwen3 open source model"""
        try:
            # Qwen3 API call (change to actual endpoint)
            qwen3_url = "https://api.qwen.ai/v1/chat/completions"
            
            # Encode image to base64
            img_buffer = io.BytesIO()
            image.save(img_buffer, format='PNG')
            img_base64 = base64.b64encode(img_buffer.getvalue()).decode()
            
            headers = {
                "Authorization": f"Bearer {GPT_OSS_API_KEY}",  # Reuse API key
                "Content-Type": "application/json"
            }
            
            data = {
                "model": "qwen-vl-plus",  # Change to actual model name
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{img_base64}"
                                }
                            }
                        ]
                    }
                ],
                "max_tokens": 1000,
                "temperature": 0.3
            }
            
            response = requests.post(qwen3_url, headers=headers, json=data)
            
            if response.status_code == 200:
                result = response.json()
                return {
                    "success": True,
                    "analysis": result["choices"][0]["message"]["content"],
                    "model": "Qwen3"
                }
            else:
                return {"error": f"Qwen3 API error: {response.status_code}"}
                
        except Exception as e:
            logger.error(f"Qwen3 image analysis failed: {e}")
            return {"error": str(e)}
    
    def get_available_models(self):
        """Return list of available LLM models"""
        models = []
        
        # Check OpenAI package availability
        if OPENAI_AVAILABLE and self.gpt_oss_client:
            models.append("GPT OSS 120B")
        
        # Qwen3 is always available (API call attempt)
        models.append("Qwen3")
        
        # If no actual models available, show simulation only
        if not models:
            models.append("Simulation")
        
        return models

class CloudVLMSystem:
    def __init__(self):
        self.excel_files = []
        self.processed_data = {}
        self.extracted_images = {}
        self.vector_database = None
        self.text_chunks = []
        self.embeddings = []
        self.embedding_model = None
        
        # Auto question generation
        self.auto_questions = []
        
        # VLM image analysis
        self.image_analysis = {}
        
        # LLM integration
        self.llm_integration = LLMIntegration()
        
        self.initialize_system()
    
    def initialize_system(self):
        """Initialize system"""
        try:
            # Streamlit Cloud cannot access local files
            # Don't generate default images, wait for uploaded files
            self.extracted_images = {}  # Initialize with empty image dictionary
            
            # Initialize basic data
            self.processed_data = {
                "System Information": {
                    "type": "system",
                    "content": "Upload Excel files to process data.",
                    "features": ["File Upload", "Image Extraction", "Data Analysis"]
                }
            }
            
            return True
        except Exception as e:
            st.error(f"‚ùå Error during system initialization: {str(e)}")
            return False
    
    def extract_images_from_excel(self):
        """Extract images from Excel file (no longer used)"""
        # Streamlit Cloud cannot access local files
        # Only uploaded files can be processed
        logger.info("Cannot access local Excel files - only uploaded files can be processed")
        self.create_default_images()
    
    def extract_images_from_uploaded_file(self, uploaded_file):
        """Extract images from uploaded Excel file"""
        try:
            # Save uploaded file temporarily
            with open("temp_excel.xlsx", "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            # Open Excel file as ZIP
            with zipfile.ZipFile("temp_excel.xlsx", 'r') as zip_file:
                # Find image files
                image_files = [f for f in zip_file.namelist() if f.startswith('xl/media/')]
                
                extracted_count = 0
                for image_file in image_files:
                    try:
                        # Read image file
                        with zip_file.open(image_file) as img_file:
                            img_data = img_file.read()
                            img = Image.open(io.BytesIO(img_data))
                            
                            # Extract image name
                            img_name = os.path.basename(image_file)
                            img_name_without_ext = os.path.splitext(img_name)[0]
                            
                            # Save image
                            self.extracted_images[img_name_without_ext] = img
                            extracted_count += 1
                            
                    except Exception as e:
                        logger.error(f"Image extraction failed {image_file}: {e}")
                
                # Delete temporary file
                if os.path.exists("temp_excel.xlsx"):
                    os.remove("temp_excel.xlsx")
                
                if extracted_count > 0:
                    # Analyze images using VLM
                    logger.info(f"VLM image analysis started: {extracted_count} images")
                    self._analyze_images_with_vlm()
                
                return extracted_count
                
        except Exception as e:
            logger.error(f"Failed to extract images from uploaded Excel: {e}")
            return 0
    
    def process_uploaded_excel_data(self, uploaded_file):
        """Parse uploaded Excel file in docling style and build vector database"""
        try:
            # Save uploaded file temporarily
            with open("temp_excel.xlsx", "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            # Step 1: Parse Excel file in docling style
            parsed_data = self._parse_excel_docling_style("temp_excel.xlsx")
            
            # Step 2: Create text chunks
            self.text_chunks = self._create_text_chunks(parsed_data)
            
            # Step 3: Load embedding model and generate vectors
            self._initialize_embedding_model()
            self.embeddings = self._generate_embeddings(self.text_chunks)
            
            # Step 4: Build FAISS vector database
            self._build_vector_database()
            
            # Step 5: Generate auto questions
            self.auto_questions = self.generate_auto_questions("temp_excel.xlsx")
            
            # Step 6: Save processed data
            file_name = uploaded_file.name
            self.processed_data[file_name] = {
                "type": "excel_file",
                "content": f"Excel file: {file_name}",
                "parsed_data": parsed_data,
                "chunks_count": len(self.text_chunks),
                "vector_db_size": len(self.embeddings),
                "auto_questions_count": len(self.auto_questions),
                "file_info": {
                    "name": file_name,
                    "size": len(uploaded_file.getbuffer()),
                    "uploaded": datetime.now()
                }
            }
            
            # ÏûÑÏãú ÌååÏùº ÏÇ≠Ï†ú
            if os.path.exists("temp_excel.xlsx"):
                os.remove("temp_excel.xlsx")
            
            logger.info(f"Excel ÌååÏùº docling ÌååÏã± Î∞è Î≤°ÌÑ∞ DB Íµ¨Ï∂ï ÏôÑÎ£å: {file_name}")
            return True
            
        except Exception as e:
            logger.error(f"Excel ÌååÏùº docling ÌååÏã± Ïã§Ìå®: {e}")
            # ÏûÑÏãú ÌååÏùº Ï†ïÎ¶¨
            if os.path.exists("temp_excel.xlsx"):
                os.remove("temp_excel.xlsx")
            return False
    
    def _parse_excel_docling_style(self, excel_file_path):
        """Excel ÌååÏùºÏùÑ docling Ïä§ÌÉÄÏùºÎ°ú ÌååÏã±"""
        try:
            # Excel ÌååÏùº ÏùΩÍ∏∞
            df = pd.read_excel(excel_file_path, sheet_name=None)
            parsed_data = {
                "file_path": excel_file_path,
                "sheets": {},
                "metadata": {
                    "total_sheets": len(df),
                    "parsed_at": datetime.now().isoformat()
                }
            }
            
            for sheet_name, sheet_df in df.items():
                # ÏãúÌä∏Î≥Ñ Îç∞Ïù¥ÌÑ∞ ÌååÏã±
                sheet_data = self._parse_sheet_content(sheet_name, sheet_df)
                parsed_data["sheets"][sheet_name] = sheet_data
            
            return parsed_data
            
        except Exception as e:
            logger.error(f"Excel docling ÌååÏã± Ïã§Ìå®: {e}")
            return None
    
    def _parse_sheet_content(self, sheet_name, sheet_df):
        """ÏãúÌä∏ ÎÇ¥Ïö©ÏùÑ docling Ïä§ÌÉÄÏùºÎ°ú ÌååÏã±"""
        try:
            # Í∏∞Î≥∏ Ï†ïÎ≥¥
            sheet_info = {
                "name": sheet_name,
                "dimensions": {
                    "rows": len(sheet_df),
                    "columns": len(sheet_df.columns)
                },
                "content": {}
            }
            
            # 1. Ìó§Îçî Ï†ïÎ≥¥ Ï∂îÏ∂ú
            if len(sheet_df) > 0:
                headers = sheet_df.columns.tolist()
                sheet_info["content"]["headers"] = headers
                
                # 2. Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ Î∂ÑÏÑù
                data_types = sheet_df.dtypes.to_dict()
                sheet_info["content"]["data_types"] = {str(k): str(v) for k, v in data_types.items()}
                
                # 3. ÌÖçÏä§Ìä∏ ÎÇ¥Ïö© Ï∂îÏ∂ú (docling Ïä§ÌÉÄÏùº)
                text_content = []
                
                # Ìó§Îçî ÌÖçÏä§Ìä∏
                header_text = f"ÏãúÌä∏ '{sheet_name}'Ïùò Ïª¨Îüº: {', '.join(headers)}"
                text_content.append(header_text)
                
                # Îç∞Ïù¥ÌÑ∞ Ìñâ ÌÖçÏä§Ìä∏ (Ï≤òÏùå 10Ìñâ)
                for idx, row in sheet_df.head(10).iterrows():
                    row_text = f"Ìñâ {idx+1}: {', '.join([f'{col}={val}' for col, val in row.items() if pd.notna(val)])}"
                    text_content.append(row_text)
                
                # 4. ÌÖåÏù¥Î∏î Íµ¨Ï°∞ Î∂ÑÏÑù
                if len(sheet_df) > 0:
                    # Ïà´Ïûê Ïª¨ÎüºÍ≥º ÌÖçÏä§Ìä∏ Ïª¨Îüº Íµ¨Î∂Ñ
                    numeric_cols = sheet_df.select_dtypes(include=[np.number]).columns.tolist()
                    text_cols = sheet_df.select_dtypes(include=['object']).columns.tolist()
                    
                    sheet_info["content"]["structure"] = {
                        "numeric_columns": numeric_cols,
                        "text_columns": text_cols,
                        "total_records": len(sheet_df)
                    }
                    
                    # Ïà´Ïûê Ïª¨Îüº ÌÜµÍ≥Ñ
                    if numeric_cols:
                        numeric_stats = {}
                        for col in numeric_cols:
                            col_data = sheet_df[col].dropna()
                            if len(col_data) > 0:
                                numeric_stats[col] = {
                                    "min": float(col_data.min()),
                                    "max": float(col_data.max()),
                                    "mean": float(col_data.mean()),
                                    "count": len(col_data)
                                }
                        sheet_info["content"]["numeric_stats"] = numeric_stats
                
                sheet_info["content"]["text_content"] = text_content
            
            return sheet_info
            
        except Exception as e:
            logger.error(f"ÏãúÌä∏ ÌååÏã± Ïã§Ìå® {sheet_name}: {e}")
            return {"name": sheet_name, "error": str(e)}
    
    def _create_text_chunks(self, parsed_data):
        """ÌååÏã±Îêú Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Í≤ÄÏÉâ Í∞ÄÎä•Ìïú ÌÖçÏä§Ìä∏ Ï≤≠ÌÅ¨ ÏÉùÏÑ±"""
        chunks = []
        
        try:
            for sheet_name, sheet_data in parsed_data["sheets"].items():
                if "content" in sheet_data and "text_content" in sheet_data["content"]:
                    # ÏãúÌä∏Î≥Ñ Ï≤≠ÌÅ¨ ÏÉùÏÑ±
                    sheet_chunk = {
                        "type": "sheet_overview",
                        "sheet_name": sheet_name,
                        "content": f"ÏãúÌä∏ '{sheet_name}': {sheet_data['content']['text_content'][0]}",
                        "metadata": {
                            "rows": sheet_data["dimensions"]["rows"],
                            "columns": sheet_data["dimensions"]["columns"]
                        }
                    }
                    chunks.append(sheet_chunk)
                    
                    # ÏÉÅÏÑ∏ Îç∞Ïù¥ÌÑ∞ Ï≤≠ÌÅ¨
                    for text_line in sheet_data["content"]["text_content"][1:]:
                        data_chunk = {
                            "type": "data_row",
                            "sheet_name": sheet_name,
                            "content": text_line,
                            "metadata": {"row_type": "data"}
                        }
                        chunks.append(data_chunk)
                    
                    # Íµ¨Ï°∞ Ï†ïÎ≥¥ Ï≤≠ÌÅ¨
                    if "structure" in sheet_data["content"]:
                        structure = sheet_data["content"]["structure"]
                        structure_chunk = {
                            "type": "structure_info",
                            "sheet_name": sheet_name,
                            "content": f"ÏãúÌä∏ '{sheet_name}' Íµ¨Ï°∞: Ïà´Ïûê Ïª¨Îüº {len(structure['numeric_columns'])}, ÌÖçÏä§Ìä∏ Ïª¨Îüº {len(structure['text_columns'])}, Ï¥ù {structure['total_records']}Í∞ú Î†àÏΩîÎìú",
                            "metadata": structure
                        }
                        chunks.append(structure_chunk)
                        
                        # Ïà´Ïûê ÌÜµÍ≥Ñ Ï≤≠ÌÅ¨
                        if "numeric_stats" in sheet_data["content"]:
                            for col, stats in sheet_data["content"]["numeric_stats"].items():
                                stats_chunk = {
                                    "type": "numeric_stats",
                                    "sheet_name": sheet_name,
                                    "content": f"Ïª¨Îüº '{col}' ÌÜµÍ≥Ñ: ÏµúÏÜåÍ∞í {stats['min']}, ÏµúÎåÄÍ∞í {stats['max']}, ÌèâÍ∑† {stats['mean']}, Îç∞Ïù¥ÌÑ∞ Ïàò {stats['count']}",
                                    "metadata": {"column": col, "stats": stats}
                                }
                                chunks.append(stats_chunk)
            
            logger.info(f"Ï¥ù {len(chunks)}Í∞úÏùò ÌÖçÏä§Ìä∏ Ï≤≠ÌÅ¨ ÏÉùÏÑ± ÏôÑÎ£å")
            return chunks
            
        except Exception as e:
            logger.error(f"ÌÖçÏä§Ìä∏ Ï≤≠ÌÅ¨ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return []
    
    def _initialize_embedding_model(self):
        """Í≤ΩÎüâÌôîÎêú ÏûÑÎ≤†Îî© Î™®Îç∏ Ï¥àÍ∏∞Ìôî"""
        try:
            # Streamlit Cloud ÌôòÍ≤ΩÏóê ÎßûÍ≤å Í≤ΩÎüâÌôîÎêú Ìï¥Ïãú Í∏∞Î∞ò ÏûÑÎ≤†Îî© ÏÇ¨Ïö©
            self.embedding_model = "hash_based"
            logger.info("Í≤ΩÎüâÌôîÎêú Ìï¥Ïãú Í∏∞Î∞ò ÏûÑÎ≤†Îî© Î™®Îç∏ ÏÇ¨Ïö©")
        except Exception as e:
            logger.error(f"ÏûÑÎ≤†Îî© Î™®Îç∏ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            self.embedding_model = None
    
    def _generate_embeddings(self, text_chunks):
        """ÏôÑÏ†ÑÌûà Í≤∞Ï†ïÏ†ÅÏù∏ Ìï¥Ïãú Í∏∞Î∞ò ÏûÑÎ≤†Îî© Î≤°ÌÑ∞ ÏÉùÏÑ±"""
        try:
            embeddings = []
            for chunk in text_chunks:
                # Ìï¥Ïãú Í∏∞Î∞ò Î≤°ÌÑ∞ ÏÉùÏÑ± (64Ï∞®ÏõêÏúºÎ°ú Ï∂ïÏÜå)
                text_content = chunk["content"]
                text_hash = hashlib.md5(text_content.encode('utf-8')).hexdigest()
                
                # ÏôÑÏ†ÑÌûà Í≤∞Ï†ïÏ†ÅÏù∏ Î≤°ÌÑ∞ ÏÉùÏÑ± (random.seed ÏÇ¨Ïö©ÌïòÏßÄ ÏïäÏùå)
                vector = self._hash_to_vector(text_hash, 64)
                
                # Ï†ïÍ∑úÌôî
                vector = vector / np.linalg.norm(vector)
                embeddings.append(vector)
            
            logger.info(f"ÏôÑÏ†ÑÌûà Í≤∞Ï†ïÏ†ÅÏù∏ Ìï¥Ïãú Í∏∞Î∞ò ÏûÑÎ≤†Îî© {len(embeddings)}Í∞ú ÏÉùÏÑ± ÏôÑÎ£å")
            return embeddings
            
        except Exception as e:
            logger.error(f"ÏûÑÎ≤†Îî© ÏÉùÏÑ± Ïã§Ìå®: {e}")
            # fallback: Í≤∞Ï†ïÏ†ÅÏù∏ Î≤°ÌÑ∞
            embeddings = []
            for i, chunk in enumerate(text_chunks):
                text_hash = hashlib.md5(f"fallback_{i}".encode('utf-8')).hexdigest()
                vector = self._hash_to_vector(text_hash, 64)
                vector = vector / np.linalg.norm(vector)
                embeddings.append(vector)
            logger.info(f"Í≤∞Ï†ïÏ†ÅÏù∏ fallback Î≤°ÌÑ∞ {len(embeddings)}Í∞ú ÏÉùÏÑ± ÏôÑÎ£å")
            return embeddings
    
    def _hash_to_vector(self, text_hash, dimensions):
        """Ìï¥ÏãúÎ•º Í≤∞Ï†ïÏ†ÅÏù∏ Î≤°ÌÑ∞Î°ú Î≥ÄÌôò"""
        vector = np.zeros(dimensions, dtype='float32')
        
        # Ìï¥ÏãúÏùò Í∞Å Î¨∏ÏûêÎ•º Ïà´ÏûêÎ°ú Î≥ÄÌôòÌïòÏó¨ Î≤°ÌÑ∞ ÏÉùÏÑ±
        for i in range(dimensions):
            # Ìï¥ÏãúÏóêÏÑú ÏàúÌôòÌïòÎ©¥ÏÑú Í∞íÏùÑ Ï∂îÏ∂ú
            hash_idx = i % len(text_hash)
            char_val = ord(text_hash[hash_idx])
            
            # Î¨∏Ïûê Í∞íÏùÑ -1ÏóêÏÑú 1 ÏÇ¨Ïù¥Î°ú Ï†ïÍ∑úÌôî
            normalized_val = (char_val - 48) / 122.0  # 48(0) ~ 122(z) Î≤îÏúÑÎ•º -1~1Î°ú
            vector[i] = normalized_val
        
        return vector
    
    def _build_vector_database(self):
        """Í≤ΩÎüâÌôîÎêú Python Í∏∞Î∞ò Î≤°ÌÑ∞ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Íµ¨Ï∂ï"""
        try:
            if len(self.embeddings) == 0:
                logger.warning("ÏûÑÎ≤†Îî©Ïù¥ ÏóÜÏñ¥ Î≤°ÌÑ∞ DB Íµ¨Ï∂ï Î∂àÍ∞Ä")
                return
            
            # Î≤°ÌÑ∞ Ï∞®Ïõê ÌôïÏù∏
            vector_dim = self.embeddings[0].shape[0]
            
            # ÏàúÏàò PythonÏúºÎ°ú Î≤°ÌÑ∞ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Íµ¨Ï∂ï
            self.vector_database = {
                "vectors": np.array(self.embeddings),
                "dimension": vector_dim,
                "count": len(self.embeddings)
            }
            
            logger.info(f"Í≤ΩÎüâÌôîÎêú Python Î≤°ÌÑ∞ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Íµ¨Ï∂ï ÏôÑÎ£å: {len(self.embeddings)}Í∞ú Î≤°ÌÑ∞, {vector_dim}Ï∞®Ïõê")
            
        except Exception as e:
            logger.error(f"Î≤°ÌÑ∞ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Íµ¨Ï∂ï Ïã§Ìå®: {e}")
            self.vector_database = None
    
    def create_default_images(self):
        """Í∏∞Î≥∏ Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ± (Îçî Ïù¥ÏÉÅ ÏÇ¨Ïö©ÌïòÏßÄ ÏïäÏùå)"""
        # Í∏∞Î≥∏ Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ±ÌïòÏßÄ ÏïäÏùå
        # ExcelÏóêÏÑú Ï∂îÏ∂úÎêú Ïã§Ï†ú Ïù¥ÎØ∏ÏßÄÎßå ÏÇ¨Ïö©
        logger.info("Í∏∞Î≥∏ Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ± ÎπÑÌôúÏÑ±Ìôî - Excel Ïù¥ÎØ∏ÏßÄÎßå ÏÇ¨Ïö©")
        pass
    
    def create_quality_inspection_image(self):
        """ÌíàÏßàÍ≤ÄÏÇ¨Ìëú Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ±"""
        # 400x300 ÌÅ¨Í∏∞Ïùò Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ±
        img = Image.new('RGB', (400, 300), color='white')
        
        # Í∞ÑÎã®Ìïú ÌíàÏßàÍ≤ÄÏÇ¨Ìëú Í∑∏Î¶¨Í∏∞
        from PIL import ImageDraw, ImageFont
        
        draw = ImageDraw.Draw(img)
        
        # Ï†úÎ™©
        draw.text((20, 20), "ÌíàÏßàÍ≤ÄÏÇ¨Ìëú", fill='black')
        draw.line([(20, 50), (380, 50)], fill='black', width=2)
        
        # Í≤ÄÏÇ¨ Ìï≠Î™©Îì§
        items = [
            "1. Ïô∏Í¥Ä Í≤ÄÏÇ¨",
            "2. ÏπòÏàò Í≤ÄÏÇ¨", 
            "3. Í∏∞Îä• Í≤ÄÏÇ¨",
            "4. ÎÇ¥Íµ¨ÏÑ± Í≤ÄÏÇ¨"
        ]
        
        y_pos = 70
        for item in items:
            draw.text((30, y_pos), item, fill='blue')
            y_pos += 30
        
        # Ìï©Í≤©/Î∂àÌï©Í≤© Ï≤¥ÌÅ¨Î∞ïÏä§
        draw.text((200, 70), "‚ñ° Ìï©Í≤©", fill='green')
        draw.text((200, 100), "‚ñ° Î∂àÌï©Í≤©", fill='red')
        
        return img
    
    def create_assembly_process_image(self):
        """Ï°∞Î¶ΩÍ≥µÏ†ïÎèÑ Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ±"""
        # 400x300 ÌÅ¨Í∏∞Ïùò Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ±
        img = Image.new('RGB', (400, 300), color='lightblue')
        
        from PIL import ImageDraw
        
        draw = ImageDraw.Draw(img)
        
        # Ï†úÎ™©
        draw.text((20, 20), "Ï°∞Î¶ΩÍ≥µÏ†ïÎèÑ", fill='darkblue')
        draw.line([(20, 50), (380, 50)], fill='darkblue', width=2)
        
        # Í≥µÏ†ï ÌùêÎ¶ÑÎèÑ Í∑∏Î¶¨Í∏∞
        processes = [
            "ÏàòÏûÖÍ≤ÄÏÇ¨",
            "Ïù¥Ïò§ÎÇòÏù¥Ï†Ä",
            "DINO Í≤ÄÏÇ¨", 
            "CU+SPONGE",
            "ÎèÑÏ†Ñ TAPE",
            "Ï∂úÌïòÍ≤ÄÏÇ¨",
            "Ìè¨Ïû•"
        ]
        
        x_pos = 30
        y_pos = 80
        for i, process in enumerate(processes):
            # Î∞ïÏä§ Í∑∏Î¶¨Í∏∞
            draw.rectangle([x_pos, y_pos, x_pos+80, y_pos+40], outline='darkblue', width=2, fill='white')
            draw.text((x_pos+5, y_pos+10), process, fill='darkblue', size=8)
            
            # ÌôîÏÇ¥Ìëú Í∑∏Î¶¨Í∏∞ (ÎßàÏßÄÎßâ Ï†úÏô∏)
            if i < len(processes) - 1:
                draw.line([x_pos+80, y_pos+20, x_pos+100, y_pos+20], fill='darkblue', width=2)
                # ÌôîÏÇ¥Ìëú Î®∏Î¶¨
                draw.polygon([(x_pos+100, y_pos+15), (x_pos+100, y_pos+25), (x_pos+110, y_pos+20)], fill='darkblue')
            
            x_pos += 100
            
            # Îëê Î≤àÏß∏ Ï§ÑÎ°ú ÎÑòÏñ¥Í∞ÄÍ∏∞
            if x_pos > 350:
                x_pos = 30
                y_pos += 80
        
        return img
    
    def create_part_drawing_image(self):
        """Î∂ÄÌíàÎèÑÎ©¥ Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ±"""
        # 400x300 ÌÅ¨Í∏∞Ïùò Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ±
        img = Image.new('RGB', (400, 300), color='lightgreen')
        
        from PIL import ImageDraw
        
        draw = ImageDraw.Draw(img)
        
        # Ï†úÎ™©
        draw.text((20, 20), "Î∂ÄÌíàÎèÑÎ©¥ - FRONT DECO SUB", fill='darkgreen')
        draw.line([(20, 50), (380, 50)], fill='darkgreen', width=2)
        
        # Í∞ÑÎã®Ìïú ÎèÑÎ©¥ Í∑∏Î¶¨Í∏∞
        # Ïô∏Í≥ΩÏÑ†
        draw.rectangle([50, 80, 350, 250], outline='darkgreen', width=3)
        
        # ÎÇ¥Î∂Ä Íµ¨Ï°∞
        draw.rectangle([70, 100, 150, 180], outline='darkgreen', width=2, fill='white')
        draw.text((80, 120), "GATE", fill='darkgreen')
        
        draw.rectangle([170, 100, 250, 180], outline='darkgreen', width=2, fill='white')
        draw.text((180, 120), "SPONGE", fill='darkgreen')
        
        draw.rectangle([270, 100, 330, 180], outline='darkgreen', width=2, fill='white')
        draw.text((280, 120), "TAPE", fill='darkgreen')
        
        # ÏπòÏàòÏÑ†
        draw.line([50, 260, 350, 260], fill='darkgreen', width=1)
        draw.text((200, 270), "300mm", fill='darkgreen')
        
        draw.line([370, 80, 370, 250], fill='darkgreen', width=1)
        draw.text((380, 165), "170mm", fill='darkgreen')
        
        return img
    
    def process_real_excel_data(self):
        """Ïã§Ï†ú Excel ÌååÏùº ÎÇ¥Ïö© Í∏∞Î∞ò Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ (Îçî Ïù¥ÏÉÅ ÏÇ¨Ïö©ÌïòÏßÄ ÏïäÏùå)"""
        # Streamlit CloudÏóêÏÑúÎäî Î°úÏª¨ ÌååÏùº Ï†ëÍ∑º Î∂àÍ∞Ä
        # ÏóÖÎ°úÎìúÎêú ÌååÏùºÎßå Ï≤òÎ¶¨ Í∞ÄÎä•
        logger.info("Î°úÏª¨ Excel ÌååÏùº Ï†ëÍ∑º Î∂àÍ∞Ä - ÏóÖÎ°úÎìúÎêú ÌååÏùºÎßå Ï≤òÎ¶¨ Í∞ÄÎä•")
        self.processed_data = {
            "ÏãúÏä§ÌÖú Ï†ïÎ≥¥": {
                "type": "system",
                "content": "Excel ÌååÏùºÏùÑ ÏóÖÎ°úÎìúÌïòÏó¨ Îç∞Ïù¥ÌÑ∞Î•º Ï≤òÎ¶¨Ìï† Ïàò ÏûàÏäµÎãàÎã§.",
                "features": ["ÌååÏùº ÏóÖÎ°úÎìú", "Ïù¥ÎØ∏ÏßÄ Ï∂îÏ∂ú", "Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù"]
            }
        }
    
    def query_system(self, query):
        """Í∞úÏÑ†Îêú ÏøºÎ¶¨ Ï≤òÎ¶¨ ÏãúÏä§ÌÖú"""
        query_lower = query.lower()
        
        # 1. Ïù¥ÎØ∏ÏßÄ Í¥ÄÎ†® ÏßàÎ¨∏ (ÏµúÏö∞ÏÑ†)
        if any(keyword in query_lower for keyword in ["Ïù¥ÎØ∏ÏßÄ", "ÏÇ¨ÏßÑ", "Í∑∏Î¶º", "Î≥¥Ïó¨", "Ï∂úÎ†•", "Ï°∞Î¶ΩÎèÑ", "ÎèÑÎ©¥"]):
            image_result = self.get_image_data(query)
            if image_result and image_result.get("type") != "no_image":
                return image_result
        
        # 2. Excel ÌååÏùº Ï†ïÎ≥¥ ÏöîÏ≤≠
        if "ÌååÏùº Ï†ïÎ≥¥" in query_lower or "excel ÌååÏùº" in query_lower:
            return self.get_excel_file_info()
        
        # 3. Î≤°ÌÑ∞ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í≤ÄÏÉâ (AI Í∏∞Î∞ò)
        if self.vector_database is not None and len(self.text_chunks) > 0:
            vector_results = self._vector_search_query(query)
            if vector_results:
                return vector_results
        
        # 4. Excel Îç∞Ïù¥ÌÑ∞ ÏßÅÏ†ë Í≤ÄÏÉâ (fallback)
        excel_results = self._search_excel_data(query)
        if excel_results:
            return excel_results
        
        # 5. ÏùºÎ∞òÏ†ÅÏù∏ ÏùëÎãµ
        return self.get_general_response(query)
    
    def _search_excel_data(self, query):
        """Excel Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Í≤ÄÏÉâ (fallback)"""
        try:
            query_lower = query.lower()
            results = []
            
            for file_name, file_data in self.processed_data.items():
                if file_data.get("type") == "excel_file":
                    # ÌååÏã±Îêú Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Í≤ÄÏÉâ
                    if "parsed_data" in file_data:
                        parsed_data = file_data["parsed_data"]
                        for sheet_name, sheet_data in parsed_data.get("sheets", {}).items():
                            if "content" in sheet_data and "text_content" in sheet_data["content"]:
                                for text_line in sheet_data["content"]["text_content"]:
                                    if query_lower in text_line.lower():
                                        results.append({
                                            "file": file_name,
                                            "sheet": sheet_name,
                                            "content": text_line,
                                            "type": "text_match"
                                        })
                    
                    # Í∏∞Ï°¥ ÏãúÌä∏ Ï†ïÎ≥¥ÏóêÏÑú Í≤ÄÏÉâ (fallback)
                    for sheet_name, sheet_info in file_data.get("sheets", {}).items():
                        # ÏÉòÌîå Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Í≤ÄÏÉâ
                        for row_data in sheet_info.get("sample_data", []):
                            for key, value in row_data.items():
                                if query_lower in str(value).lower():
                                    results.append({
                                        "file": file_name,
                                        "sheet": sheet_name,
                                        "data": row_data,
                                        "match": f"{key}: {value}",
                                        "type": "data_match"
                                    })
            
            if results:
                # Í≤∞Í≥ºÎ•º DataFrameÏúºÎ°ú Î≥ÄÌôò
                df_data = []
                for result in results:
                    if result.get("type") == "text_match":
                        df_data.append({
                            "ÌååÏùºÎ™Ö": result["file"],
                            "ÏãúÌä∏Î™Ö": result["sheet"],
                            "Îß§Ïπ≠ Ïú†Ìòï": "ÌÖçÏä§Ìä∏ Îß§Ïπ≠",
                            "ÎÇ¥Ïö©": result["content"][:100] + "..." if len(result["content"]) > 100 else result["content"]
                        })
                    else:
                        df_data.append({
                            "ÌååÏùºÎ™Ö": result["file"],
                            "ÏãúÌä∏Î™Ö": result["sheet"],
                            "Îß§Ïπ≠ Ïú†Ìòï": "Îç∞Ïù¥ÌÑ∞ Îß§Ïπ≠",
                            "Îß§Ïπ≠ Îç∞Ïù¥ÌÑ∞": result["match"],
                            "Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞": str(result["data"])[:100] + "..." if len(str(result["data"])) > 100 else str(result["data"])
                        })
                
                df = pd.DataFrame(df_data)
                
                return {
                    "type": "excel_search",
                    "title": f"üîç '{query}' Í≤ÄÏÉâ Í≤∞Í≥º (fallback)",
                    "data": df,
                    "summary": f"Ï¥ù {len(results)}Í∞ú Í≤∞Í≥º Î∞úÍ≤¨",
                    "chart_type": "table"
                }
            
            return None
            
        except Exception as e:
            logger.error(f"Excel Îç∞Ïù¥ÌÑ∞ Í≤ÄÏÉâ Ïã§Ìå®: {e}")
            return None
    
    def _vector_search_query(self, query):
        """ÏôÑÏ†ÑÌûà Í≤∞Ï†ïÏ†ÅÏù∏ Î≤°ÌÑ∞ Í≤ÄÏÉâÏùÑ ÌÜµÌïú ÏøºÎ¶¨ Ï≤òÎ¶¨"""
        try:
            if self.vector_database is None or len(self.text_chunks) == 0:
                return None
            
            # ÏøºÎ¶¨ ÌÖçÏä§Ìä∏Î•º ÏûÑÎ≤†Îî© Î≤°ÌÑ∞Î°ú Î≥ÄÌôò (Í≤∞Ï†ïÏ†Å)
            query_hash = hashlib.md5(query.encode('utf-8')).hexdigest()
            query_vector = self._hash_to_vector(query_hash, 64)
            query_vector = query_vector / np.linalg.norm(query_vector)
            
            # ÏàúÏàò PythonÏúºÎ°ú Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞ (ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑ)
            similarities = []
            vectors = self.vector_database["vectors"]
            
            for i, vector in enumerate(vectors):
                # ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
                similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))
                similarities.append((similarity, i))
            
            # Ïú†ÏÇ¨ÎèÑ ÏàúÏúºÎ°ú Ï†ïÎ†¨ (ÏÉÅÏúÑ 5Í∞ú)
            similarities.sort(reverse=True)
            k = min(5, len(self.text_chunks))
            
            # Í≤ÄÏÉâ Í≤∞Í≥º Ï†ïÎ¶¨
            search_results = []
            for i, (similarity, idx) in enumerate(similarities[:k]):
                if idx < len(self.text_chunks):
                    chunk = self.text_chunks[idx]
                    search_results.append({
                        "rank": i + 1,
                        "similarity": float(similarity),
                        "content": chunk["content"],
                        "type": chunk["type"],
                        "sheet_name": chunk.get("sheet_name", "N/A"),
                        "metadata": chunk.get("metadata", {})
                    })
            
            if search_results:
                # Í≤∞Í≥ºÎ•º DataFrameÏúºÎ°ú Î≥ÄÌôò
                df_data = []
                for result in search_results:
                    df_data.append({
                        "ÏàúÏúÑ": result["rank"],
                        "Ïú†ÏÇ¨ÎèÑ": f"{result['similarity']:.3f}",
                        "ÏãúÌä∏": result["sheet_name"],
                        "Ïú†Ìòï": result["type"],
                        "ÎÇ¥Ïö©": result["content"][:100] + "..." if len(result["content"]) > 100 else result["content"]
                    })
                
                df = pd.DataFrame(df_data)
                
                return {
                    "type": "vector_search",
                    "title": f"üîç AI Î≤°ÌÑ∞ Í≤ÄÏÉâ Í≤∞Í≥º: '{query}'",
                    "data": df,
                    "summary": f"AI Î≤°ÌÑ∞ Í≤ÄÏÉâÏúºÎ°ú {len(search_results)}Í∞ú Í≤∞Í≥º Î∞úÍ≤¨ (Ïú†ÏÇ¨ÎèÑ Í∏∞Î∞ò)",
                    "chart_type": "table",
                    "raw_results": search_results
                }
            
            return None
            
        except Exception as e:
            logger.error(f"Î≤°ÌÑ∞ Í≤ÄÏÉâ Ïã§Ìå®: {e}")
            return None
    
    def get_excel_file_info(self):
        """Excel ÌååÏùº Ï†ïÎ≥¥ Î∞òÌôò"""
        try:
            file_info = []
            for file_name, file_data in self.processed_data.items():
                if file_data.get("type") == "excel_file":
                    # ÌååÏã±Îêú Îç∞Ïù¥ÌÑ∞ Ï†ïÎ≥¥
                    parsed_info = file_data.get("parsed_data", {})
                    sheets_info = parsed_info.get("sheets", {})
                    
                    info = {
                        "ÌååÏùºÎ™Ö": file_name,
                        "ÏãúÌä∏ Ïàò": len(sheets_info),
                        "ÌÖçÏä§Ìä∏ Ï≤≠ÌÅ¨ Ïàò": file_data.get("chunks_count", 0),
                        "Î≤°ÌÑ∞ DB ÌÅ¨Í∏∞": file_data.get("vector_db_size", 0),
                        "ÌååÏùº ÌÅ¨Í∏∞": f"{file_data.get('file_info', {}).get('size', 0) / 1024:.1f} KB",
                        "ÏóÖÎ°úÎìú ÏãúÍ∞Ñ": str(file_data.get('file_info', {}).get('uploaded', 'N/A'))
                    }
                    file_info.append(info)
            
            if file_info:
                df = pd.DataFrame(file_info)
                return {
                    "type": "file_info",
                    "title": "üìÅ Excel ÌååÏùº Ï†ïÎ≥¥ (Î≤°ÌÑ∞ DB Ìè¨Ìï®)",
                    "data": df,
                    "summary": f"Ï¥ù {len(file_info)}Í∞ú Excel ÌååÏùº, Î≤°ÌÑ∞ Í≤ÄÏÉâ Í∞ÄÎä•",
                    "chart_type": "table"
                }
            else:
                return {
                    "type": "no_files",
                    "title": "üìÅ Excel ÌååÏùº ÏóÜÏùå",
                    "content": "Ï≤òÎ¶¨Îêú Excel ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§. ÌååÏùºÏùÑ ÏóÖÎ°úÎìúÌï¥Ï£ºÏÑ∏Ïöî."
                }
                
        except Exception as e:
            logger.error(f"ÌååÏùº Ï†ïÎ≥¥ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return {
                "type": "error",
                "title": "‚ùå Ïò§Î•ò",
                "content": f"ÌååÏùº Ï†ïÎ≥¥Î•º Í∞ÄÏ†∏Ïò§Îäî Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}"
            }
    
    def get_image_data(self, query):
        """Í∞ÑÎã®ÌïòÍ≥† Ìö®Í≥ºÏ†ÅÏù∏ Ïù¥ÎØ∏ÏßÄ Îß§Ïπ≠ ÏãúÏä§ÌÖú"""
        query_lower = query.lower()
        
        # ExcelÏóêÏÑú Ï∂îÏ∂úÎêú Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏóÜÏúºÎ©¥ ÏïàÎÇ¥
        if not self.extracted_images:
            return {
                "type": "no_image",
                "title": "üñºÔ∏è Ïù¥ÎØ∏ÏßÄ ÏóÜÏùå",
                "content": "Excel ÌååÏùºÏóêÏÑú Ï∂îÏ∂úÎêú Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏóÜÏäµÎãàÎã§. Î®ºÏ†Ä Excel ÌååÏùºÏùÑ ÏóÖÎ°úÎìúÌïòÍ≥† 'üì§ Ïù¥ÎØ∏ÏßÄ Ï∂îÏ∂ú' Î≤ÑÌäºÏùÑ ÌÅ¥Î¶≠Ìï¥Ï£ºÏÑ∏Ïöî.",
                "available_images": [],
                "suggestions": ["Excel ÌååÏùº ÏóÖÎ°úÎìú", "üì§ Ïù¥ÎØ∏ÏßÄ Ï∂îÏ∂ú Î≤ÑÌäº ÌÅ¥Î¶≠"]
            }
        
        # Í∞ÑÎã®Ìïú ÌÇ§ÏõåÎìú Îß§Ïπ≠ ÏãúÏä§ÌÖú
        matched_images = []
        
        logger.info(f"Ïù¥ÎØ∏ÏßÄ Îß§Ïπ≠ ÏãúÏûë: ÏßàÎ¨∏='{query}', ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Ïù¥ÎØ∏ÏßÄ={list(self.extracted_images.keys())}")
        
        # Ï°∞Î¶ΩÎèÑ Í¥ÄÎ†® ÏßàÎ¨∏ (Í∞ÄÏû• Íµ¨Ï≤¥Ï†ÅÏù∏ ÌÇ§ÏõåÎìúÎ∂ÄÌÑ∞ Ï≤¥ÌÅ¨)
        if any(word in query_lower for word in ["Ï°∞Î¶ΩÎèÑ", "Ï°∞Î¶ΩÍ≥µÏ†ï", "Ï°∞Î¶ΩÏûëÏóÖ", "Ï°∞Î¶ΩÍ≥ºÏ†ï"]):
            logger.info("Ï°∞Î¶ΩÎèÑ Í¥ÄÎ†® ÏßàÎ¨∏ Í∞êÏßÄ")
            # Ï°∞Î¶Ω Í¥ÄÎ†® Ïù¥ÎØ∏ÏßÄ Ï∞æÍ∏∞ (image1~30 Ïö∞ÏÑ†)
            for img_name, img in self.extracted_images.items():
                if "image" in img_name.lower():
                    try:
                        img_num = int(''.join(filter(str.isdigit, img_name)))
                        if img_num <= 30:  # Ï°∞Î¶ΩÎèÑÎäî Î≥¥ÌÜµ ÏïûÏ™Ω Ïù¥ÎØ∏ÏßÄ
                            matched_images.append((img_name, img, f"Ï°∞Î¶Ω Í¥ÄÎ†® Ïù¥ÎØ∏ÏßÄ (Î≤àÌò∏: {img_num})", 10))
                            logger.info(f"Ï°∞Î¶Ω Ïù¥ÎØ∏ÏßÄ Îß§Ïπ≠: {img_name} (Ï†êÏàò: 10)")
                        else:
                            matched_images.append((img_name, img, f"Ï°∞Î¶Ω Í¥ÄÎ†® Ïù¥ÎØ∏ÏßÄ (Î≤àÌò∏: {img_num})", 5))
                            logger.info(f"Ï°∞Î¶Ω Ïù¥ÎØ∏ÏßÄ Îß§Ïπ≠: {img_name} (Ï†êÏàò: 5)")
                    except:
                        matched_images.append((img_name, img, "Ï°∞Î¶Ω Í¥ÄÎ†® Ïù¥ÎØ∏ÏßÄ", 3))
                        logger.info(f"Ï°∞Î¶Ω Ïù¥ÎØ∏ÏßÄ Îß§Ïπ≠: {img_name} (Ï†êÏàò: 3)")
        
        # Í≤ÄÏÇ¨ Í¥ÄÎ†® ÏßàÎ¨∏ (ÌíàÏßà Í≤ÄÏÇ¨ Ïö∞ÏÑ†)
        elif any(word in query_lower for word in ["Í≤ÄÏÇ¨", "ÌíàÏßà", "ÌÖåÏä§Ìä∏", "ÌôïÏù∏", "Í≤ÄÏàò"]):
            logger.info("Í≤ÄÏÇ¨ Í¥ÄÎ†® ÏßàÎ¨∏ Í∞êÏßÄ")
            for img_name, img in self.extracted_images.items():
                if "image" in img_name.lower():
                    try:
                        img_num = int(''.join(filter(str.isdigit, img_name)))
                        if 20 <= img_num <= 50:  # Í≤ÄÏÇ¨ Í¥ÄÎ†®ÏùÄ Ï§ëÍ∞Ñ Ïù¥ÎØ∏ÏßÄ
                            matched_images.append((img_name, img, f"Í≤ÄÏÇ¨ Í¥ÄÎ†® Ïù¥ÎØ∏ÏßÄ (Î≤àÌò∏: {img_num})", 10))
                            logger.info(f"Í≤ÄÏÇ¨ Ïù¥ÎØ∏ÏßÄ Îß§Ïπ≠: {img_name} (Ï†êÏàò: 10)")
                        else:
                            matched_images.append((img_name, img, f"Í≤ÄÏÇ¨ Í¥ÄÎ†® Ïù¥ÎØ∏ÏßÄ (Î≤àÌò∏: {img_num})", 5))
                            logger.info(f"Í≤ÄÏÇ¨ Ïù¥ÎØ∏ÏßÄ Îß§Ïπ≠: {img_name} (Ï†êÏàò: 5)")
                    except:
                        matched_images.append((img_name, img, "Í≤ÄÏÇ¨ Í¥ÄÎ†® Ïù¥ÎØ∏ÏßÄ", 3))
                        logger.info(f"Í≤ÄÏÇ¨ Ïù¥ÎØ∏ÏßÄ Îß§Ïπ≠: {img_name} (Ï†êÏàò: 3)")
        
        # Î∂ÄÌíà/ÎèÑÎ©¥ Í¥ÄÎ†® ÏßàÎ¨∏
        elif any(word in query_lower for word in ["Î∂ÄÌíà", "ÎèÑÎ©¥", "ÏÑ§Í≥Ñ", "ÏπòÏàò", "BOM"]):
            logger.info("Î∂ÄÌíà/ÎèÑÎ©¥ Í¥ÄÎ†® ÏßàÎ¨∏ Í∞êÏßÄ")
            for img_name, img in self.extracted_images.items():
                if "image" in img_name.lower():
                    try:
                        img_num = int(''.join(filter(str.isdigit, img_name)))
                        if img_num <= 25:  # Î∂ÄÌíàÎèÑÎ©¥ÏùÄ ÏïûÏ™Ω Ïù¥ÎØ∏ÏßÄ
                            matched_images.append((img_name, img, f"Î∂ÄÌíà/ÎèÑÎ©¥ Ïù¥ÎØ∏ÏßÄ (Î≤àÌò∏: {img_num})", 10))
                            logger.info(f"Î∂ÄÌíà/ÎèÑÎ©¥ Ïù¥ÎØ∏ÏßÄ Îß§Ïπ≠: {img_name} (Ï†êÏàò: 10)")
                        else:
                            matched_images.append((img_name, img, f"Î∂ÄÌíà/ÎèÑÎ©¥ Ïù¥ÎØ∏ÏßÄ (Î≤àÌò∏: {img_num})", 5))
                            logger.info(f"Î∂ÄÌíà/ÎèÑÎ©¥ Ïù¥ÎØ∏ÏßÄ Îß§Ïπ≠: {img_name} (Ï†êÏàò: 5)")
                    except:
                        matched_images.append((img_name, img, "Î∂ÄÌíà/ÎèÑÎ©¥ Ïù¥ÎØ∏ÏßÄ", 3))
                        logger.info(f"Î∂ÄÌíà/ÎèÑÎ©¥ Ïù¥ÎØ∏ÏßÄ Îß§Ïπ≠: {img_name} (Ï†êÏàò: 3)")
        
        # Ï†úÌíà Í¥ÄÎ†® ÏßàÎ¨∏
        elif any(word in query_lower for word in ["Ï†úÌíà", "ÏïàÏ∞©", "ÏÉÅÏÑ∏", "ÌÅ¥Î°úÏ¶àÏóÖ", "ÏôÑÏÑ±"]):
            logger.info("Ï†úÌíà Í¥ÄÎ†® ÏßàÎ¨∏ Í∞êÏßÄ")
            # Ï†úÌíà Í¥ÄÎ†® Ïù¥ÎØ∏ÏßÄ Ï∞æÍ∏∞ (image40+ Ïö∞ÏÑ†)
            for img_name, img in self.extracted_images.items():
                if "image" in img_name.lower():
                    try:
                        img_num = int(''.join(filter(str.isdigit, img_name)))
                        if img_num >= 40:  # Ï†úÌíà Í¥ÄÎ†®ÏùÄ Îí§Ï™Ω Ïù¥ÎØ∏ÏßÄ
                            matched_images.append((img_name, img, f"Ï†úÌíà Í¥ÄÎ†® Ïù¥ÎØ∏ÏßÄ (Î≤àÌò∏: {img_num})", 10))
                            logger.info(f"Ï†úÌíà Ïù¥ÎØ∏ÏßÄ Îß§Ïπ≠: {img_name} (Ï†êÏàò: 10)")
                        else:
                            matched_images.append((img_name, img, f"Ï†úÌíà Í¥ÄÎ†® Ïù¥ÎØ∏ÏßÄ (Î≤àÌò∏: {img_num})", 5))
                            logger.info(f"Ï†úÌíà Ïù¥ÎØ∏ÏßÄ Îß§Ïπ≠: {img_name} (Ï†êÏàò: 5)")
                    except:
                        matched_images.append((img_name, img, "Ï†úÌíà Í¥ÄÎ†® Ïù¥ÎØ∏ÏßÄ", 3))
                        logger.info(f"Ï†úÌíà Ïù¥ÎØ∏ÏßÄ Îß§Ïπ≠: {img_name} (Ï†êÏàò: 3)")
        
        # ÏùºÎ∞òÏ†ÅÏù∏ Ï°∞Î¶Ω Í¥ÄÎ†® ÏßàÎ¨∏ (ÎßàÏßÄÎßâÏóê Ï≤¥ÌÅ¨)
        elif any(word in query_lower for word in ["Ï°∞Î¶Ω", "Í≥µÏ†ï", "ÏûëÏóÖ", "Í≥ºÏ†ï"]):
            logger.info("ÏùºÎ∞ò Ï°∞Î¶Ω Í¥ÄÎ†® ÏßàÎ¨∏ Í∞êÏßÄ")
            # Ï°∞Î¶Ω Í¥ÄÎ†® Ïù¥ÎØ∏ÏßÄ Ï∞æÍ∏∞ (image1~30 Ïö∞ÏÑ†)
            for img_name, img in self.extracted_images.items():
                if "image" in img_name.lower():
                    try:
                        img_num = int(''.join(filter(str.isdigit, img_name)))
                        if img_num <= 30:  # Ï°∞Î¶ΩÎèÑÎäî Î≥¥ÌÜµ ÏïûÏ™Ω Ïù¥ÎØ∏ÏßÄ
                            matched_images.append((img_name, img, f"Ï°∞Î¶Ω Í¥ÄÎ†® Ïù¥ÎØ∏ÏßÄ (Î≤àÌò∏: {img_num})", 10))
                            logger.info(f"Ï°∞Î¶Ω Ïù¥ÎØ∏ÏßÄ Îß§Ïπ≠: {img_name} (Ï†êÏàò: 10)")
                        else:
                            matched_images.append((img_name, img, f"Ï°∞Î¶Ω Í¥ÄÎ†® Ïù¥ÎØ∏ÏßÄ (Î≤àÌò∏: {img_num})", 5))
                            logger.info(f"Ï°∞Î¶Ω Ïù¥ÎØ∏ÏßÄ Îß§Ïπ≠: {img_name} (Ï†êÏàò: 5)")
                    except:
                        matched_images.append((img_name, img, "Ï°∞Î¶Ω Í¥ÄÎ†® Ïù¥ÎØ∏ÏßÄ", 3))
                        logger.info(f"Ï°∞Î¶Ω Ïù¥ÎØ∏ÏßÄ Îß§Ïπ≠: {img_name} (Ï†êÏàò: 3)")
        

        
        # ÏùºÎ∞òÏ†ÅÏù∏ Ïù¥ÎØ∏ÏßÄ ÏöîÏ≤≠
        else:
            # Î™®Îì† Ïù¥ÎØ∏ÏßÄÎ•º Ï†êÏàòÏôÄ Ìï®Íªò Ï∂îÍ∞Ä
            for img_name, img in self.extracted_images.items():
                matched_images.append((img_name, img, f"Ïù¥ÎØ∏ÏßÄ: {img_name}", 1))
        
        # Ï†êÏàò ÏàúÏúºÎ°ú Ï†ïÎ†¨
        matched_images.sort(key=lambda x: x[3], reverse=True)
        
        logger.info(f"Ïù¥ÎØ∏ÏßÄ Îß§Ïπ≠ ÏôÑÎ£å: Ï¥ù {len(matched_images)}Í∞ú Îß§Ïπ≠, ÏÉÅÏúÑ 3Í∞ú: {[(name, score) for name, img, desc, score in matched_images[:3]]}")
        
        # ÏµúÍ≥† Ï†êÏàò Ïù¥ÎØ∏ÏßÄ Î∞òÌôò
        if matched_images:
            best_img_name, best_img, best_desc, best_score = matched_images[0]
            
            logger.info(f"ÏµúÏ†Å Ïù¥ÎØ∏ÏßÄ ÏÑ†ÌÉù: {best_img_name} (Ï†êÏàò: {best_score})")
            
            # VLM Î∂ÑÏÑù Í≤∞Í≥ºÍ∞Ä ÏûàÏúºÎ©¥ Ï∂îÍ∞Ä Ï†ïÎ≥¥ Ï†úÍ≥µ
            vlm_analysis = None
            if hasattr(self, 'image_analysis') and best_img_name in self.image_analysis:
                vlm_analysis = self.image_analysis[best_img_name]
            
            result = {
                "type": "image",
                "title": f"üñºÔ∏è {best_img_name} - {query}",
                "image": best_img,
                "description": best_desc,
                "all_images": [(name, img, desc) for name, img, desc, score in matched_images[:3]],  # ÏÉÅÏúÑ 3Í∞ú
                "query_info": f"ÏßàÎ¨∏: '{query}'Ïóê ÎåÄÌïú ÏµúÏ†Å Îß§Ïπ≠ Ïù¥ÎØ∏ÏßÄ (Ï†êÏàò: {best_score})",
                "total_matches": len(matched_images)
            }
            
            # VLM Î∂ÑÏÑù Í≤∞Í≥º Ï∂îÍ∞Ä
            if vlm_analysis and 'error' not in vlm_analysis:
                result["vlm_analysis"] = {
                    "summary": vlm_analysis['summary'],
                    "type": vlm_analysis['type'],
                    "tags": vlm_analysis['tags'],
                    "confidence": vlm_analysis['confidence'],
                    "details": vlm_analysis['details']
                }
            
            return result
        
        # Îß§Ïπ≠ÎêòÎäî Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏóÜÏúºÎ©¥ Î™®Îì† Ïù¥ÎØ∏ÏßÄ Î™©Î°ù ÌëúÏãú
        return {
            "type": "image_list",
            "title": "üñºÔ∏è ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Ïù¥ÎØ∏ÏßÄÎì§",
            "content": f"ÏßàÎ¨∏ '{query}'Ïóê ÎßûÎäî Ïù¥ÎØ∏ÏßÄÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. Îã§Ïùå Ïù¥ÎØ∏ÏßÄÎì§Ïù¥ ÏûàÏäµÎãàÎã§:",
            "available_images": list(self.extracted_images.keys()),
            "all_images": [(name, img, f"Ïù¥ÎØ∏ÏßÄ: {name}") for name, img in self.extracted_images.items()],
            "suggestions": [
                "Îçî Íµ¨Ï≤¥Ï†ÅÏù∏ ÏßàÎ¨∏ÏùÑ Ìï¥Î≥¥ÏÑ∏Ïöî",
                "Ïòà: 'Ï°∞Î¶Ω Í≥µÏ†ïÎèÑÎ•º Î≥¥Ïó¨Ï§ò'",
                "Ïòà: 'Ï†úÌíà ÏïàÏ∞© Ïù¥ÎØ∏ÏßÄÎ•º Î≥¥Ïó¨Ï§ò'",
                "Ïòà: 'ÌíàÏßà Í≤ÄÏÇ¨ Í≥ºÏ†ïÏùÑ Î≥¥Ïó¨Ï§ò'"
            ]
        }
    
    def _analyze_images_with_vlm(self):
        """VLMÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Ï∂îÏ∂úÎêú Ïù¥ÎØ∏ÏßÄÎì§ÏùÑ Î∂ÑÏÑù"""
        try:
            logger.info("VLM Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù ÏãúÏûë")
            
            # Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù Í≤∞Í≥º Ï†ÄÏû•
            self.image_analysis = {}
            
            for img_name, img in self.extracted_images.items():
                try:
                    # VLM Î∂ÑÏÑù ÏàòÌñâ
                    analysis_result = self._analyze_single_image_with_vlm(img_name, img)
                    self.image_analysis[img_name] = analysis_result
                    
                    logger.info(f"Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù ÏôÑÎ£å: {img_name} - {analysis_result['summary']}")
                    
                except Exception as e:
                    logger.error(f"Ïù¥ÎØ∏ÏßÄ {img_name} VLM Î∂ÑÏÑù Ïã§Ìå®: {e}")
                    self.image_analysis[img_name] = {
                        "error": str(e),
                        "summary": "Î∂ÑÏÑù Ïã§Ìå®",
                        "details": [],
                        "tags": []
                    }
            
            logger.info(f"VLM Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù ÏôÑÎ£å: {len(self.image_analysis)}Í∞ú Ïù¥ÎØ∏ÏßÄ")
            
        except Exception as e:
            logger.error(f"VLM Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù Ïã§Ìå®: {e}")
    
    def _analyze_single_image_with_vlm(self, img_name, img):
        """Îã®Ïùº Ïù¥ÎØ∏ÏßÄÎ•º VLMÏúºÎ°ú Î∂ÑÏÑù"""
        try:
            # Ïù¥ÎØ∏ÏßÄ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù
            img_info = {
                "name": img_name,
                "size": img.size,
                "mode": img.mode,
                "format": getattr(img, 'format', 'Unknown')
            }
            
            # Ïã§Ï†ú LLMÏùÑ ÏÇ¨Ïö©Ìïú Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù ÏãúÎèÑ
            analysis_result = self._analyze_with_real_llm(img_name, img, img_info)
            
            # LLM Î∂ÑÏÑùÏù¥ Ïã§Ìå®ÌïòÎ©¥ ÏãúÎÆ¨Î†àÏù¥ÏÖò ÏÇ¨Ïö©
            if not analysis_result or "error" in analysis_result:
                logger.warning(f"LLM Î∂ÑÏÑù Ïã§Ìå®, ÏãúÎÆ¨Î†àÏù¥ÏÖò ÏÇ¨Ïö©: {img_name}")
                analysis_result = self._simulate_vlm_analysis(img_name, img_info)
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"Îã®Ïùº Ïù¥ÎØ∏ÏßÄ VLM Î∂ÑÏÑù Ïã§Ìå® {img_name}: {e}")
            raise
    
    def _simulate_vlm_analysis(self, img_name, img_info):
        """VLM Î∂ÑÏÑù ÏãúÎÆ¨Î†àÏù¥ÏÖò (Ïã§Ï†ú VLM Î™®Îç∏Î°ú ÎåÄÏ≤¥ Í∞ÄÎä•)"""
        try:
            # Ïù¥ÎØ∏ÏßÄ Ïù¥Î¶ÑÍ≥º Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Î•º Í∏∞Î∞òÏúºÎ°ú Ïä§ÎßàÌä∏ Î∂ÑÏÑù
            img_name_lower = img_name.lower()
            
            # Ïù¥ÎØ∏ÏßÄ Î≤àÌò∏ Ï∂îÏ∂ú
            img_num = None
            if "image" in img_name_lower:
                try:
                    img_num = int(''.join(filter(str.isdigit, img_name)))
                except:
                    pass
            
            # Ïù¥ÎØ∏ÏßÄ Ïú†Ìòï Î∂ÑÎ•ò Î∞è Î∂ÑÏÑù
            if img_num is not None:
                if img_num <= 30:
                    # Ï°∞Î¶Ω/Í≥µÏ†ï Í¥ÄÎ†® Ïù¥ÎØ∏ÏßÄ
                    analysis = {
                        "type": "assembly_process",
                        "summary": f"Ï°∞Î¶Ω Í≥µÏ†ï Ïù¥ÎØ∏ÏßÄ (Î≤àÌò∏: {img_num})",
                        "details": [
                            f"Ïù¥ÎØ∏ÏßÄ {img_num}ÏùÄ Ï°∞Î¶Ω Í≥µÏ†ïÏùò {img_num}Î≤àÏß∏ Îã®Í≥ÑÎ•º Î≥¥Ïó¨Ï§çÎãàÎã§",
                            "ÏûëÏóÖÏûêÍ∞Ä Î∂ÄÌíàÏùÑ Ï°∞Î¶ΩÌïòÎäî Í≥ºÏ†ïÏù¥ÎÇò Í≥µÏ†ï Îã®Í≥ÑÎ•º ÎÇòÌÉÄÎÉÖÎãàÎã§",
                            "Ï°∞Î¶Ω ÏûëÏóÖÏùò ÌëúÏ§ÄÌôîÎêú Ï†àÏ∞®Î•º ÏãúÍ∞ÅÌôîÌï©ÎãàÎã§"
                        ],
                        "tags": ["Ï°∞Î¶Ω", "Í≥µÏ†ï", "ÏûëÏóÖ", "Îã®Í≥Ñ", f"image{img_num}"],
                        "confidence": 0.85
                    }
                elif 31 <= img_num <= 50:
                    # Í≤ÄÏÇ¨/ÌíàÏßà Í¥ÄÎ†® Ïù¥ÎØ∏ÏßÄ
                    analysis = {
                        "type": "quality_inspection",
                        "summary": f"ÌíàÏßà Í≤ÄÏÇ¨ Ïù¥ÎØ∏ÏßÄ (Î≤àÌò∏: {img_num})",
                        "details": [
                            f"Ïù¥ÎØ∏ÏßÄ {img_num}ÏùÄ ÌíàÏßà Í≤ÄÏÇ¨ Í≥ºÏ†ïÏùÑ Î≥¥Ïó¨Ï§çÎãàÎã§",
                            "Ï†úÌíàÏùò ÌíàÏßàÏùÑ ÌôïÏù∏ÌïòÍ≥† Í≤ÄÏ¶ùÌïòÎäî Îã®Í≥ÑÏûÖÎãàÎã§",
                            "Í≤ÄÏÇ¨ Í∏∞Ï§ÄÍ≥º Î∞©Î≤ïÏùÑ ÏãúÍ∞ÅÏ†ÅÏúºÎ°ú Ï†úÏãúÌï©ÎãàÎã§"
                        ],
                        "tags": ["Í≤ÄÏÇ¨", "ÌíàÏßà", "ÌÖåÏä§Ìä∏", "ÌôïÏù∏", f"image{img_num}"],
                        "confidence": 0.80
                    }
                else:
                    # Ï†úÌíà/ÏôÑÏÑ± Í¥ÄÎ†® Ïù¥ÎØ∏ÏßÄ
                    analysis = {
                        "type": "product_final",
                        "summary": f"Ï†úÌíà ÏôÑÏÑ± Ïù¥ÎØ∏ÏßÄ (Î≤àÌò∏: {img_num})",
                        "details": [
                            f"Ïù¥ÎØ∏ÏßÄ {img_num}ÏùÄ ÏôÑÏÑ±Îêú Ï†úÌíàÏù¥ÎÇò ÏµúÏ¢Ö ÏÉÅÌÉúÎ•º Î≥¥Ïó¨Ï§çÎãàÎã§",
                            "Ï†úÌíàÏùò ÏµúÏ¢Ö ÌòïÌÉúÎÇò ÏïàÏ∞© ÏÉÅÌÉúÎ•º ÎÇòÌÉÄÎÉÖÎãàÎã§",
                            "Ï∂úÌïò Ï†Ñ ÏµúÏ¢Ö Ï†êÍ≤Ä Í≤∞Í≥ºÎ•º ÏãúÍ∞ÅÌôîÌï©ÎãàÎã§"
                        ],
                        "tags": ["Ï†úÌíà", "ÏôÑÏÑ±", "ÏïàÏ∞©", "ÏµúÏ¢Ö", f"image{img_num}"],
                        "confidence": 0.75
                    }
            else:
                # ÏùºÎ∞ò Ïù¥ÎØ∏ÏßÄ
                analysis = {
                    "type": "general_image",
                    "summary": f"ÏùºÎ∞ò Ïù¥ÎØ∏ÏßÄ: {img_name}",
                    "details": [
                        f"Ïù¥ÎØ∏ÏßÄ {img_name}ÏùÄ Î¨∏ÏÑúÏóê Ìè¨Ìï®Îêú ÏùºÎ∞òÏ†ÅÏù∏ Ïù¥ÎØ∏ÏßÄÏûÖÎãàÎã§",
                        "Íµ¨Ï≤¥Ï†ÅÏù∏ ÎÇ¥Ïö©ÏùÄ Ïù¥ÎØ∏ÏßÄ ÏûêÏ≤¥Î•º ÌôïÏù∏Ìï¥Ïïº Ìï©ÎãàÎã§"
                    ],
                    "tags": ["Ïù¥ÎØ∏ÏßÄ", "ÏùºÎ∞ò", img_name],
                    "confidence": 0.60
                }
            
            # Ïù¥ÎØ∏ÏßÄ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï∂îÍ∞Ä
            analysis["metadata"] = img_info
            analysis["analysis_method"] = "VLM_Simulation"
            
            return analysis
            
        except Exception as e:
            logger.error(f"VLM Î∂ÑÏÑù ÏãúÎÆ¨Î†àÏù¥ÏÖò Ïã§Ìå®: {e}")
            return {
                "type": "error",
                "summary": "Î∂ÑÏÑù Ïã§Ìå®",
                "details": [f"Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}"],
                "tags": ["Ïò§Î•ò", "Î∂ÑÏÑùÏã§Ìå®"],
                "confidence": 0.0
            }
    
    def _analyze_with_real_llm(self, img_name, img, img_info):
        """Ïã§Ï†ú LLMÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù"""
        try:
            # Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑùÏùÑ ÏúÑÌïú ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±
            prompt = f"""
            Ïù¥ Ïù¥ÎØ∏ÏßÄ({img_name})Î•º Î∂ÑÏÑùÌï¥Ï£ºÏÑ∏Ïöî. 
            
            Îã§Ïùå Ï†ïÎ≥¥Î•º Ìè¨Ìï®ÌïòÏó¨ Î∂ÑÏÑùÌï¥Ï£ºÏÑ∏Ïöî:
            1. Ïù¥ÎØ∏ÏßÄÍ∞Ä Î≥¥Ïó¨Ï£ºÎäî ÎÇ¥Ïö© (Ï°∞Î¶Ω Í≥µÏ†ï, ÌíàÏßà Í≤ÄÏÇ¨, Ï†úÌíà Îì±)
            2. Ïù¥ÎØ∏ÏßÄÏùò Î™©Ï†ÅÍ≥º Ïö©ÎèÑ
            3. ÏûëÏóÖÏûêÍ∞Ä ÏïåÏïÑÏïº Ìï† ÌïµÏã¨ Ï†ïÎ≥¥
            4. Í¥ÄÎ†®Îêú ÌÇ§ÏõåÎìúÎÇò ÌÉúÍ∑∏
            
            ÌïúÍµ≠Ïñ¥Î°ú ÏÉÅÏÑ∏ÌïòÍ≤å ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî.
            """
            
            # GPT OSS 120BÎ°ú Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù ÏãúÎèÑ
            gpt_result = self.llm_integration.analyze_image_with_gpt_oss(img, prompt)
            
            if gpt_result.get("success"):
                # GPT OSS Î∂ÑÏÑù Í≤∞Í≥º ÌååÏã±
                analysis = self._parse_llm_analysis_result(gpt_result["analysis"], img_name, img_info)
                analysis["llm_model"] = "GPT OSS 120B"
                analysis["analysis_method"] = "Real_LLM"
                return analysis
            
            # GPT OSS Ïã§Ìå® Ïãú Qwen3 ÏãúÎèÑ
            qwen_result = self.llm_integration.analyze_image_with_qwen3(img, prompt)
            
            if qwen_result.get("success"):
                # Qwen3 Î∂ÑÏÑù Í≤∞Í≥º ÌååÏã±
                analysis = self._parse_llm_analysis_result(qwen_result["analysis"], img_name, img_info)
                analysis["llm_model"] = "Qwen3"
                analysis["analysis_method"] = "Real_LLM"
                return analysis
            
            # Î™®Îì† LLM Î∂ÑÏÑù Ïã§Ìå®
            logger.warning(f"Î™®Îì† LLM Î∂ÑÏÑù Ïã§Ìå®: {img_name}")
            return None
            
        except Exception as e:
            logger.error(f"Ïã§Ï†ú LLM Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù Ïã§Ìå® {img_name}: {e}")
            return None
    
    def _parse_llm_analysis_result(self, llm_text, img_name, img_info):
        """LLM Î∂ÑÏÑù Í≤∞Í≥ºÎ•º Íµ¨Ï°∞ÌôîÎêú ÌòïÌÉúÎ°ú ÌååÏã±"""
        try:
            # Ïù¥ÎØ∏ÏßÄ Î≤àÌò∏ Ï∂îÏ∂ú
            img_num = None
            if "image" in img_name.lower():
                try:
                    img_num = int(''.join(filter(str.isdigit, img_name)))
                except:
                    pass
            
            # LLM ÌÖçÏä§Ìä∏ÏóêÏÑú ÌÇ§ÏõåÎìú Ï∂îÏ∂ú
            text_lower = llm_text.lower()
            
            # Ïù¥ÎØ∏ÏßÄ Ïú†Ìòï Î∂ÑÎ•ò
            if any(word in text_lower for word in ["Ï°∞Î¶Ω", "Í≥µÏ†ï", "ÏûëÏóÖ", "Îã®Í≥Ñ", "Í≥ºÏ†ï"]):
                img_type = "assembly_process"
                confidence = 0.90
            elif any(word in text_lower for word in ["Í≤ÄÏÇ¨", "ÌíàÏßà", "ÌÖåÏä§Ìä∏", "ÌôïÏù∏"]):
                img_type = "quality_inspection"
                confidence = 0.90
            elif any(word in text_lower for word in ["Ï†úÌíà", "ÏôÑÏÑ±", "ÏïàÏ∞©", "ÏµúÏ¢Ö"]):
                img_type = "product_final"
                confidence = 0.90
            else:
                img_type = "general_image"
                confidence = 0.75
            
            # ÌÉúÍ∑∏ ÏÉùÏÑ±
            tags = []
            if img_num:
                tags.append(f"image{img_num}")
            
            # LLM ÌÖçÏä§Ìä∏ÏóêÏÑú ÌÇ§ÏõåÎìú Ï∂îÏ∂úÌïòÏó¨ ÌÉúÍ∑∏ Ï∂îÍ∞Ä
            keywords = ["Ï°∞Î¶Ω", "Í≥µÏ†ï", "ÏûëÏóÖ", "Í≤ÄÏÇ¨", "ÌíàÏßà", "ÌÖåÏä§Ìä∏", "Ï†úÌíà", "ÏôÑÏÑ±", "Î∂ÄÌíà", "ÎèÑÎ©¥"]
            for keyword in keywords:
                if keyword in text_lower:
                    tags.append(keyword)
            
            # ÏÉÅÏÑ∏ Î∂ÑÏÑùÏùÑ Î¨∏Ïû• Îã®ÏúÑÎ°ú Î∂ÑÎ¶¨
            details = []
            sentences = llm_text.split('.')
            for sentence in sentences:
                sentence = sentence.strip()
                if sentence and len(sentence) > 10:
                    details.append(sentence)
            
            # ÏµúÎåÄ 5Í∞ú ÏÉÅÏÑ∏ ÏÑ§Î™ÖÏúºÎ°ú Ï†úÌïú
            details = details[:5]
            
            analysis = {
                "type": img_type,
                "summary": f"LLM Î∂ÑÏÑù: {img_name}",
                "details": details,
                "tags": tags,
                "confidence": confidence,
                "metadata": img_info,
                "llm_raw_text": llm_text
            }
            
            return analysis
            
        except Exception as e:
            logger.error(f"LLM Î∂ÑÏÑù Í≤∞Í≥º ÌååÏã± Ïã§Ìå®: {e}")
            return None
    
    def generate_auto_questions(self, excel_file_path):
        """Excel ÌååÏùº ÎÇ¥Ïö©ÏùÑ Î∂ÑÏÑùÌïòÏó¨ ÏûêÎèôÏúºÎ°ú ÏßàÎ¨∏ ÏÉùÏÑ±"""
        try:
            # Excel ÌååÏùº ÏùΩÍ∏∞
            df = pd.read_excel(excel_file_path, sheet_name=None)
            
            questions = []
            
            # ÌÇ§ÏõåÎìú Ï†ïÏùò
            process_keywords = {'Ï°∞Î¶Ω', 'Í≥µÏ†ï', 'ÏûëÏóÖ', 'Í≥ºÏ†ï', 'Îã®Í≥Ñ', 'ÏàúÏÑú', 'Ï†àÏ∞®', 'Î∞©Î≤ï', 'Í∏∞Ïà†'}
            quality_keywords = {'Í≤ÄÏÇ¨', 'ÌíàÏßà', 'ÌÖåÏä§Ìä∏', 'ÌôïÏù∏', 'Í≤ÄÏàò', 'Ï†êÍ≤Ä', 'Ï∏°Ï†ï', 'Í∏∞Ï§Ä'}
            product_keywords = {'Ï†úÌíà', 'ÏôÑÏÑ±', 'Ï∂úÌïò', 'Ìè¨Ïû•', 'ÏïàÏ∞©', 'ÏÉÅÏÑ∏', 'ÌÅ¥Î°úÏ¶àÏóÖ'}
            material_keywords = {'Î∂ÄÌíà', 'ÏûêÏû¨', 'ÏÜåÏû¨', 'Ïû¨Î£å', 'BOM', 'ÎèÑÎ©¥', 'ÏÑ§Í≥Ñ', 'ÏπòÏàò'}
            equipment_keywords = {'Ïû•ÎπÑ', 'Í∏∞Í≥Ñ', 'ÎèÑÍµ¨', 'ÏßÄÍ∑∏', 'ÌòÑÎØ∏Í≤Ω', 'Î†åÏ¶à', 'Ïû•Ïπò'}
            
            # ÏãúÌä∏Î≥Ñ ÌÇ§ÏõåÎìú Î∂ÑÏÑù
            process_found = False
            quality_found = False
            material_found = False
            product_found = False
            equipment_found = False
            
            for sheet_name, sheet_df in df.items():
                if len(sheet_df) > 0:
                    # Ï≤òÏùå 5ÌñâÏóêÏÑú ÌÇ§ÏõåÎìú Í≤ÄÏÉâ
                    for idx, row in sheet_df.head(5).iterrows():
                        row_text = ' '.join([str(val) for val in row.values if pd.notna(val)]).lower()
                        
                        if not process_found and any(kw in row_text for kw in process_keywords):
                            questions.extend([
                                "Ï°∞Î¶Ω Í≥µÏ†ïÎèÑÎ•º Î≥¥Ïó¨Ï§ò",
                                "ÏûëÏóÖ Í≥ºÏ†ïÏùÑ ÏÑ§Î™ÖÌï¥Ï§ò",
                                "Ï°∞Î¶Ω Îã®Í≥ÑÎ≥Ñ Ïù¥ÎØ∏ÏßÄÎ•º Î≥¥Ïó¨Ï§ò"
                            ])
                            process_found = True
                        
                        if not quality_found and any(kw in row_text for kw in quality_keywords):
                            questions.extend([
                                "ÌíàÏßà Í≤ÄÏÇ¨ Í≥ºÏ†ïÏùÑ Î≥¥Ïó¨Ï§ò",
                                "Í≤ÄÏÇ¨ Í∏∞Ï§ÄÏùÑ ÏïåÎ†§Ï§ò",
                                "ÌÖåÏä§Ìä∏ Î∞©Î≤ïÏùÑ Î≥¥Ïó¨Ï§ò"
                            ])
                            quality_found = True
                        
                        if not material_found and any(kw in row_text for kw in material_keywords):
                            questions.extend([
                                "Î∂ÄÌíà ÎèÑÎ©¥ÏùÑ Î≥¥Ïó¨Ï§ò",
                                "BOM Ï†ïÎ≥¥Î•º ÏïåÎ†§Ï§ò",
                                "ÏûêÏû¨ Î™ÖÏÑ∏Î•º Î≥¥Ïó¨Ï§ò"
                            ])
                            material_found = True
                        
                        if not product_found and any(kw in row_text for kw in product_keywords):
                            questions.extend([
                                "ÏôÑÏÑ±Îêú Ï†úÌíàÏùÑ Î≥¥Ïó¨Ï§ò",
                                "Ï†úÌíà ÏïàÏ∞© Ïù¥ÎØ∏ÏßÄÎ•º Î≥¥Ïó¨Ï§ò",
                                "Ï∂úÌïò ÏÉÅÌÉúÎ•º Î≥¥Ïó¨Ï§ò"
                            ])
                            product_found = True
                        
                        if not equipment_found and any(kw in row_text for kw in equipment_keywords):
                            questions.extend([
                                "ÏÇ¨Ïö© Ïû•ÎπÑÎ•º Î≥¥Ïó¨Ï§ò",
                                "ÏûëÏóÖ ÎèÑÍµ¨Î•º ÏïåÎ†§Ï§ò",
                                "Ï∏°Ï†ï Ïû•ÎπÑÎ•º Î≥¥Ïó¨Ï§ò"
                            ])
                            equipment_found = True
                        
                        if len(questions) >= 12:  # ÏµúÎåÄ 12Í∞ú ÏßàÎ¨∏
                            break
                    if len(questions) >= 12:
                        break
            
            # ÏùºÎ∞òÏ†ÅÏù∏ ÏßàÎ¨∏ Ï∂îÍ∞Ä
            questions.extend([
                "ÌååÏùº Ï†ïÎ≥¥Î•º ÏïåÎ†§Ï§ò",
                "ÏãúÌä∏ Íµ¨Ï°∞Î•º ÏÑ§Î™ÖÌï¥Ï§ò",
                "Îç∞Ïù¥ÌÑ∞ ÏöîÏïΩÏùÑ Î≥¥Ïó¨Ï§ò"
            ])
            
            logger.info(f"ÏûêÎèô ÏßàÎ¨∏ ÏÉùÏÑ± ÏôÑÎ£å: {len(questions)}Í∞ú")
            return questions[:15]  # ÏµúÎåÄ 15Í∞úÎ°ú Ï†úÌïú
            
        except Exception as e:
            logger.error(f"ÏûêÎèô ÏßàÎ¨∏ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return [
                "ÌååÏùº Ï†ïÎ≥¥Î•º ÏïåÎ†§Ï§ò",
                "Ïù¥ÎØ∏ÏßÄÎ•º Î≥¥Ïó¨Ï§ò",
                "Îç∞Ïù¥ÌÑ∞Î•º ÏöîÏïΩÌï¥Ï§ò"
            ]
    
    def get_general_response(self, query):
        """ÏùºÎ∞ò ÏùëÎãµ"""
        return {
            "type": "general",
            "title": "üí° ÏùºÎ∞ò Ï†ïÎ≥¥",
            "content": f"'{query}'Ïóê ÎåÄÌïú Ï†ïÎ≥¥Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. Îçî Íµ¨Ï≤¥Ï†ÅÏù∏ ÏßàÎ¨∏ÏùÑ Ìï¥Ï£ºÏÑ∏Ïöî.",
            "suggestions": [
                "Ï°∞Î¶Ω Í≥µÏ†ïÏùÄ Ïñ¥Îñ§ Í≤ÉÎì§Ïù¥ ÏûàÎÇòÏöî?",
                "Ï†úÌíà Ï†ïÎ≥¥Î•º ÏïåÎ†§Ï£ºÏÑ∏Ïöî",
                "ERP ÏãúÏä§ÌÖú Í∏∞Îä•ÏùÄ Î¨¥ÏóáÏù∏Í∞ÄÏöî?",
                "ÌíàÏßà Í≤ÄÏÇ¨ Í∏∞Ï§ÄÏùÄ Î¨¥ÏóáÏù∏Í∞ÄÏöî?",
                "Ï°∞Î¶Ω Í≥µÏ†ïÎèÑ Ïù¥ÎØ∏ÏßÄÎ•º Î≥¥Ïó¨Ï£ºÏÑ∏Ïöî"
            ]
        }

def main():
    st.title("üè≠ Manufacturing Excel VLM System - Cloud")
    st.markdown("---")
    
    # Sidebar
    with st.sidebar:
        st.header("üîß System Configuration")
        
        # LLM Model Selection
        st.subheader("ü§ñ LLM Model Selection")
        available_models = st.session_state.system.llm_integration.get_available_models()
        
        if "selected_llm_model" not in st.session_state:
            st.session_state.selected_llm_model = available_models[0] if available_models else "Simulation"
        
        # Model options configuration
        model_options = []
        if "GPT OSS 120B" in available_models:
            model_options.append("GPT OSS 120B")
        if "Qwen3" in available_models:
            model_options.append("Qwen3")
        model_options.append("Simulation")
        
        selected_model = st.selectbox(
            "LLM Model for Analysis",
            options=model_options,
            index=model_options.index(st.session_state.selected_llm_model) if st.session_state.selected_llm_model in model_options else len(model_options) - 1,
            help="Select LLM model for image analysis"
        )
        
        if selected_model != st.session_state.selected_llm_model:
            st.session_state.selected_llm_model = selected_model
            st.rerun()
        
        # LLM Status Display
        if selected_model == "GPT OSS 120B":
            if not OPENAI_AVAILABLE:
                st.error("‚ùå Cannot use OpenAI package. Package installation required.")
            elif st.session_state.system.llm_integration.gpt_oss_client:
                st.success("‚úÖ GPT OSS 120B model activated (API connected)")
            else:
                st.error("‚ùå GPT OSS 120B model deactivated (API connection failed)")
        elif selected_model == "Qwen3":
            st.warning("‚ö†Ô∏è Qwen3 model (API connection test required)")
        else:
            st.info("‚ÑπÔ∏è Simulation mode (No actual LLM usage)")
        
        # Package Status Display
        if not OPENAI_AVAILABLE:
            st.error("‚ùå OpenAI package not installed. Check requirements.txt.")
        else:
            st.success("‚úÖ OpenAI package available")
        
        # API Key Status Display
        if GPT_OSS_API_KEY.startswith("sk-or-v1-"):
            st.warning("‚ö†Ô∏è Invalid API key format. Set OpenAI API key.")
        elif GPT_OSS_API_KEY:
            st.success("‚úÖ API key configured")
        else:
            st.error("‚ùå API key not configured")
        
        if st.button("üîÑ Reinitialize System", type="primary"):
            st.session_state.system = CloudVLMSystem()
            st.rerun()
        
        st.header("üìÅ Excel File Upload")
        st.write("Upload Excel files to extract images.")
        
        uploaded_file = st.file_uploader(
            "Select Excel File (.xlsx)",
            type=['xlsx'],
            help="Upload Excel file containing images"
        )
        
        if uploaded_file is not None:
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button("üì§ Extract Images", type="primary"):
                    with st.spinner("Extracting images from Excel file..."):
                        extracted_count = st.session_state.system.extract_images_from_uploaded_file(uploaded_file)
                        if extracted_count > 0:
                            st.success(f"‚úÖ {extracted_count} images extracted successfully!")
                        else:
                            st.warning("‚ö†Ô∏è No images found in Excel file.")
                            st.info("üí° Please upload Excel file containing images.")
                        st.rerun()
            
            with col2:
                if st.button("üìä Parse Data", type="secondary"):
                    with st.spinner("Parsing Excel file..."):
                        success = st.session_state.system.process_uploaded_excel_data(uploaded_file)
                        if success:
                            st.success("‚úÖ Excel data parsing completed!")
                        else:
                            st.warning("‚ö†Ô∏è Data parsing failed.")
                        st.rerun()
        
        st.header("üìä Excel File Information")
        
        if st.button("üìÅ View File Information", key="btn_file_info"):
            st.session_state.query = "Show Excel file information"
            st.rerun()
        
        st.header("üìù Example Questions")
        
        # Display auto-generated questions
        if uploaded_file is not None and hasattr(st.session_state.system, 'auto_questions'):
            st.subheader("ü§ñ AI Auto-Generated Questions")
            for i, question in enumerate(st.session_state.system.auto_questions[:8], 1):  # Show top 8 only
                if st.button(f"{i}. {question}", key=f"btn_auto_{i}"):
                    st.session_state.query = question
                    st.rerun()
            
            if len(st.session_state.system.auto_questions) > 8:
                with st.expander(f"View More Questions ({len(st.session_state.system.auto_questions)})"):
                    for i, question in enumerate(st.session_state.system.auto_questions[8:], 9):
                        if st.button(f"{i}. {question}", key=f"btn_auto_{i}"):
                            st.session_state.query = question
                            st.rerun()
        else:
            # Default example questions
            example_questions = [
                "Show Excel file information",
                "What is BOM information?",
                "What materials are needed for product production?",
                "Show assembly process diagram image",
                "What are the quality inspection standards?"
            ]
            
            for question in example_questions:
                if st.button(question, key=f"btn_{question}"):
                    st.session_state.query = question
                    st.rerun()
    
    # Main Content
    if 'system' not in st.session_state:
        st.session_state.system = CloudVLMSystem()
    
    if 'query' not in st.session_state:
        st.session_state.query = ""
    
    # Display current extracted image information
    if st.session_state.system.extracted_images:
        st.info(f"üì∏ Currently {len(st.session_state.system.extracted_images)} images are loaded.")
        
        # Display VLM analysis results if available
        if hasattr(st.session_state.system, 'image_analysis') and st.session_state.system.image_analysis:
            st.success("ü§ñ VLM Image Analysis Completed!")
            with st.expander("üîç VLM Image Analysis Results"):
                for img_name, analysis in st.session_state.system.image_analysis.items():
                    if 'error' not in analysis:
                        st.markdown(f"**{img_name}**")
                        st.write(f"üìù **Summary**: {analysis['summary']}")
                        st.write(f"üè∑Ô∏è **Tags**: {', '.join(analysis['tags'])}")
                        st.write(f"üìä **Confidence**: {analysis['confidence']:.2f}")
                        
                        # Display LLM model information
                        if "llm_model" in analysis:
                            st.write(f"ü§ñ **LLM Model**: {analysis['llm_model']}")
                            st.write(f"üîß **Analysis Method**: {analysis['analysis_method']}")
                        
                        with st.expander("üìã Detailed Analysis"):
                            for detail in analysis['details']:
                                st.write(f"‚Ä¢ {detail}")
                            
                            # Display LLM original text
                            if "llm_raw_text" in analysis:
                                st.write("---")
                                st.write("**ü§ñ LLM Original Analysis:**")
                                st.write(analysis['llm_raw_text'])
                        
                        st.divider()
                    else:
                        st.error(f"‚ùå {img_name}: {analysis['error']}")
        
        with st.expander("üìã Loaded Image List"):
            for img_name in st.session_state.system.extracted_images.keys():
                st.write(f"- {img_name}")
    
    # Query Input
    query = st.text_input(
        "üîç Enter your question:",
        value=st.session_state.query,
        placeholder="e.g., What assembly processes are there?"
    )
    
    if st.button("üöÄ Ask Question", type="primary") or st.session_state.query:
        if query:
            st.session_state.query = query
            with st.spinner("Processing your question..."):
                result = st.session_state.system.query_system(query)
                display_result(result)
        else:
            st.warning("Please enter a question.")

def display_result(result):
    """Display Results"""
    if result["type"] == "assembly":
        st.subheader(result["title"])
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.dataframe(result["data"], width='stretch')
        
        with col2:
            st.metric("Total Processes", result["summary"])
            st.info("SM-F741U Model Assembly Process Procedures")
    
    elif result["type"] == "product":
        st.subheader(result["title"])
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.dataframe(result["data"], width='stretch')
        
        with col2:
            st.metric("Model Name", result["summary"])
            st.info("Product Basic Information")
    
    elif result["type"] == "erp":
        st.subheader(result["title"])
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.dataframe(result["data"], width='stretch')
        
        with col2:
            st.metric("System", result["summary"])
            st.info("ERP System Functions")
    
    elif result["type"] == "quality":
        st.subheader(result["title"])
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.dataframe(result["data"], width='stretch')
        
        with col2:
            st.metric("Quality Management", result["summary"])
            st.info("Quality Inspection Standards and Procedures")
    
    elif result["type"] == "image":
        st.subheader(result["title"])
        
        # Ïù¥ÎØ∏ÏßÄÎ•º Î∞îÏù¥Ìä∏Î°ú Î≥ÄÌôòÌïòÏó¨ ÌëúÏãú
        img_byte_arr = io.BytesIO()
        result["image"].save(img_byte_arr, format='PNG')
        img_byte_arr = img_byte_arr.getvalue()
        
        st.image(img_byte_arr, caption=result["description"], width=400)
        
        # Display image information
        st.info(f"üìê Image Size: {result['image'].size[0]} x {result['image'].size[1]} pixels")
        
        # Display VLM analysis results
        if "vlm_analysis" in result:
            st.success("ü§ñ VLM Image Analysis Results")
            vlm = result["vlm_analysis"]
            st.write(f"**üìù Summary**: {vlm['summary']}")
            st.write(f"**üè∑Ô∏è Type**: {vlm['type']}")
            st.write(f"**üîñ Tags**: {', '.join(vlm['tags'])}")
            st.write(f"**üìä Confidence**: {vlm['confidence']:.2f}")
            
            # Display LLM model information
            if "llm_model" in vlm:
                st.write(f"**ü§ñ LLM Model**: {vlm['llm_model']}")
                st.write(f"**üîß Analysis Method**: {vlm['analysis_method']}")
            
            with st.expander("üìã Detailed Analysis"):
                for detail in vlm['details']:
                    st.write(f"‚Ä¢ {detail}")
                
                # Display LLM original text
                if "llm_raw_text" in vlm:
                    st.write("---")
                    st.write("**ü§ñ LLM Original Analysis:**")
                    st.write(vlm['llm_raw_text'])
        
        # Display other matched images
        if "all_images" in result and len(result["all_images"]) > 1:
            st.write("üîç Other Related Images:")
            for i, (img_name, img, desc) in enumerate(result["all_images"][1:], 1):
                with st.expander(f"{i}. {img_name}"):
                    img_byte_arr = io.BytesIO()
                    img.save(img_byte_arr, format='PNG')
                    img_byte_arr = img_byte_arr.getvalue()
                    st.image(img_byte_arr, caption=desc, width=300)
    
    elif result["type"] == "image_list":
        st.subheader(result["title"])
        st.write(result["content"])
        
        # Display available image list
        if "available_images" in result:
            st.write("üìã Available Images:")
            for img_name in result["available_images"]:
                st.write(f"- {img_name}")
        
        # Display all images
        if "all_images" in result:
            st.write("üñºÔ∏è All Images:")
            for i, (img_name, img, desc) in enumerate(result["all_images"], 1):
                with st.expander(f"{i}. {img_name}"):
                    img_byte_arr = io.BytesIO()
                    img.save(img_byte_arr, format='PNG')
                    img_byte_arr = img_byte_arr.getvalue()
                    st.image(img_byte_arr, caption=desc, width=300)
    
    elif result["type"] == "no_image":
        st.subheader(result["title"])
        st.write(result["content"])
        
        if "available_images" in result:
            st.write("üìã Available Images:")
            for img_name in result["available_images"]:
                st.write(f"- {img_name}")
    
    elif result["type"] == "excel_search":
        st.subheader(result["title"])
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.dataframe(result["data"], width='stretch')
        
        with col2:
            st.metric("Search Results", result["summary"])
            st.info("Data found in Excel file")
    
    elif result["type"] == "vector_search":
        st.subheader(result["title"])
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.dataframe(result["data"], width='stretch')
        
        with col2:
            st.metric("Vector Search Results", result["summary"])
            st.info("Similar content found using AI vector search")
            
            # Display detailed information
            if "raw_results" in result:
                st.write("üîç Detailed Search Results:")
                for i, raw_result in enumerate(result["raw_results"][:3], 1):
                    with st.expander(f"Result {i} (Similarity: {raw_result['similarity']:.3f})"):
                        st.write(f"**Sheet**: {raw_result['sheet_name']}")
                        st.write(f"**Type**: {raw_result['type']}")
                        st.write(f"**Content**: {raw_result['content']}")
                        if raw_result.get("metadata"):
                            st.write(f"**Metadata**: {raw_result['metadata']}")
    
    elif result["type"] == "file_info":
        st.subheader(result["title"])
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.dataframe(result["data"], width='stretch')
        
        with col2:
            st.metric("File Count", result["summary"])
            st.info("Processed Excel file information")
    
    elif result["type"] == "no_files":
        st.subheader(result["title"])
        st.write(result["content"])
        st.info("üì§ Upload Excel files to process data.")
    
    elif result["type"] == "general":
        st.subheader(result["title"])
        st.write(result["content"])
        
        if "suggestions" in result:
            st.write("üí° Recommended Questions:")
            for suggestion in result["suggestions"]:
                st.write(f"- {suggestion}")

if __name__ == "__main__":
    main()
