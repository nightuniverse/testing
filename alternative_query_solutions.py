#!/usr/bin/env python3
"""
RAG-Anythingê³¼ Qdrant ì—°ë™ ì—†ì´ ê¸°ì¡´ ê²°ê³¼ë¥¼ í™œìš©í•œ ì¿¼ë¦¬ ì†”ë£¨ì…˜ë“¤
"""

import asyncio
import json
import os
from pathlib import Path
from typing import Dict, List, Any, Optional
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AlternativeQuerySolutions:
    """ê¸°ì¡´ ê²°ê³¼ë¥¼ í™œìš©í•œ ë‹¤ì–‘í•œ ì¿¼ë¦¬ ì†”ë£¨ì…˜"""
    
    def __init__(self, results_dir: str = "test_excels/test_results"):
        self.results_dir = Path(results_dir)
        self.knowledge_graphs_dir = self.results_dir / "knowledge_graphs"
        self.docling_dir = self.results_dir / "docling_parsing"
        self.image_modal_dir = self.results_dir / "image_modal_results"
        self.rag_processing_dir = self.results_dir / "rag_processing"
        
        # OpenAI API ì„¤ì •
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.openai_base_url = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
        
        if not self.openai_api_key:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    async def solution1_direct_json_query(self, query: str, file_name: str = None) -> Dict[str, Any]:
        """ì†”ë£¨ì…˜ 1: ì§ì ‘ JSON ê²°ê³¼ì—ì„œ ê²€ìƒ‰"""
        print("ğŸ” **ì†”ë£¨ì…˜ 1: ì§ì ‘ JSON ê²°ê³¼ì—ì„œ ê²€ìƒ‰**")
        print("=" * 60)
        
        try:
            # ëª¨ë“  ê²°ê³¼ íŒŒì¼ ë¡œë“œ
            all_data = {}
            
            # Knowledge Graph ë¡œë“œ
            if self.knowledge_graphs_dir.exists():
                for kg_file in self.knowledge_graphs_dir.glob("*.json"):
                    if file_name is None or file_name in kg_file.name:
                        with open(kg_file, 'r', encoding='utf-8') as f:
                            kg_data = json.load(f)
                            all_data[f"kg_{kg_file.stem}"] = kg_data
            
            # Docling íŒŒì‹± ê²°ê³¼ ë¡œë“œ
            if self.docling_dir.exists():
                for docling_file in self.docling_dir.glob("*.json"):
                    if file_name is None or file_name in docling_file.name:
                        with open(docling_file, 'r', encoding='utf-8') as f:
                            docling_data = json.load(f)
                            all_data[f"docling_{docling_file.stem}"] = docling_data
            
            # ì´ë¯¸ì§€ ëª¨ë‹¬ ê²°ê³¼ ë¡œë“œ
            if self.image_modal_dir.exists():
                for image_file in self.image_modal_dir.glob("*.json"):
                    if file_name is None or file_name in image_file.name:
                        with open(image_file, 'r', encoding='utf-8') as f:
                            image_data = json.load(f)
                            all_data[f"image_{image_file.stem}"] = image_data
            
            # ê²€ìƒ‰ ì‹¤í–‰
            search_results = []
            for data_type, data in all_data.items():
                matches = self._search_in_json(data, query)
                if matches:
                    search_results.extend([{
                        "source": data_type,
                        "match": match,
                        "context": self._extract_context(data, match)
                    } for match in matches])
            
            # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
            if search_results:
                answer = await self._generate_answer_from_results(query, search_results)
            else:
                answer = "(no-context)"
            
            return {
                "query": query,
                "answer": answer,
                "search_results": search_results,
                "total_matches": len(search_results)
            }
            
        except Exception as e:
            print(f"âŒ ì§ì ‘ JSON ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _search_in_json(self, data: Any, query: str) -> List[str]:
        """JSON ë°ì´í„°ì—ì„œ ê²€ìƒ‰"""
        matches = []
        
        def search_recursive(obj, path=""):
            if isinstance(obj, dict):
                for key, value in obj.items():
                    current_path = f"{path}.{key}" if path else key
                    search_recursive(value, current_path)
            elif isinstance(obj, list):
                for i, item in enumerate(obj):
                    current_path = f"{path}[{i}]"
                    search_recursive(item, current_path)
            elif isinstance(obj, str):
                if query.lower() in obj.lower():
                    matches.append(f"{path}: {obj[:200]}...")
        
        search_recursive(data)
        return matches
    
    def _extract_context(self, data: Any, match: str) -> str:
        """ë§¤ì¹˜ëœ í•­ëª©ì˜ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            # ê°„ë‹¨í•œ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if ":" in match:
                content = match.split(":", 1)[1]
                return content[:500] + "..." if len(content) > 500 else content
            return match
        except:
            return match
    
    async def solution2_semantic_search(self, query: str, file_name: str = None) -> Dict[str, Any]:
        """ì†”ë£¨ì…˜ 2: ì˜ë¯¸ì  ê²€ìƒ‰ (ì„ë² ë”© ê¸°ë°˜)"""
        print("ğŸ” **ì†”ë£¨ì…˜ 2: ì˜ë¯¸ì  ê²€ìƒ‰ (ì„ë² ë”© ê¸°ë°˜)**")
        print("=" * 60)
        
        try:
            from langchain_openai import OpenAIEmbeddings
            import numpy as np
            
            # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
            embeddings = OpenAIEmbeddings(
                model="text-embedding-3-small",
                openai_api_key=self.openai_api_key,
                openai_api_base=self.openai_base_url
            )
            
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = embeddings.embed_query(query)
            
            # ëª¨ë“  í…ìŠ¤íŠ¸ ì²­í¬ ìˆ˜ì§‘
            text_chunks = []
            
            # Knowledge Graphì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if self.knowledge_graphs_dir.exists():
                for kg_file in self.knowledge_graphs_dir.glob("*.json"):
                    if file_name is None or file_name in kg_file.name:
                        with open(kg_file, 'r', encoding='utf-8') as f:
                            kg_data = json.load(f)
                            chunks = self._extract_text_from_kg(kg_data)
                            text_chunks.extend(chunks)
            
            # Docling ê²°ê³¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if self.docling_dir.exists():
                for docling_file in self.docling_dir.glob("*.json"):
                    if file_name is None or file_name in docling_file.name:
                        with open(docling_file, 'r', encoding='utf-8') as f:
                            docling_data = json.load(f)
                            chunks = self._extract_text_from_docling(docling_data)
                            text_chunks.extend(chunks)
            
            if not text_chunks:
                return {"query": query, "answer": "(no-context)", "search_results": []}
            
            # ê° ì²­í¬ì˜ ì„ë² ë”© ìƒì„± ë° ìœ ì‚¬ë„ ê³„ì‚°
            similarities = []
            for i, chunk in enumerate(text_chunks):
                try:
                    chunk_embedding = embeddings.embed_query(chunk["text"])
                    similarity = self._cosine_similarity(query_embedding, chunk_embedding)
                    similarities.append({
                        "index": i,
                        "chunk": chunk,
                        "similarity": similarity
                    })
                except Exception as e:
                    print(f"ì²­í¬ {i} ì„ë² ë”© ì‹¤íŒ¨: {e}")
                    continue
            
            # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            similarities.sort(key=lambda x: x["similarity"], reverse=True)
            
            # ìƒìœ„ ê²°ê³¼ ì„ íƒ
            top_results = similarities[:5]
            
            # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
            if top_results:
                answer = await self._generate_answer_from_semantic_results(query, top_results)
            else:
                answer = "(no-context)"
            
            return {
                "query": query,
                "answer": answer,
                "search_results": top_results,
                "total_chunks": len(text_chunks),
                "top_similarity": top_results[0]["similarity"] if top_results else 0
            }
            
        except Exception as e:
            print(f"âŒ ì˜ë¯¸ì  ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _extract_text_from_kg(self, kg_data: Dict) -> List[Dict]:
        """Knowledge Graphì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        chunks = []
        
        if "nodes" in kg_data:
            for node in kg_data["nodes"]:
                if "properties" in node:
                    props = node["properties"]
                    text_parts = []
                    
                    # ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ í•„ë“œ ì¶”ì¶œ
                    for key, value in props.items():
                        if isinstance(value, str) and value.strip():
                            text_parts.append(f"{key}: {value}")
                    
                    if text_parts:
                        chunks.append({
                            "text": " | ".join(text_parts),
                            "source": "knowledge_graph",
                            "node_id": node.get("id", ""),
                            "node_type": node.get("type", "")
                        })
        
        return chunks
    
    def _extract_text_from_docling(self, docling_data: Dict) -> List[Dict]:
        """Docling ê²°ê³¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        chunks = []
        
        # í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ
        if "tables" in docling_data:
            for table in docling_data["tables"]:
                if "rows" in table:
                    for row in table["rows"]:
                        if isinstance(row, list):
                            text = " | ".join([str(cell) for cell in row if cell])
                            if text.strip():
                                chunks.append({
                                    "text": text,
                                    "source": "docling_table",
                                    "table_index": table.get("table_index", 0)
                                })
        
        # í…ìŠ¤íŠ¸ ì½˜í…ì¸  ì¶”ì¶œ
        if "content" in docling_data:
            content = docling_data["content"]
            if isinstance(content, str) and content.strip():
                # ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
                words = content.split()
                chunk_size = 100
                for i in range(0, len(words), chunk_size):
                    chunk_text = " ".join(words[i:i+chunk_size])
                    chunks.append({
                        "text": chunk_text,
                        "source": "docling_content",
                        "chunk_index": i // chunk_size
                    })
        
        return chunks
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        import numpy as np
        vec1 = np.array(vec1)
        vec2 = np.array(vec2)
        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
    
    async def solution3_hybrid_search(self, query: str, file_name: str = None) -> Dict[str, Any]:
        """ì†”ë£¨ì…˜ 3: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í‚¤ì›Œë“œ + ì˜ë¯¸ì )"""
        print("ğŸ” **ì†”ë£¨ì…˜ 3: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í‚¤ì›Œë“œ + ì˜ë¯¸ì )**")
        print("=" * 60)
        
        try:
            # 1. í‚¤ì›Œë“œ ê²€ìƒ‰
            keyword_results = await self.solution1_direct_json_query(query, file_name)
            
            # 2. ì˜ë¯¸ì  ê²€ìƒ‰
            semantic_results = await self.solution2_semantic_search(query, file_name)
            
            # 3. ê²°ê³¼ ê²°í•©
            combined_results = {
                "keyword_matches": keyword_results.get("search_results", []),
                "semantic_matches": semantic_results.get("search_results", []),
                "keyword_answer": keyword_results.get("answer", ""),
                "semantic_answer": semantic_results.get("answer", "")
            }
            
            # 4. ìµœì¢… ë‹µë³€ ìƒì„±
            final_answer = await self._generate_hybrid_answer(query, combined_results)
            
            return {
                "query": query,
                "answer": final_answer,
                "keyword_results": keyword_results,
                "semantic_results": semantic_results,
                "combined_results": combined_results
            }
            
        except Exception as e:
            print(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    async def solution4_knowledge_graph_query(self, query: str, file_name: str = None) -> Dict[str, Any]:
        """ì†”ë£¨ì…˜ 4: Knowledge Graph ê¸°ë°˜ ì¿¼ë¦¬"""
        print("ğŸ” **ì†”ë£¨ì…˜ 4: Knowledge Graph ê¸°ë°˜ ì¿¼ë¦¬**")
        print("=" * 60)
        
        try:
            # Knowledge Graph ë¡œë“œ
            kg_data = {}
            if self.knowledge_graphs_dir.exists():
                for kg_file in self.knowledge_graphs_dir.glob("*.json"):
                    if file_name is None or file_name in kg_file.name:
                        with open(kg_file, 'r', encoding='utf-8') as f:
                            kg_data[kg_file.stem] = json.load(f)
            
            if not kg_data:
                return {"query": query, "answer": "(no-context)", "kg_analysis": {}}
            
            # Knowledge Graph ë¶„ì„
            kg_analysis = self._analyze_knowledge_graph(kg_data, query)
            
            # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
            answer = await self._generate_kg_answer(query, kg_analysis)
            
            return {
                "query": query,
                "answer": answer,
                "kg_analysis": kg_analysis,
                "total_nodes": sum(len(kg.get("nodes", [])) for kg in kg_data.values()),
                "total_edges": sum(len(kg.get("edges", [])) for kg in kg_data.values())
            }
            
        except Exception as e:
            print(f"âŒ Knowledge Graph ì¿¼ë¦¬ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _analyze_knowledge_graph(self, kg_data: Dict, query: str) -> Dict[str, Any]:
        """Knowledge Graph ë¶„ì„"""
        analysis = {
            "relevant_nodes": [],
            "node_types": {},
            "image_references": [],
            "table_references": [],
            "text_content": []
        }
        
        for kg_name, kg in kg_data.items():
            if "nodes" in kg:
                for node in kg["nodes"]:
                    # ë…¸ë“œ íƒ€ì… í†µê³„
                    node_type = node.get("type", "unknown")
                    analysis["node_types"][node_type] = analysis["node_types"].get(node_type, 0) + 1
                    
                    # ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ë…¸ë“œ ì°¾ê¸°
                    if "properties" in node:
                        props = node["properties"]
                        node_text = str(props).lower()
                        if any(keyword in node_text for keyword in query.lower().split()):
                            analysis["relevant_nodes"].append({
                                "kg_name": kg_name,
                                "node_id": node.get("id", ""),
                                "node_type": node_type,
                                "properties": props
                            })
                    
                    # ì´ë¯¸ì§€ ì°¸ì¡°
                    if node_type == "assembly_diagram" or "image" in node_type:
                        if "properties" in node and "image_ref" in node["properties"]:
                            analysis["image_references"].append(node["properties"]["image_ref"])
        
        return analysis
    
    async def _generate_answer_from_results(self, query: str, search_results: List[Dict]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¡œë¶€í„° ë‹µë³€ ìƒì„±"""
        try:
            import openai
            client = openai.OpenAI(
                api_key=self.openai_api_key,
                base_url=self.openai_base_url
            )
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context_parts = []
            for result in search_results[:5]:  # ìƒìœ„ 5ê°œ ê²°ê³¼ë§Œ ì‚¬ìš©
                context_parts.append(f"ì¶œì²˜: {result['source']}\në‚´ìš©: {result['context']}")
            
            context_text = "\n\n".join(context_parts)
            
            prompt = f"""ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context_text}

ì§ˆë¬¸: {query}

ë‹µë³€:"""
            
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1000,
                temperature=0
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            print(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ì˜¤ë¥˜ ë°œìƒ: {e}"
    
    async def _generate_answer_from_semantic_results(self, query: str, semantic_results: List[Dict]) -> str:
        """ì˜ë¯¸ì  ê²€ìƒ‰ ê²°ê³¼ë¡œë¶€í„° ë‹µë³€ ìƒì„±"""
        try:
            import openai
            client = openai.OpenAI(
                api_key=self.openai_api_key,
                base_url=self.openai_base_url
            )
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context_parts = []
            for result in semantic_results:
                chunk = result["chunk"]
                context_parts.append(f"ì¶œì²˜: {chunk['source']}\nìœ ì‚¬ë„: {result['similarity']:.4f}\në‚´ìš©: {chunk['text']}")
            
            context_text = "\n\n".join(context_parts)
            
            prompt = f"""ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. ìœ ì‚¬ë„ ì ìˆ˜ê°€ ë†’ì€ ë‚´ìš©ì„ ìš°ì„ ì ìœ¼ë¡œ ì°¸ê³ í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context_text}

ì§ˆë¬¸: {query}

ë‹µë³€:"""
            
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1000,
                temperature=0
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            print(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ì˜¤ë¥˜ ë°œìƒ: {e}"
    
    async def _generate_hybrid_answer(self, query: str, combined_results: Dict) -> str:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ë¡œë¶€í„° ë‹µë³€ ìƒì„±"""
        try:
            import openai
            client = openai.OpenAI(
                api_key=self.openai_api_key,
                base_url=self.openai_base_url
            )
            
            # í‚¤ì›Œë“œì™€ ì˜ë¯¸ì  ê²€ìƒ‰ ê²°ê³¼ ê²°í•©
            keyword_context = combined_results.get("keyword_answer", "")
            semantic_context = combined_results.get("semantic_answer", "")
            
            prompt = f"""í‚¤ì›Œë“œ ê²€ìƒ‰ê³¼ ì˜ë¯¸ì  ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼:
{keyword_context}

ì˜ë¯¸ì  ê²€ìƒ‰ ê²°ê³¼:
{semantic_context}

ì§ˆë¬¸: {query}

í†µí•© ë‹µë³€:"""
            
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1000,
                temperature=0
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            print(f"í•˜ì´ë¸Œë¦¬ë“œ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ì˜¤ë¥˜ ë°œìƒ: {e}"
    
    async def _generate_kg_answer(self, query: str, kg_analysis: Dict) -> str:
        """Knowledge Graph ë¶„ì„ ê²°ê³¼ë¡œë¶€í„° ë‹µë³€ ìƒì„±"""
        try:
            import openai
            client = openai.OpenAI(
                api_key=self.openai_api_key,
                base_url=self.openai_base_url
            )
            
            # Knowledge Graph ë¶„ì„ ì •ë³´ êµ¬ì„±
            kg_info = f"""
ë…¸ë“œ íƒ€ì… ë¶„í¬: {kg_analysis.get('node_types', {})}
ê´€ë ¨ ë…¸ë“œ ìˆ˜: {len(kg_analysis.get('relevant_nodes', []))}
ì´ë¯¸ì§€ ì°¸ì¡° ìˆ˜: {len(kg_analysis.get('image_references', []))}
"""
            
            relevant_nodes_info = ""
            for node in kg_analysis.get('relevant_nodes', [])[:3]:  # ìƒìœ„ 3ê°œë§Œ
                relevant_nodes_info += f"- {node['node_type']}: {str(node['properties'])[:200]}...\n"
            
            prompt = f"""Knowledge Graph ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

Knowledge Graph ì •ë³´:
{kg_info}

ê´€ë ¨ ë…¸ë“œ:
{relevant_nodes_info}

ì§ˆë¬¸: {query}

ë‹µë³€:"""
            
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1000,
                temperature=0
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            print(f"Knowledge Graph ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ì˜¤ë¥˜ ë°œìƒ: {e}"

# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    """ë©”ì¸ í•¨ìˆ˜ - ì‚¬ìš© ì˜ˆì‹œ"""
    print("ğŸš€ **ëŒ€ì•ˆ ì¿¼ë¦¬ ì†”ë£¨ì…˜ í…ŒìŠ¤íŠ¸**")
    print("=" * 60)
    
    try:
        # ì†”ë£¨ì…˜ ì´ˆê¸°í™”
        solutions = AlternativeQuerySolutions()
        
        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
        test_queries = [
            "ì¡°ë¦½ ë‹¤ì´ì–´ê·¸ë¨ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ìˆ˜ì…ê²€ì‚¬ ê´€ë ¨ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
            "ì´ë¯¸ì§€ íŒŒì¼ì˜ ìœ„ì¹˜ëŠ” ì–´ë””ì¸ê°€ìš”?",
            "í…Œì´ë¸” ë°ì´í„°ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”"
        ]
        
        for query in test_queries:
            print(f"\nğŸ” ì¿¼ë¦¬: {query}")
            
            # ì†”ë£¨ì…˜ 1: ì§ì ‘ JSON ê²€ìƒ‰
            print("\n1ï¸âƒ£ ì§ì ‘ JSON ê²€ìƒ‰:")
            result1 = await solutions.solution1_direct_json_query(query)
            print(f"   ë‹µë³€: {result1.get('answer', 'ì˜¤ë¥˜')[:100]}...")
            
            # ì†”ë£¨ì…˜ 2: ì˜ë¯¸ì  ê²€ìƒ‰
            print("\n2ï¸âƒ£ ì˜ë¯¸ì  ê²€ìƒ‰:")
            result2 = await solutions.solution2_semantic_search(query)
            print(f"   ë‹µë³€: {result2.get('answer', 'ì˜¤ë¥˜')[:100]}...")
            
            # ì†”ë£¨ì…˜ 3: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
            print("\n3ï¸âƒ£ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰:")
            result3 = await solutions.solution3_hybrid_search(query)
            print(f"   ë‹µë³€: {result3.get('answer', 'ì˜¤ë¥˜')[:100]}...")
            
            # ì†”ë£¨ì…˜ 4: Knowledge Graph ì¿¼ë¦¬
            print("\n4ï¸âƒ£ Knowledge Graph ì¿¼ë¦¬:")
            result4 = await solutions.solution4_knowledge_graph_query(query)
            print(f"   ë‹µë³€: {result4.get('answer', 'ì˜¤ë¥˜')[:100]}...")
            
            print("\n" + "-" * 60)
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    asyncio.run(main())
