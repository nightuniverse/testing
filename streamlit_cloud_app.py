import streamlit as st
import pandas as pd
from pathlib import Path
import zipfile
import os
from PIL import Image
import io
import base64
import json
from datetime import datetime
import logging
import numpy as np
import hashlib
import random

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Test Excels VLM System - Cloud",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CloudVLMSystem:
    def __init__(self):
        self.excel_files = []
        self.processed_data = {}
        self.extracted_images = {}
        self.vector_database = None
        self.text_chunks = []
        self.embeddings = []
        self.embedding_model = None
        self.initialize_system()
    
    def initialize_system(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            # Streamlit Cloudì—ì„œëŠ” ë¡œì»¬ íŒŒì¼ ì ‘ê·¼ ë¶ˆê°€
            # ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±í•˜ì§€ ì•Šê³  ì—…ë¡œë“œëœ íŒŒì¼ ì²˜ë¦¬ ëŒ€ê¸°
            self.extracted_images = {}  # ë¹ˆ ì´ë¯¸ì§€ ë”•ì…”ë„ˆë¦¬ë¡œ ì´ˆê¸°í™”
            
            # ê¸°ë³¸ ë°ì´í„° ì´ˆê¸°í™”
            self.processed_data = {
                "ì‹œìŠ¤í…œ ì •ë³´": {
                    "type": "system",
                    "content": "Excel íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                    "features": ["íŒŒì¼ ì—…ë¡œë“œ", "ì´ë¯¸ì§€ ì¶”ì¶œ", "ë°ì´í„° ë¶„ì„"]
                }
            }
            
            return True
        except Exception as e:
            st.error(f"âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return False
    
    def extract_images_from_excel(self):
        """Excel íŒŒì¼ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ (ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)"""
        # Streamlit Cloudì—ì„œëŠ” ë¡œì»¬ íŒŒì¼ ì ‘ê·¼ ë¶ˆê°€
        # ì—…ë¡œë“œëœ íŒŒì¼ë§Œ ì²˜ë¦¬ ê°€ëŠ¥
        logger.info("ë¡œì»¬ Excel íŒŒì¼ ì ‘ê·¼ ë¶ˆê°€ - ì—…ë¡œë“œëœ íŒŒì¼ë§Œ ì²˜ë¦¬ ê°€ëŠ¥")
        self.create_default_images()
    
    def extract_images_from_uploaded_file(self, uploaded_file):
        """ì—…ë¡œë“œëœ Excel íŒŒì¼ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ"""
        try:
            # ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œë¡œ ì €ì¥
            with open("temp_excel.xlsx", "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            # Excel íŒŒì¼ì„ ZIPìœ¼ë¡œ ì—´ê¸°
            with zipfile.ZipFile("temp_excel.xlsx", 'r') as zip_file:
                # ì´ë¯¸ì§€ íŒŒì¼ë“¤ ì°¾ê¸°
                image_files = [f for f in zip_file.namelist() if f.startswith('xl/media/')]
                
                extracted_count = 0
                for image_file in image_files:
                    try:
                        # ì´ë¯¸ì§€ íŒŒì¼ ì½ê¸°
                        with zip_file.open(image_file) as img_file:
                            img_data = img_file.read()
                            img = Image.open(io.BytesIO(img_data))
                            
                            # ì´ë¯¸ì§€ ì´ë¦„ ì¶”ì¶œ
                            img_name = os.path.basename(image_file)
                            img_name_without_ext = os.path.splitext(img_name)[0]
                            
                            # ì´ë¯¸ì§€ ì €ì¥
                            self.extracted_images[img_name_without_ext] = img
                            extracted_count += 1
                            
                    except Exception as e:
                        logger.error(f"ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨ {image_file}: {e}")
                
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                if os.path.exists("temp_excel.xlsx"):
                    os.remove("temp_excel.xlsx")
                
                return extracted_count
                
        except Exception as e:
            logger.error(f"ì—…ë¡œë“œëœ Excel ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return 0
    
    def process_uploaded_excel_data(self, uploaded_file):
        """ì—…ë¡œë“œëœ Excel íŒŒì¼ì„ docling ìŠ¤íƒ€ì¼ë¡œ íŒŒì‹±í•˜ê³  ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•"""
        try:
            # ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œë¡œ ì €ì¥
            with open("temp_excel.xlsx", "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            # 1ë‹¨ê³„: Excel íŒŒì¼ì„ docling ìŠ¤íƒ€ì¼ë¡œ íŒŒì‹±
            parsed_data = self._parse_excel_docling_style("temp_excel.xlsx")
            
            # 2ë‹¨ê³„: í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„±
            self.text_chunks = self._create_text_chunks(parsed_data)
            
            # 3ë‹¨ê³„: ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ë° ë²¡í„° ìƒì„±
            self._initialize_embedding_model()
            self.embeddings = self._generate_embeddings(self.text_chunks)
            
            # 4ë‹¨ê³„: FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
            self._build_vector_database()
            
            # 5ë‹¨ê³„: ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
            file_name = uploaded_file.name
            self.processed_data[file_name] = {
                "type": "excel_file",
                "content": f"Excel íŒŒì¼: {file_name}",
                "parsed_data": parsed_data,
                "chunks_count": len(self.text_chunks),
                "vector_db_size": len(self.embeddings),
                "file_info": {
                    "name": file_name,
                    "size": len(uploaded_file.getbuffer()),
                    "uploaded": datetime.now()
                }
            }
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists("temp_excel.xlsx"):
                os.remove("temp_excel.xlsx")
            
            logger.info(f"Excel íŒŒì¼ docling íŒŒì‹± ë° ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ: {file_name}")
            return True
            
        except Exception as e:
            logger.error(f"Excel íŒŒì¼ docling íŒŒì‹± ì‹¤íŒ¨: {e}")
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if os.path.exists("temp_excel.xlsx"):
                os.remove("temp_excel.xlsx")
            return False
    
    def _parse_excel_docling_style(self, excel_file_path):
        """Excel íŒŒì¼ì„ docling ìŠ¤íƒ€ì¼ë¡œ íŒŒì‹±"""
        try:
            # Excel íŒŒì¼ ì½ê¸°
            df = pd.read_excel(excel_file_path, sheet_name=None)
            parsed_data = {
                "file_path": excel_file_path,
                "sheets": {},
                "metadata": {
                    "total_sheets": len(df),
                    "parsed_at": datetime.now().isoformat()
                }
            }
            
            for sheet_name, sheet_df in df.items():
                # ì‹œíŠ¸ë³„ ë°ì´í„° íŒŒì‹±
                sheet_data = self._parse_sheet_content(sheet_name, sheet_df)
                parsed_data["sheets"][sheet_name] = sheet_data
            
            return parsed_data
            
        except Exception as e:
            logger.error(f"Excel docling íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    def _parse_sheet_content(self, sheet_name, sheet_df):
        """ì‹œíŠ¸ ë‚´ìš©ì„ docling ìŠ¤íƒ€ì¼ë¡œ íŒŒì‹±"""
        try:
            # ê¸°ë³¸ ì •ë³´
            sheet_info = {
                "name": sheet_name,
                "dimensions": {
                    "rows": len(sheet_df),
                    "columns": len(sheet_df.columns)
                },
                "content": {}
            }
            
            # 1. í—¤ë” ì •ë³´ ì¶”ì¶œ
            if len(sheet_df) > 0:
                headers = sheet_df.columns.tolist()
                sheet_info["content"]["headers"] = headers
                
                # 2. ë°ì´í„° íƒ€ì… ë¶„ì„
                data_types = sheet_df.dtypes.to_dict()
                sheet_info["content"]["data_types"] = {str(k): str(v) for k, v in data_types.items()}
                
                # 3. í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ (docling ìŠ¤íƒ€ì¼)
                text_content = []
                
                # í—¤ë” í…ìŠ¤íŠ¸
                header_text = f"ì‹œíŠ¸ '{sheet_name}'ì˜ ì»¬ëŸ¼: {', '.join(headers)}"
                text_content.append(header_text)
                
                # ë°ì´í„° í–‰ í…ìŠ¤íŠ¸ (ì²˜ìŒ 10í–‰)
                for idx, row in sheet_df.head(10).iterrows():
                    row_text = f"í–‰ {idx+1}: {', '.join([f'{col}={val}' for col, val in row.items() if pd.notna(val)])}"
                    text_content.append(row_text)
                
                # 4. í…Œì´ë¸” êµ¬ì¡° ë¶„ì„
                if len(sheet_df) > 0:
                    # ìˆ«ì ì»¬ëŸ¼ê³¼ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ êµ¬ë¶„
                    numeric_cols = sheet_df.select_dtypes(include=[np.number]).columns.tolist()
                    text_cols = sheet_df.select_dtypes(include=['object']).columns.tolist()
                    
                    sheet_info["content"]["structure"] = {
                        "numeric_columns": numeric_cols,
                        "text_columns": text_cols,
                        "total_records": len(sheet_df)
                    }
                    
                    # ìˆ«ì ì»¬ëŸ¼ í†µê³„
                    if numeric_cols:
                        numeric_stats = {}
                        for col in numeric_cols:
                            col_data = sheet_df[col].dropna()
                            if len(col_data) > 0:
                                numeric_stats[col] = {
                                    "min": float(col_data.min()),
                                    "max": float(col_data.max()),
                                    "mean": float(col_data.mean()),
                                    "count": len(col_data)
                                }
                        sheet_info["content"]["numeric_stats"] = numeric_stats
                
                sheet_info["content"]["text_content"] = text_content
            
            return sheet_info
            
        except Exception as e:
            logger.error(f"ì‹œíŠ¸ íŒŒì‹± ì‹¤íŒ¨ {sheet_name}: {e}")
            return {"name": sheet_name, "error": str(e)}
    
    def _create_text_chunks(self, parsed_data):
        """íŒŒì‹±ëœ ë°ì´í„°ì—ì„œ ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„±"""
        chunks = []
        
        try:
            for sheet_name, sheet_data in parsed_data["sheets"].items():
                if "content" in sheet_data and "text_content" in sheet_data["content"]:
                    # ì‹œíŠ¸ë³„ ì²­í¬ ìƒì„±
                    sheet_chunk = {
                        "type": "sheet_overview",
                        "sheet_name": sheet_name,
                        "content": f"ì‹œíŠ¸ '{sheet_name}': {sheet_data['content']['text_content'][0]}",
                        "metadata": {
                            "rows": sheet_data["dimensions"]["rows"],
                            "columns": sheet_data["dimensions"]["columns"]
                        }
                    }
                    chunks.append(sheet_chunk)
                    
                    # ìƒì„¸ ë°ì´í„° ì²­í¬
                    for text_line in sheet_data["content"]["text_content"][1:]:
                        data_chunk = {
                            "type": "data_row",
                            "sheet_name": sheet_name,
                            "content": text_line,
                            "metadata": {"row_type": "data"}
                        }
                        chunks.append(data_chunk)
                    
                    # êµ¬ì¡° ì •ë³´ ì²­í¬
                    if "structure" in sheet_data["content"]:
                        structure = sheet_data["content"]["structure"]
                        structure_chunk = {
                            "type": "structure_info",
                            "sheet_name": sheet_name,
                            "content": f"ì‹œíŠ¸ '{sheet_name}' êµ¬ì¡°: ìˆ«ì ì»¬ëŸ¼ {len(structure['numeric_columns'])}, í…ìŠ¤íŠ¸ ì»¬ëŸ¼ {len(structure['text_columns'])}, ì´ {structure['total_records']}ê°œ ë ˆì½”ë“œ",
                            "metadata": structure
                        }
                        chunks.append(structure_chunk)
                        
                        # ìˆ«ì í†µê³„ ì²­í¬
                        if "numeric_stats" in sheet_data["content"]:
                            for col, stats in sheet_data["content"]["numeric_stats"].items():
                                stats_chunk = {
                                    "type": "numeric_stats",
                                    "sheet_name": sheet_name,
                                    "content": f"ì»¬ëŸ¼ '{col}' í†µê³„: ìµœì†Œê°’ {stats['min']}, ìµœëŒ€ê°’ {stats['max']}, í‰ê·  {stats['mean']}, ë°ì´í„° ìˆ˜ {stats['count']}",
                                    "metadata": {"column": col, "stats": stats}
                                }
                                chunks.append(stats_chunk)
            
            logger.info(f"ì´ {len(chunks)}ê°œì˜ í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„± ì™„ë£Œ")
            return chunks
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„± ì‹¤íŒ¨: {e}")
            return []
    
    def _initialize_embedding_model(self):
        """ê²½ëŸ‰í™”ëœ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            # Streamlit Cloud í™˜ê²½ì— ë§ê²Œ ê²½ëŸ‰í™”ëœ í•´ì‹œ ê¸°ë°˜ ì„ë² ë”© ì‚¬ìš©
            self.embedding_model = "hash_based"
            logger.info("ê²½ëŸ‰í™”ëœ í•´ì‹œ ê¸°ë°˜ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©")
        except Exception as e:
            logger.error(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.embedding_model = None
    
    def _generate_embeddings(self, text_chunks):
        """ì™„ì „íˆ ê²°ì •ì ì¸ í•´ì‹œ ê¸°ë°˜ ì„ë² ë”© ë²¡í„° ìƒì„±"""
        try:
            embeddings = []
            for chunk in text_chunks:
                # í•´ì‹œ ê¸°ë°˜ ë²¡í„° ìƒì„± (64ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ)
                text_content = chunk["content"]
                text_hash = hashlib.md5(text_content.encode('utf-8')).hexdigest()
                
                # ì™„ì „íˆ ê²°ì •ì ì¸ ë²¡í„° ìƒì„± (random.seed ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
                vector = self._hash_to_vector(text_hash, 64)
                
                # ì •ê·œí™”
                vector = vector / np.linalg.norm(vector)
                embeddings.append(vector)
            
            logger.info(f"ì™„ì „íˆ ê²°ì •ì ì¸ í•´ì‹œ ê¸°ë°˜ ì„ë² ë”© {len(embeddings)}ê°œ ìƒì„± ì™„ë£Œ")
            return embeddings
            
        except Exception as e:
            logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            # fallback: ê²°ì •ì ì¸ ë²¡í„°
            embeddings = []
            for i, chunk in enumerate(text_chunks):
                text_hash = hashlib.md5(f"fallback_{i}".encode('utf-8')).hexdigest()
                vector = self._hash_to_vector(text_hash, 64)
                vector = vector / np.linalg.norm(vector)
                embeddings.append(vector)
            logger.info(f"ê²°ì •ì ì¸ fallback ë²¡í„° {len(embeddings)}ê°œ ìƒì„± ì™„ë£Œ")
            return embeddings
    
    def _hash_to_vector(self, text_hash, dimensions):
        """í•´ì‹œë¥¼ ê²°ì •ì ì¸ ë²¡í„°ë¡œ ë³€í™˜"""
        vector = np.zeros(dimensions, dtype='float32')
        
        # í•´ì‹œì˜ ê° ë¬¸ìë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ì—¬ ë²¡í„° ìƒì„±
        for i in range(dimensions):
            # í•´ì‹œì—ì„œ ìˆœí™˜í•˜ë©´ì„œ ê°’ì„ ì¶”ì¶œ
            hash_idx = i % len(text_hash)
            char_val = ord(text_hash[hash_idx])
            
            # ë¬¸ì ê°’ì„ -1ì—ì„œ 1 ì‚¬ì´ë¡œ ì •ê·œí™”
            normalized_val = (char_val - 48) / 122.0  # 48(0) ~ 122(z) ë²”ìœ„ë¥¼ -1~1ë¡œ
            vector[i] = normalized_val
        
        return vector
    
    def _build_vector_database(self):
        """ê²½ëŸ‰í™”ëœ Python ê¸°ë°˜ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•"""
        try:
            if len(self.embeddings) == 0:
                logger.warning("ì„ë² ë”©ì´ ì—†ì–´ ë²¡í„° DB êµ¬ì¶• ë¶ˆê°€")
                return
            
            # ë²¡í„° ì°¨ì› í™•ì¸
            vector_dim = self.embeddings[0].shape[0]
            
            # ìˆœìˆ˜ Pythonìœ¼ë¡œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
            self.vector_database = {
                "vectors": np.array(self.embeddings),
                "dimension": vector_dim,
                "count": len(self.embeddings)
            }
            
            logger.info(f"ê²½ëŸ‰í™”ëœ Python ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(self.embeddings)}ê°œ ë²¡í„°, {vector_dim}ì°¨ì›")
            
        except Exception as e:
            logger.error(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨: {e}")
            self.vector_database = None
    
    def create_default_images(self):
        """ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„± (ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)"""
        # ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±í•˜ì§€ ì•ŠìŒ
        # Excelì—ì„œ ì¶”ì¶œëœ ì‹¤ì œ ì´ë¯¸ì§€ë§Œ ì‚¬ìš©
        logger.info("ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„± ë¹„í™œì„±í™” - Excel ì´ë¯¸ì§€ë§Œ ì‚¬ìš©")
        pass
    
    def create_quality_inspection_image(self):
        """í’ˆì§ˆê²€ì‚¬í‘œ ì´ë¯¸ì§€ ìƒì„±"""
        # 400x300 í¬ê¸°ì˜ ì´ë¯¸ì§€ ìƒì„±
        img = Image.new('RGB', (400, 300), color='white')
        
        # ê°„ë‹¨í•œ í’ˆì§ˆê²€ì‚¬í‘œ ê·¸ë¦¬ê¸°
        from PIL import ImageDraw, ImageFont
        
        draw = ImageDraw.Draw(img)
        
        # ì œëª©
        draw.text((20, 20), "í’ˆì§ˆê²€ì‚¬í‘œ", fill='black')
        draw.line([(20, 50), (380, 50)], fill='black', width=2)
        
        # ê²€ì‚¬ í•­ëª©ë“¤
        items = [
            "1. ì™¸ê´€ ê²€ì‚¬",
            "2. ì¹˜ìˆ˜ ê²€ì‚¬", 
            "3. ê¸°ëŠ¥ ê²€ì‚¬",
            "4. ë‚´êµ¬ì„± ê²€ì‚¬"
        ]
        
        y_pos = 70
        for item in items:
            draw.text((30, y_pos), item, fill='blue')
            y_pos += 30
        
        # í•©ê²©/ë¶ˆí•©ê²© ì²´í¬ë°•ìŠ¤
        draw.text((200, 70), "â–¡ í•©ê²©", fill='green')
        draw.text((200, 100), "â–¡ ë¶ˆí•©ê²©", fill='red')
        
        return img
    
    def create_assembly_process_image(self):
        """ì¡°ë¦½ê³µì •ë„ ì´ë¯¸ì§€ ìƒì„±"""
        # 400x300 í¬ê¸°ì˜ ì´ë¯¸ì§€ ìƒì„±
        img = Image.new('RGB', (400, 300), color='lightblue')
        
        from PIL import ImageDraw
        
        draw = ImageDraw.Draw(img)
        
        # ì œëª©
        draw.text((20, 20), "ì¡°ë¦½ê³µì •ë„", fill='darkblue')
        draw.line([(20, 50), (380, 50)], fill='darkblue', width=2)
        
        # ê³µì • íë¦„ë„ ê·¸ë¦¬ê¸°
        processes = [
            "ìˆ˜ì…ê²€ì‚¬",
            "ì´ì˜¤ë‚˜ì´ì €",
            "DINO ê²€ì‚¬", 
            "CU+SPONGE",
            "ë„ì „ TAPE",
            "ì¶œí•˜ê²€ì‚¬",
            "í¬ì¥"
        ]
        
        x_pos = 30
        y_pos = 80
        for i, process in enumerate(processes):
            # ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            draw.rectangle([x_pos, y_pos, x_pos+80, y_pos+40], outline='darkblue', width=2, fill='white')
            draw.text((x_pos+5, y_pos+10), process, fill='darkblue', size=8)
            
            # í™”ì‚´í‘œ ê·¸ë¦¬ê¸° (ë§ˆì§€ë§‰ ì œì™¸)
            if i < len(processes) - 1:
                draw.line([x_pos+80, y_pos+20, x_pos+100, y_pos+20], fill='darkblue', width=2)
                # í™”ì‚´í‘œ ë¨¸ë¦¬
                draw.polygon([(x_pos+100, y_pos+15), (x_pos+100, y_pos+25), (x_pos+110, y_pos+20)], fill='darkblue')
            
            x_pos += 100
            
            # ë‘ ë²ˆì§¸ ì¤„ë¡œ ë„˜ì–´ê°€ê¸°
            if x_pos > 350:
                x_pos = 30
                y_pos += 80
        
        return img
    
    def create_part_drawing_image(self):
        """ë¶€í’ˆë„ë©´ ì´ë¯¸ì§€ ìƒì„±"""
        # 400x300 í¬ê¸°ì˜ ì´ë¯¸ì§€ ìƒì„±
        img = Image.new('RGB', (400, 300), color='lightgreen')
        
        from PIL import ImageDraw
        
        draw = ImageDraw.Draw(img)
        
        # ì œëª©
        draw.text((20, 20), "ë¶€í’ˆë„ë©´ - FRONT DECO SUB", fill='darkgreen')
        draw.line([(20, 50), (380, 50)], fill='darkgreen', width=2)
        
        # ê°„ë‹¨í•œ ë„ë©´ ê·¸ë¦¬ê¸°
        # ì™¸ê³½ì„ 
        draw.rectangle([50, 80, 350, 250], outline='darkgreen', width=3)
        
        # ë‚´ë¶€ êµ¬ì¡°
        draw.rectangle([70, 100, 150, 180], outline='darkgreen', width=2, fill='white')
        draw.text((80, 120), "GATE", fill='darkgreen')
        
        draw.rectangle([170, 100, 250, 180], outline='darkgreen', width=2, fill='white')
        draw.text((180, 120), "SPONGE", fill='darkgreen')
        
        draw.rectangle([270, 100, 330, 180], outline='darkgreen', width=2, fill='white')
        draw.text((280, 120), "TAPE", fill='darkgreen')
        
        # ì¹˜ìˆ˜ì„ 
        draw.line([50, 260, 350, 260], fill='darkgreen', width=1)
        draw.text((200, 270), "300mm", fill='darkgreen')
        
        draw.line([370, 80, 370, 250], fill='darkgreen', width=1)
        draw.text((380, 165), "170mm", fill='darkgreen')
        
        return img
    
    def process_real_excel_data(self):
        """ì‹¤ì œ Excel íŒŒì¼ ë‚´ìš© ê¸°ë°˜ ë°ì´í„° ì²˜ë¦¬ (ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)"""
        # Streamlit Cloudì—ì„œëŠ” ë¡œì»¬ íŒŒì¼ ì ‘ê·¼ ë¶ˆê°€
        # ì—…ë¡œë“œëœ íŒŒì¼ë§Œ ì²˜ë¦¬ ê°€ëŠ¥
        logger.info("ë¡œì»¬ Excel íŒŒì¼ ì ‘ê·¼ ë¶ˆê°€ - ì—…ë¡œë“œëœ íŒŒì¼ë§Œ ì²˜ë¦¬ ê°€ëŠ¥")
        self.processed_data = {
            "ì‹œìŠ¤í…œ ì •ë³´": {
                "type": "system",
                "content": "Excel íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "features": ["íŒŒì¼ ì—…ë¡œë“œ", "ì´ë¯¸ì§€ ì¶”ì¶œ", "ë°ì´í„° ë¶„ì„"]
            }
        }
    
    def query_system(self, query):
        """ë²¡í„° ê²€ìƒ‰ ê¸°ë°˜ ë²”ìš© ì¿¼ë¦¬ ì²˜ë¦¬"""
        query_lower = query.lower()
        
        # ì´ë¯¸ì§€ ê´€ë ¨ (ìš°ì„ ìˆœìœ„ ë†’ì„)
        if "ì´ë¯¸ì§€" in query_lower or "ì‚¬ì§„" in query_lower:
            return self.get_image_data(query)
        
        # Excel íŒŒì¼ ì •ë³´ ìš”ì²­
        if "íŒŒì¼ ì •ë³´" in query_lower or "excel íŒŒì¼" in query_lower:
            return self.get_excel_file_info()
        
        # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ êµ¬ì¶•ë˜ì–´ ìˆìœ¼ë©´ ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
        if self.vector_database is not None and len(self.text_chunks) > 0:
            vector_results = self._vector_search_query(query)
            if vector_results:
                return vector_results
        
        # Excel íŒŒì¼ ë°ì´í„° ê²€ìƒ‰ (fallback)
        excel_results = self._search_excel_data(query)
        if excel_results:
            return excel_results
        
        # ì¼ë°˜ì ì¸ ì‘ë‹µ
        return self.get_general_response(query)
    
    def _search_excel_data(self, query):
        """Excel ë°ì´í„°ì—ì„œ ê²€ìƒ‰ (fallback)"""
        try:
            query_lower = query.lower()
            results = []
            
            for file_name, file_data in self.processed_data.items():
                if file_data.get("type") == "excel_file":
                    # íŒŒì‹±ëœ ë°ì´í„°ì—ì„œ ê²€ìƒ‰
                    if "parsed_data" in file_data:
                        parsed_data = file_data["parsed_data"]
                        for sheet_name, sheet_data in parsed_data.get("sheets", {}).items():
                            if "content" in sheet_data and "text_content" in sheet_data["content"]:
                                for text_line in sheet_data["content"]["text_content"]:
                                    if query_lower in text_line.lower():
                                        results.append({
                                            "file": file_name,
                                            "sheet": sheet_name,
                                            "content": text_line,
                                            "type": "text_match"
                                        })
                    
                    # ê¸°ì¡´ ì‹œíŠ¸ ì •ë³´ì—ì„œ ê²€ìƒ‰ (fallback)
                    for sheet_name, sheet_info in file_data.get("sheets", {}).items():
                        # ìƒ˜í”Œ ë°ì´í„°ì—ì„œ ê²€ìƒ‰
                        for row_data in sheet_info.get("sample_data", []):
                            for key, value in row_data.items():
                                if query_lower in str(value).lower():
                                    results.append({
                                        "file": file_name,
                                        "sheet": sheet_name,
                                        "data": row_data,
                                        "match": f"{key}: {value}",
                                        "type": "data_match"
                                    })
            
            if results:
                # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
                df_data = []
                for result in results:
                    if result.get("type") == "text_match":
                        df_data.append({
                            "íŒŒì¼ëª…": result["file"],
                            "ì‹œíŠ¸ëª…": result["sheet"],
                            "ë§¤ì¹­ ìœ í˜•": "í…ìŠ¤íŠ¸ ë§¤ì¹­",
                            "ë‚´ìš©": result["content"][:100] + "..." if len(result["content"]) > 100 else result["content"]
                        })
                    else:
                        df_data.append({
                            "íŒŒì¼ëª…": result["file"],
                            "ì‹œíŠ¸ëª…": result["sheet"],
                            "ë§¤ì¹­ ìœ í˜•": "ë°ì´í„° ë§¤ì¹­",
                            "ë§¤ì¹­ ë°ì´í„°": result["match"],
                            "ì „ì²´ ë°ì´í„°": str(result["data"])[:100] + "..." if len(str(result["data"])) > 100 else str(result["data"])
                        })
                
                df = pd.DataFrame(df_data)
                
                return {
                    "type": "excel_search",
                    "title": f"ğŸ” '{query}' ê²€ìƒ‰ ê²°ê³¼ (fallback)",
                    "data": df,
                    "summary": f"ì´ {len(results)}ê°œ ê²°ê³¼ ë°œê²¬",
                    "chart_type": "table"
                }
            
            return None
            
        except Exception as e:
            logger.error(f"Excel ë°ì´í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None
    
    def _vector_search_query(self, query):
        """ì™„ì „íˆ ê²°ì •ì ì¸ ë²¡í„° ê²€ìƒ‰ì„ í†µí•œ ì¿¼ë¦¬ ì²˜ë¦¬"""
        try:
            if self.vector_database is None or len(self.text_chunks) == 0:
                return None
            
            # ì¿¼ë¦¬ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜ (ê²°ì •ì )
            query_hash = hashlib.md5(query.encode('utf-8')).hexdigest()
            query_vector = self._hash_to_vector(query_hash, 64)
            query_vector = query_vector / np.linalg.norm(query_vector)
            
            # ìˆœìˆ˜ Pythonìœ¼ë¡œ ìœ ì‚¬ë„ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
            similarities = []
            vectors = self.vector_database["vectors"]
            
            for i, vector in enumerate(vectors):
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))
                similarities.append((similarity, i))
            
            # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬ (ìƒìœ„ 5ê°œ)
            similarities.sort(reverse=True)
            k = min(5, len(self.text_chunks))
            
            # ê²€ìƒ‰ ê²°ê³¼ ì •ë¦¬
            search_results = []
            for i, (similarity, idx) in enumerate(similarities[:k]):
                if idx < len(self.text_chunks):
                    chunk = self.text_chunks[idx]
                    search_results.append({
                        "rank": i + 1,
                        "similarity": float(similarity),
                        "content": chunk["content"],
                        "type": chunk["type"],
                        "sheet_name": chunk.get("sheet_name", "N/A"),
                        "metadata": chunk.get("metadata", {})
                    })
            
            if search_results:
                # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
                df_data = []
                for result in search_results:
                    df_data.append({
                        "ìˆœìœ„": result["rank"],
                        "ìœ ì‚¬ë„": f"{result['similarity']:.3f}",
                        "ì‹œíŠ¸": result["sheet_name"],
                        "ìœ í˜•": result["type"],
                        "ë‚´ìš©": result["content"][:100] + "..." if len(result["content"]) > 100 else result["content"]
                    })
                
                df = pd.DataFrame(df_data)
                
                return {
                    "type": "vector_search",
                    "title": f"ğŸ” AI ë²¡í„° ê²€ìƒ‰ ê²°ê³¼: '{query}'",
                    "data": df,
                    "summary": f"AI ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ {len(search_results)}ê°œ ê²°ê³¼ ë°œê²¬ (ìœ ì‚¬ë„ ê¸°ë°˜)",
                    "chart_type": "table",
                    "raw_results": search_results
                }
            
            return None
            
        except Exception as e:
            logger.error(f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None
    
    def get_excel_file_info(self):
        """Excel íŒŒì¼ ì •ë³´ ë°˜í™˜"""
        try:
            file_info = []
            for file_name, file_data in self.processed_data.items():
                if file_data.get("type") == "excel_file":
                    # íŒŒì‹±ëœ ë°ì´í„° ì •ë³´
                    parsed_info = file_data.get("parsed_data", {})
                    sheets_info = parsed_info.get("sheets", {})
                    
                    info = {
                        "íŒŒì¼ëª…": file_name,
                        "ì‹œíŠ¸ ìˆ˜": len(sheets_info),
                        "í…ìŠ¤íŠ¸ ì²­í¬ ìˆ˜": file_data.get("chunks_count", 0),
                        "ë²¡í„° DB í¬ê¸°": file_data.get("vector_db_size", 0),
                        "íŒŒì¼ í¬ê¸°": f"{file_data.get('file_info', {}).get('size', 0) / 1024:.1f} KB",
                        "ì—…ë¡œë“œ ì‹œê°„": str(file_data.get('file_info', {}).get('uploaded', 'N/A'))
                    }
                    file_info.append(info)
            
            if file_info:
                df = pd.DataFrame(file_info)
                return {
                    "type": "file_info",
                    "title": "ğŸ“ Excel íŒŒì¼ ì •ë³´ (ë²¡í„° DB í¬í•¨)",
                    "data": df,
                    "summary": f"ì´ {len(file_info)}ê°œ Excel íŒŒì¼, ë²¡í„° ê²€ìƒ‰ ê°€ëŠ¥",
                    "chart_type": "table"
                }
            else:
                return {
                    "type": "no_files",
                    "title": "ğŸ“ Excel íŒŒì¼ ì—†ìŒ",
                    "content": "ì²˜ë¦¬ëœ Excel íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
                }
                
        except Exception as e:
            logger.error(f"íŒŒì¼ ì •ë³´ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "type": "error",
                "title": "âŒ ì˜¤ë¥˜",
                "content": f"íŒŒì¼ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }
    
    def get_image_data(self, query):
        """Excelì—ì„œ ì¶”ì¶œëœ ì‹¤ì œ ì´ë¯¸ì§€ ë°ì´í„° ë°˜í™˜"""
        query_lower = query.lower()
        
        # Excelì—ì„œ ì¶”ì¶œëœ ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ì•ˆë‚´
        if not self.extracted_images:
            return {
                "type": "no_image",
                "title": "ğŸ–¼ï¸ ì´ë¯¸ì§€ ì—†ìŒ",
                "content": "Excel íŒŒì¼ì—ì„œ ì¶”ì¶œëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € Excel íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  'ğŸ“¤ ì´ë¯¸ì§€ ì¶”ì¶œ' ë²„íŠ¼ì„ í´ë¦­í•´ì£¼ì„¸ìš”.",
                "available_images": [],
                "suggestions": ["Excel íŒŒì¼ ì—…ë¡œë“œ", "ğŸ“¤ ì´ë¯¸ì§€ ì¶”ì¶œ ë²„íŠ¼ í´ë¦­"]
            }
        
        # ì§ˆë¬¸ í‚¤ì›Œë“œ ë¶„ì„ ë° ìš°ì„ ìˆœìœ„ ì„¤ì •
        query_keywords = []
        priority_keywords = []
        
        # ì¡°ë¦½ë„ ê´€ë ¨ ì§ˆë¬¸ (ìµœìš°ì„ )
        if any(word in query_lower for word in ["ì¡°ë¦½ë„", "ì¡°ë¦½", "ê³µì •", "ì‘ì—…"]):
            priority_keywords.extend(["ì¡°ë¦½", "ê³µì •", "ì‘ì—…", "ë‹¨ê³„", "ê³¼ì •"])
            query_keywords.extend(["ì¡°ë¦½", "ê³µì •", "ì‘ì—…", "ë‹¨ê³„", "ê³¼ì •"])
        
        # ì œí’ˆ ê´€ë ¨ ì§ˆë¬¸
        if any(word in query_lower for word in ["ì œí’ˆ", "ì•ˆì°©", "ìƒì„¸", "í´ë¡œì¦ˆì—…"]):
            query_keywords.extend(["ì œí’ˆ", "ì•ˆì°©", "ìƒì„¸", "í´ë¡œì¦ˆì—…", "ë¶€í’ˆ"])
        
        # ê²€ì‚¬ ê´€ë ¨ ì§ˆë¬¸
        if any(word in query_lower for word in ["ê²€ì‚¬", "í’ˆì§ˆ", "í…ŒìŠ¤íŠ¸", "í™•ì¸"]):
            query_keywords.extend(["ê²€ì‚¬", "í’ˆì§ˆ", "í…ŒìŠ¤íŠ¸", "í™•ì¸", "ê¸°ì¤€"])
        
        # ë¶€í’ˆ/ë„ë©´ ê´€ë ¨ ì§ˆë¬¸
        if any(word in query_lower for word in ["ë¶€í’ˆ", "ë„ë©´", "ì„¤ê³„", "ì¹˜ìˆ˜"]):
            query_keywords.extend(["ë¶€í’ˆ", "ë„ë©´", "ì„¤ê³„", "ì¹˜ìˆ˜", "ìƒì„¸"])
        
        # ì¥ë¹„ ê´€ë ¨ ì§ˆë¬¸
        if any(word in query_lower for word in ["ì¥ë¹„", "í˜„ë¯¸ê²½", "ì§€ê·¸", "ë Œì¦ˆ"]):
            query_keywords.extend(["ì¥ë¹„", "í˜„ë¯¸ê²½", "ì§€ê·¸", "ë Œì¦ˆ", "ë„êµ¬"])
        
        # í¬ì¥/ì™„ì„± ê´€ë ¨ ì§ˆë¬¸
        if any(word in query_lower for word in ["í¬ì¥", "ì™„ì„±", "ìµœì¢…", "ì¶œí•˜"]):
            query_keywords.extend(["í¬ì¥", "ì™„ì„±", "ìµœì¢…", "ì¶œí•˜", "ë°°ì†¡"])
        
        # ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
        best_match = None
        best_score = 0
        
        for img_name, img in self.extracted_images.items():
            img_name_lower = img_name.lower()
            score = 0
            
            # ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œ ë§¤ì¹­ (ë†’ì€ ì ìˆ˜)
            for priority_keyword in priority_keywords:
                if priority_keyword in img_name_lower:
                    score += 10  # ìµœìš°ì„  ì ìˆ˜
                elif any(priority_keyword in str(img_name) for img_name in self.extracted_images.keys()):
                    score += 8   # ê°„ì ‘ ë§¤ì¹­
            
            # ì¼ë°˜ í‚¤ì›Œë“œ ë§¤ì¹­
            for keyword in query_keywords:
                if keyword in img_name_lower:
                    score += 5   # ì§ì ‘ ë§¤ì¹­
                elif any(keyword in str(img_name) for img_name in self.extracted_images.keys()):
                    score += 3   # ê°„ì ‘ ë§¤ì¹­
            
            # ì´ë¯¸ì§€ ì´ë¦„ íŒ¨í„´ ë§¤ì¹­
            if "image" in img_name_lower:
                # ìˆ«ì ê¸°ë°˜ ìš°ì„ ìˆœìœ„ (ì¡°ë¦½ë„ëŠ” ë³´í†µ ì•ìª½ ì´ë¯¸ì§€)
                try:
                    img_num = int(''.join(filter(str.isdigit, img_name)))
                    if "ì¡°ë¦½" in query_lower and img_num <= 30:  # ì¡°ë¦½ë„ëŠ” ì•ìª½ ì´ë¯¸ì§€
                        score += 3
                    elif "ì œí’ˆ" in query_lower and img_num >= 40:  # ì œí’ˆ ê´€ë ¨ì€ ë’¤ìª½ ì´ë¯¸ì§€
                        score += 3
                except:
                    pass
            
            # ì ìˆ˜ ì—…ë°ì´íŠ¸
            if score > best_score:
                best_score = score
                best_match = (img_name, img, f"ë§¤ì¹­ ì ìˆ˜: {score} (í‚¤ì›Œë“œ: {', '.join(query_keywords[:3])})")
        
        # ë§¤ì¹­ëœ ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ë°˜í™˜
        if best_match and best_score > 0:
            img_name, img, description = best_match
            return {
                "type": "image",
                "title": f"ğŸ–¼ï¸ {img_name} - {query}",
                "image": img,
                "description": description,
                "all_images": [best_match],
                "query_info": f"ì§ˆë¬¸: '{query}'ì— ëŒ€í•œ ìµœì  ë§¤ì¹­ ì´ë¯¸ì§€"
            }
        
        # ë§¤ì¹­ë˜ëŠ” ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ëª¨ë“  ì´ë¯¸ì§€ ëª©ë¡ í‘œì‹œ
        return {
            "type": "image_list",
            "title": "ğŸ–¼ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë¯¸ì§€ë“¤",
            "content": f"ì§ˆë¬¸ '{query}'ì— ë§ëŠ” ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒ ì´ë¯¸ì§€ë“¤ì´ ìˆìŠµë‹ˆë‹¤:",
            "available_images": list(self.extracted_images.keys()),
            "all_images": [(name, img, f"ì´ë¯¸ì§€: {name}") for name, img in self.extracted_images.items()],
            "suggestions": ["ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”", "ì˜ˆ: 'ì¡°ë¦½ ê³µì •ë„ë¥¼ ë³´ì—¬ì¤˜'", "ì˜ˆ: 'ì œí’ˆ ì•ˆì°© ì´ë¯¸ì§€ë¥¼ ë³´ì—¬ì¤˜'"]
        }
    
    def get_general_response(self, query):
        """ì¼ë°˜ ì‘ë‹µ"""
        return {
            "type": "general",
            "title": "ğŸ’¡ ì¼ë°˜ ì •ë³´",
            "content": f"'{query}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.",
            "suggestions": [
                "ì¡°ë¦½ ê³µì •ì€ ì–´ë–¤ ê²ƒë“¤ì´ ìˆë‚˜ìš”?",
                "ì œí’ˆ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
                "ERP ì‹œìŠ¤í…œ ê¸°ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "í’ˆì§ˆ ê²€ì‚¬ ê¸°ì¤€ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "ì¡°ë¦½ ê³µì •ë„ ì´ë¯¸ì§€ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”"
            ]
        }

def main():
    st.title("ğŸ“Š Test Excels VLM System - Cloud")
    st.markdown("---")
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("ğŸ”§ ì‹œìŠ¤í…œ ì„¤ì •")
        
        if st.button("ğŸ”„ ì‹œìŠ¤í…œ ì¬ì´ˆê¸°í™”", type="primary"):
            st.session_state.system = CloudVLMSystem()
            st.rerun()
        
        st.header("ğŸ“ Excel íŒŒì¼ ì—…ë¡œë“œ")
        st.write("Excel íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        uploaded_file = st.file_uploader(
            "Excel íŒŒì¼ ì„ íƒ (.xlsx)",
            type=['xlsx'],
            help="ì´ë¯¸ì§€ê°€ í¬í•¨ëœ Excel íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”"
        )
        
        if uploaded_file is not None:
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button("ğŸ“¤ ì´ë¯¸ì§€ ì¶”ì¶œ", type="primary"):
                    with st.spinner("Excel íŒŒì¼ì—ì„œ ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                        extracted_count = st.session_state.system.extract_images_from_uploaded_file(uploaded_file)
                        if extracted_count > 0:
                            st.success(f"âœ… {extracted_count}ê°œ ì´ë¯¸ì§€ ì¶”ì¶œ ì™„ë£Œ!")
                        else:
                            st.warning("âš ï¸ Excel íŒŒì¼ì—ì„œ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            st.info("ğŸ’¡ ì´ë¯¸ì§€ê°€ í¬í•¨ëœ Excel íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
                        st.rerun()
            
            with col2:
                if st.button("ğŸ“Š ë°ì´í„° íŒŒì‹±", type="secondary"):
                    with st.spinner("Excel íŒŒì¼ì„ íŒŒì‹±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                        success = st.session_state.system.process_uploaded_excel_data(uploaded_file)
                        if success:
                            st.success("âœ… Excel ë°ì´í„° íŒŒì‹± ì™„ë£Œ!")
                        else:
                            st.warning("âš ï¸ ë°ì´í„° íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                        st.rerun()
        
        st.header("ğŸ“Š Excel íŒŒì¼ ì •ë³´")
        
        if st.button("ğŸ“ íŒŒì¼ ì •ë³´ ë³´ê¸°", key="btn_file_info"):
            st.session_state.query = "Excel íŒŒì¼ ì •ë³´ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”"
            st.rerun()
        
        st.header("ğŸ“ ì˜ˆì‹œ ì§ˆë¬¸ë“¤")
        
        example_questions = [
            "Excel íŒŒì¼ ì •ë³´ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”",
            "BOM ì •ë³´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
            "ì œí’ˆ ìƒì‚°ì— í•„ìš”í•œ ìì¬ëŠ”?",
            "ì¡°ë¦½ ê³µì •ë„ ì´ë¯¸ì§€ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”",
            "í’ˆì§ˆê²€ì‚¬ ê¸°ì¤€ì€ ë¬´ì—‡ì¸ê°€ìš”?"
        ]
        
        for question in example_questions:
            if st.button(question, key=f"btn_{question}"):
                st.session_state.query = question
                st.rerun()
    
    # ë©”ì¸ ì»¨í…ì¸ 
    if 'system' not in st.session_state:
        st.session_state.system = CloudVLMSystem()
    
    if 'query' not in st.session_state:
        st.session_state.query = ""
    
    # í˜„ì¬ ì¶”ì¶œëœ ì´ë¯¸ì§€ ì •ë³´ í‘œì‹œ
    if st.session_state.system.extracted_images:
        st.info(f"ğŸ“¸ í˜„ì¬ {len(st.session_state.system.extracted_images)}ê°œ ì´ë¯¸ì§€ê°€ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        with st.expander("ğŸ“‹ ë¡œë“œëœ ì´ë¯¸ì§€ ëª©ë¡"):
            for img_name in st.session_state.system.extracted_images.keys():
                st.write(f"- {img_name}")
    
    # ì¿¼ë¦¬ ì…ë ¥
    query = st.text_input(
        "ğŸ” ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
        value=st.session_state.query,
        placeholder="ì˜ˆ: ì¡°ë¦½ ê³µì •ì€ ì–´ë–¤ ê²ƒë“¤ì´ ìˆë‚˜ìš”?"
    )
    
    if st.button("ğŸš€ ì§ˆë¬¸í•˜ê¸°", type="primary") or st.session_state.query:
        if query:
            st.session_state.query = query
            with st.spinner("ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                result = st.session_state.system.query_system(query)
                display_result(result)
        else:
            st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

def display_result(result):
    """ê²°ê³¼ í‘œì‹œ"""
    if result["type"] == "assembly":
        st.subheader(result["title"])
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.dataframe(result["data"], width='stretch')
        
        with col2:
            st.metric("ì´ ê³µì • ìˆ˜", result["summary"])
            st.info("SM-F741U ëª¨ë¸ì˜ ì¡°ë¦½ ê³µì • ì ˆì°¨")
    
    elif result["type"] == "product":
        st.subheader(result["title"])
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.dataframe(result["data"], width='stretch')
        
        with col2:
            st.metric("ëª¨ë¸ëª…", result["summary"])
            st.info("ì œí’ˆ ê¸°ë³¸ ì •ë³´")
    
    elif result["type"] == "erp":
        st.subheader(result["title"])
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.dataframe(result["data"], width='stretch')
        
        with col2:
            st.metric("ì‹œìŠ¤í…œ", result["summary"])
            st.info("ERP ì‹œìŠ¤í…œ ê¸°ëŠ¥")
    
    elif result["type"] == "quality":
        st.subheader(result["title"])
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.dataframe(result["data"], width='stretch')
        
        with col2:
            st.metric("í’ˆì§ˆ ê´€ë¦¬", result["summary"])
            st.info("í’ˆì§ˆ ê²€ì‚¬ ê¸°ì¤€ ë° ì ˆì°¨")
    
    elif result["type"] == "image":
        st.subheader(result["title"])
        
        # ì´ë¯¸ì§€ë¥¼ ë°”ì´íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ í‘œì‹œ
        import io
        img_byte_arr = io.BytesIO()
        result["image"].save(img_byte_arr, format='PNG')
        img_byte_arr = img_byte_arr.getvalue()
        
        st.image(img_byte_arr, caption=result["description"], width=400)
        
        # ì´ë¯¸ì§€ ì •ë³´ í‘œì‹œ
        st.info(f"ğŸ“ ì´ë¯¸ì§€ í¬ê¸°: {result['image'].size[0]} x {result['image'].size[1]} í”½ì…€")
        
        # ë‹¤ë¥¸ ë§¤ì¹­ëœ ì´ë¯¸ì§€ë“¤ë„ í‘œì‹œ
        if "all_images" in result and len(result["all_images"]) > 1:
            st.write("ğŸ” ë‹¤ë¥¸ ê´€ë ¨ ì´ë¯¸ì§€ë“¤:")
            for i, (img_name, img, desc) in enumerate(result["all_images"][1:], 1):
                with st.expander(f"{i}. {img_name}"):
                    img_byte_arr = io.BytesIO()
                    img.save(img_byte_arr, format='PNG')
                    img_byte_arr = img_byte_arr.getvalue()
                    st.image(img_byte_arr, caption=desc, width=300)
    
    elif result["type"] == "image_list":
        st.subheader(result["title"])
        st.write(result["content"])
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë¯¸ì§€ ëª©ë¡ í‘œì‹œ
        if "available_images" in result:
            st.write("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë¯¸ì§€:")
            for img_name in result["available_images"]:
                st.write(f"- {img_name}")
        
        # ëª¨ë“  ì´ë¯¸ì§€ í‘œì‹œ
        if "all_images" in result:
            st.write("ğŸ–¼ï¸ ëª¨ë“  ì´ë¯¸ì§€:")
            for i, (img_name, img, desc) in enumerate(result["all_images"], 1):
                with st.expander(f"{i}. {img_name}"):
                    img_byte_arr = io.BytesIO()
                    img.save(img_byte_arr, format='PNG')
                    img_byte_arr = img_byte_arr.getvalue()
                    st.image(img_byte_arr, caption=desc, width=300)
    
    elif result["type"] == "no_image":
        st.subheader(result["title"])
        st.write(result["content"])
        
        if "available_images" in result:
            st.write("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë¯¸ì§€:")
            for img_name in result["available_images"]:
                st.write(f"- {img_name}")
    
    elif result["type"] == "excel_search":
        st.subheader(result["title"])
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.dataframe(result["data"], width='stretch')
        
        with col2:
            st.metric("ê²€ìƒ‰ ê²°ê³¼", result["summary"])
            st.info("Excel íŒŒì¼ì—ì„œ ì°¾ì€ ë°ì´í„°")
    
    elif result["type"] == "vector_search":
        st.subheader(result["title"])
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.dataframe(result["data"], width='stretch')
        
        with col2:
            st.metric("ë²¡í„° ê²€ìƒ‰ ê²°ê³¼", result["summary"])
            st.info("AI ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ì°¾ì€ ìœ ì‚¬í•œ ë‚´ìš©")
            
            # ìƒì„¸ ì •ë³´ í‘œì‹œ
            if "raw_results" in result:
                st.write("ğŸ” ìƒì„¸ ê²€ìƒ‰ ê²°ê³¼:")
                for i, raw_result in enumerate(result["raw_results"][:3], 1):
                    with st.expander(f"ê²°ê³¼ {i} (ìœ ì‚¬ë„: {raw_result['similarity']:.3f})"):
                        st.write(f"**ì‹œíŠ¸**: {raw_result['sheet_name']}")
                        st.write(f"**ìœ í˜•**: {raw_result['type']}")
                        st.write(f"**ë‚´ìš©**: {raw_result['content']}")
                        if raw_result.get("metadata"):
                            st.write(f"**ë©”íƒ€ë°ì´í„°**: {raw_result['metadata']}")
    
    elif result["type"] == "file_info":
        st.subheader(result["title"])
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.dataframe(result["data"], width='stretch')
        
        with col2:
            st.metric("íŒŒì¼ ìˆ˜", result["summary"])
            st.info("ì²˜ë¦¬ëœ Excel íŒŒì¼ ì •ë³´")
    
    elif result["type"] == "no_files":
        st.subheader(result["title"])
        st.write(result["content"])
        st.info("ğŸ“¤ Excel íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    elif result["type"] == "general":
        st.subheader(result["title"])
        st.write(result["content"])
        
        if "suggestions" in result:
            st.write("ğŸ’¡ ì¶”ì²œ ì§ˆë¬¸:")
            for suggestion in result["suggestions"]:
                st.write(f"- {suggestion}")

if __name__ == "__main__":
    main()
