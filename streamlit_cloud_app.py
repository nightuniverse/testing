import streamlit as st
import pandas as pd
from pathlib import Path
import zipfile
import os
from PIL import Image
import io
import base64
import json
from datetime import datetime
import logging
import numpy as np
import hashlib
import random
import requests

# OpenAI package import attempt (Streamlit Cloud compatibility)
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    openai = None
    OPENAI_AVAILABLE = False
    st.warning("âš ï¸ Cannot load OpenAI package. Running in simulation mode.")

# Page configuration
st.set_page_config(
    page_title="Manufacturing Excel VLM System - Cloud",
    page_icon="ğŸ­",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Logging configuration
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# LLM API configuration
GPT_OSS_API_KEY = "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"
GPT_OSS_BASE_URL = "https://api.openai.com/v1"  # Change to actual API endpoint

# Get API key from environment variables (for security)
import os
if os.getenv("OPENAI_API_KEY"):
    GPT_OSS_API_KEY = os.getenv("OPENAI_API_KEY")

class LLMIntegration:
    """LLM Model Integration Class"""
    
    def __init__(self):
        self.gpt_oss_client = None
        self.qwen3_client = None
        self.initialize_llm_clients()
    
    def initialize_llm_clients(self):
        """Initialize LLM clients"""
        try:
            # Check OpenAI package availability
            if not OPENAI_AVAILABLE:
                logger.warning("âš ï¸ Cannot use OpenAI package. Switching to simulation mode.")
                self.gpt_oss_client = None
                return
            
            # API key validation
            if not GPT_OSS_API_KEY or GPT_OSS_API_KEY.startswith("sk-or-v1-"):
                logger.warning("âš ï¸ Invalid API key format. Switching to simulation mode.")
                self.gpt_oss_client = None
                return
            
            # Initialize GPT OSS 120B client
            self.gpt_oss_client = openai.OpenAI(
                api_key=GPT_OSS_API_KEY,
                base_url=GPT_OSS_BASE_URL
            )
            
            # Simple API test
            try:
                response = self.gpt_oss_client.models.list()
                logger.info("âœ… GPT OSS 120B client initialization and connection test completed")
            except Exception as test_error:
                logger.warning(f"âš ï¸ API connection test failed: {test_error}")
                self.gpt_oss_client = None
                
        except Exception as e:
            logger.error(f"âŒ GPT OSS client initialization failed: {e}")
            self.gpt_oss_client = None
    
    def analyze_image_with_gpt_oss(self, image, prompt):
        """GPT OSS 120Bë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë¶„ì„"""
        try:
            if not self.gpt_oss_client:
                return {"error": "GPT OSS í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
            
            # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
            img_buffer = io.BytesIO()
            image.save(img_buffer, format='PNG')
            img_base64 = base64.b64encode(img_buffer.getvalue()).decode()
            
            # GPT OSS Vision API í˜¸ì¶œ
            response = self.gpt_oss_client.chat.completions.create(
                model="gpt-4o",  # Change to actual model name
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{img_base64}"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=1000,
                temperature=0.3
            )
            
            return {
                "success": True,
                "analysis": response.choices[0].message.content,
                "model": "GPT OSS 120B"
            }
            
        except Exception as e:
            logger.error(f"GPT OSS image analysis failed: {e}")
            return {"error": str(e)}
    
    def generate_text_with_gpt_oss(self, prompt, context=""):
        """Generate text using GPT OSS 120B"""
        try:
            if not self.gpt_oss_client:
                return {"error": "GPT OSS client not initialized."}
            
            full_prompt = f"{context}\n\n{prompt}" if context else prompt
            
            response = self.gpt_oss_client.chat.completions.create(
                model="gpt-4o",  # Change to actual model name
                messages=[
                    {"role": "user", "content": full_prompt}
                ],
                max_tokens=1000,
                temperature=0.3
            )
            
            return {
                "success": True,
                "text": response.choices[0].message.content,
                "model": "GPT OSS 120B"
            }
            
        except Exception as e:
            logger.error(f"GPT OSS text generation failed: {e}")
            return {"error": str(e)}
    
    def analyze_image_with_qwen3(self, image, prompt):
        """Analyze image using Qwen3 open source model"""
        try:
            # Qwen3 API call (change to actual endpoint)
            qwen3_url = "https://api.qwen.ai/v1/chat/completions"
            
            # Encode image to base64
            img_buffer = io.BytesIO()
            image.save(img_buffer, format='PNG')
            img_base64 = base64.b64encode(img_buffer.getvalue()).decode()
            
            headers = {
                "Authorization": f"Bearer {GPT_OSS_API_KEY}",  # Reuse API key
                "Content-Type": "application/json"
            }
            
            data = {
                "model": "qwen-vl-plus",  # Change to actual model name
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{img_base64}"
                                }
                            }
                        ]
                    }
                ],
                "max_tokens": 1000,
                "temperature": 0.3
            }
            
            response = requests.post(qwen3_url, headers=headers, json=data)
            
            if response.status_code == 200:
                result = response.json()
                return {
                    "success": True,
                    "analysis": result["choices"][0]["message"]["content"],
                    "model": "Qwen3"
                }
            else:
                return {"error": f"Qwen3 API error: {response.status_code}"}
                
        except Exception as e:
            logger.error(f"Qwen3 image analysis failed: {e}")
            return {"error": str(e)}
    
    def get_available_models(self):
        """Return list of available LLM models"""
        models = []
        
        # Check OpenAI package availability
        if OPENAI_AVAILABLE and self.gpt_oss_client:
            models.append("GPT OSS 120B")
        
        # Qwen3 is always available (API call attempt)
        models.append("Qwen3")
        
        # If no actual models available, show simulation only
        if not models:
            models.append("Simulation")
        
        return models

class CloudVLMSystem:
    def __init__(self):
        self.excel_files = []
        self.processed_data = {}
        self.extracted_images = {}
        self.vector_database = None
        self.text_chunks = []
        self.embeddings = []
        self.embedding_model = None
        
        # Auto question generation
        self.auto_questions = []
        
        # VLM image analysis
        self.image_analysis = {}
        
        # LLM integration
        self.llm_integration = LLMIntegration()
        
        self.initialize_system()
    
    def initialize_system(self):
        """Initialize system"""
        try:
            # Streamlit Cloud cannot access local files
            # Don't generate default images, wait for uploaded files
            self.extracted_images = {}  # Initialize with empty image dictionary
            
            # Initialize basic data
            self.processed_data = {
                "System Information": {
                    "type": "system",
                    "content": "Upload Excel files to process data.",
                    "features": ["File Upload", "Image Extraction", "Data Analysis"]
                }
            }
            
            return True
        except Exception as e:
            st.error(f"âŒ Error during system initialization: {str(e)}")
            return False
    
    def extract_images_from_excel(self):
        """Extract images from Excel file (no longer used)"""
        # Streamlit Cloud cannot access local files
        # Only uploaded files can be processed
        logger.info("Cannot access local Excel files - only uploaded files can be processed")
        self.create_default_images()
    
    def extract_images_from_uploaded_file(self, uploaded_file):
        """Extract images from uploaded Excel file"""
        try:
            # Save uploaded file temporarily
            with open("temp_excel.xlsx", "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            # Open Excel file as ZIP
            with zipfile.ZipFile("temp_excel.xlsx", 'r') as zip_file:
                # Find image files
                image_files = [f for f in zip_file.namelist() if f.startswith('xl/media/')]
                
                extracted_count = 0
                for image_file in image_files:
                    try:
                        # Read image file
                        with zip_file.open(image_file) as img_file:
                            img_data = img_file.read()
                            img = Image.open(io.BytesIO(img_data))
                            
                            # Extract image name
                            img_name = os.path.basename(image_file)
                            img_name_without_ext = os.path.splitext(img_name)[0]
                            
                            # Save image
                            self.extracted_images[img_name_without_ext] = img
                            extracted_count += 1
                            
                    except Exception as e:
                        logger.error(f"Image extraction failed {image_file}: {e}")
                
                # Delete temporary file
                if os.path.exists("temp_excel.xlsx"):
                    os.remove("temp_excel.xlsx")
                
                if extracted_count > 0:
                    # Analyze images using VLM
                    logger.info(f"VLM image analysis started: {extracted_count} images")
                    self._analyze_images_with_vlm()
                
                return extracted_count
                
        except Exception as e:
            logger.error(f"Failed to extract images from uploaded Excel: {e}")
            return 0
    
    def process_uploaded_excel_data(self, uploaded_file):
        """Parse uploaded Excel file in docling style and build vector database"""
        try:
            # Save uploaded file temporarily
            with open("temp_excel.xlsx", "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            # Step 1: Parse Excel file in docling style
            parsed_data = self._parse_excel_docling_style("temp_excel.xlsx")
            
            # Step 2: Create text chunks
            self.text_chunks = self._create_text_chunks(parsed_data)
            
            # Step 3: Load embedding model and generate vectors
            self._initialize_embedding_model()
            self.embeddings = self._generate_embeddings(self.text_chunks)
            
            # Step 4: Build FAISS vector database
            self._build_vector_database()
            
            # Step 5: Generate auto questions
            self.auto_questions = self.generate_auto_questions("temp_excel.xlsx")
            
            # Step 6: Save processed data
            file_name = uploaded_file.name
            self.processed_data[file_name] = {
                "type": "excel_file",
                "content": f"Excel file: {file_name}",
                "parsed_data": parsed_data,
                "chunks_count": len(self.text_chunks),
                "vector_db_size": len(self.embeddings),
                "auto_questions_count": len(self.auto_questions),
                "file_info": {
                    "name": file_name,
                    "size": len(uploaded_file.getbuffer()),
                    "uploaded": datetime.now()
                }
            }
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists("temp_excel.xlsx"):
                os.remove("temp_excel.xlsx")
            
            logger.info(f"Excel íŒŒì¼ docling íŒŒì‹± ë° ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ: {file_name}")
            return True
            
        except Exception as e:
            logger.error(f"Excel íŒŒì¼ docling íŒŒì‹± ì‹¤íŒ¨: {e}")
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if os.path.exists("temp_excel.xlsx"):
                os.remove("temp_excel.xlsx")
            return False
    
    def _parse_excel_docling_style(self, excel_file_path):
        """Excel íŒŒì¼ì„ docling ìŠ¤íƒ€ì¼ë¡œ íŒŒì‹±"""
        try:
            # Excel íŒŒì¼ ì½ê¸°
            df = pd.read_excel(excel_file_path, sheet_name=None)
            parsed_data = {
                "file_path": excel_file_path,
                "sheets": {},
                "metadata": {
                    "total_sheets": len(df),
                    "parsed_at": datetime.now().isoformat()
                }
            }
            
            for sheet_name, sheet_df in df.items():
                # ì‹œíŠ¸ë³„ ë°ì´í„° íŒŒì‹±
                sheet_data = self._parse_sheet_content(sheet_name, sheet_df)
                parsed_data["sheets"][sheet_name] = sheet_data
            
            return parsed_data
            
        except Exception as e:
            logger.error(f"Excel docling íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    def _parse_sheet_content(self, sheet_name, sheet_df):
        """ì‹œíŠ¸ ë‚´ìš©ì„ docling ìŠ¤íƒ€ì¼ë¡œ íŒŒì‹±"""
        try:
            # ê¸°ë³¸ ì •ë³´
            sheet_info = {
                "name": sheet_name,
                "dimensions": {
                    "rows": len(sheet_df),
                    "columns": len(sheet_df.columns)
                },
                "content": {}
            }
            
            # 1. í—¤ë” ì •ë³´ ì¶”ì¶œ
            if len(sheet_df) > 0:
                headers = sheet_df.columns.tolist()
                sheet_info["content"]["headers"] = headers
                
                # 2. ë°ì´í„° íƒ€ì… ë¶„ì„
                data_types = sheet_df.dtypes.to_dict()
                sheet_info["content"]["data_types"] = {str(k): str(v) for k, v in data_types.items()}
                
                # 3. í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ (docling ìŠ¤íƒ€ì¼)
                text_content = []
                
                # í—¤ë” í…ìŠ¤íŠ¸
                header_text = f"ì‹œíŠ¸ '{sheet_name}'ì˜ ì»¬ëŸ¼: {', '.join(headers)}"
                text_content.append(header_text)
                
                # ë°ì´í„° í–‰ í…ìŠ¤íŠ¸ (ì²˜ìŒ 10í–‰)
                for idx, row in sheet_df.head(10).iterrows():
                    row_text = f"í–‰ {idx+1}: {', '.join([f'{col}={val}' for col, val in row.items() if pd.notna(val)])}"
                    text_content.append(row_text)
                
                # 4. í…Œì´ë¸” êµ¬ì¡° ë¶„ì„
                if len(sheet_df) > 0:
                    # ìˆ«ì ì»¬ëŸ¼ê³¼ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ êµ¬ë¶„
                    numeric_cols = sheet_df.select_dtypes(include=[np.number]).columns.tolist()
                    text_cols = sheet_df.select_dtypes(include=['object']).columns.tolist()
                    
                    sheet_info["content"]["structure"] = {
                        "numeric_columns": numeric_cols,
                        "text_columns": text_cols,
                        "total_records": len(sheet_df)
                    }
                    
                    # ìˆ«ì ì»¬ëŸ¼ í†µê³„
                    if numeric_cols:
                        numeric_stats = {}
                        for col in numeric_cols:
                            col_data = sheet_df[col].dropna()
                            if len(col_data) > 0:
                                numeric_stats[col] = {
                                    "min": float(col_data.min()),
                                    "max": float(col_data.max()),
                                    "mean": float(col_data.mean()),
                                    "count": len(col_data)
                                }
                        sheet_info["content"]["numeric_stats"] = numeric_stats
                
                sheet_info["content"]["text_content"] = text_content
            
            return sheet_info
            
        except Exception as e:
            logger.error(f"ì‹œíŠ¸ íŒŒì‹± ì‹¤íŒ¨ {sheet_name}: {e}")
            return {"name": sheet_name, "error": str(e)}
    
    def _create_text_chunks(self, parsed_data):
        """íŒŒì‹±ëœ ë°ì´í„°ì—ì„œ ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„±"""
        chunks = []
        
        try:
            for sheet_name, sheet_data in parsed_data["sheets"].items():
                if "content" in sheet_data and "text_content" in sheet_data["content"]:
                    # ì‹œíŠ¸ë³„ ì²­í¬ ìƒì„±
                    sheet_chunk = {
                        "type": "sheet_overview",
                        "sheet_name": sheet_name,
                        "content": f"ì‹œíŠ¸ '{sheet_name}': {sheet_data['content']['text_content'][0]}",
                        "metadata": {
                            "rows": sheet_data["dimensions"]["rows"],
                            "columns": sheet_data["dimensions"]["columns"]
                        }
                    }
                    chunks.append(sheet_chunk)
                    
                    # ìƒì„¸ ë°ì´í„° ì²­í¬
                    for text_line in sheet_data["content"]["text_content"][1:]:
                        data_chunk = {
                            "type": "data_row",
                            "sheet_name": sheet_name,
                            "content": text_line,
                            "metadata": {"row_type": "data"}
                        }
                        chunks.append(data_chunk)
                    
                    # êµ¬ì¡° ì •ë³´ ì²­í¬
                    if "structure" in sheet_data["content"]:
                        structure = sheet_data["content"]["structure"]
                        structure_chunk = {
                            "type": "structure_info",
                            "sheet_name": sheet_name,
                            "content": f"ì‹œíŠ¸ '{sheet_name}' êµ¬ì¡°: ìˆ«ì ì»¬ëŸ¼ {len(structure['numeric_columns'])}, í…ìŠ¤íŠ¸ ì»¬ëŸ¼ {len(structure['text_columns'])}, ì´ {structure['total_records']}ê°œ ë ˆì½”ë“œ",
                            "metadata": structure
                        }
                        chunks.append(structure_chunk)
                        
                        # ìˆ«ì í†µê³„ ì²­í¬
                        if "numeric_stats" in sheet_data["content"]:
                            for col, stats in sheet_data["content"]["numeric_stats"].items():
                                stats_chunk = {
                                    "type": "numeric_stats",
                                    "sheet_name": sheet_name,
                                    "content": f"ì»¬ëŸ¼ '{col}' í†µê³„: ìµœì†Œê°’ {stats['min']}, ìµœëŒ€ê°’ {stats['max']}, í‰ê·  {stats['mean']}, ë°ì´í„° ìˆ˜ {stats['count']}",
                                    "metadata": {"column": col, "stats": stats}
                                }
                                chunks.append(stats_chunk)
            
            logger.info(f"ì´ {len(chunks)}ê°œì˜ í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„± ì™„ë£Œ")
            return chunks
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„± ì‹¤íŒ¨: {e}")
            return []
    
    def _initialize_embedding_model(self):
        """ê²½ëŸ‰í™”ëœ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            # Streamlit Cloud í™˜ê²½ì— ë§ê²Œ ê²½ëŸ‰í™”ëœ í•´ì‹œ ê¸°ë°˜ ì„ë² ë”© ì‚¬ìš©
            self.embedding_model = "hash_based"
            logger.info("ê²½ëŸ‰í™”ëœ í•´ì‹œ ê¸°ë°˜ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©")
        except Exception as e:
            logger.error(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.embedding_model = None
    
    def _generate_embeddings(self, text_chunks):
        """ì™„ì „íˆ ê²°ì •ì ì¸ í•´ì‹œ ê¸°ë°˜ ì„ë² ë”© ë²¡í„° ìƒì„±"""
        try:
            embeddings = []
            for chunk in text_chunks:
                # í•´ì‹œ ê¸°ë°˜ ë²¡í„° ìƒì„± (64ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ)
                text_content = chunk["content"]
                text_hash = hashlib.md5(text_content.encode('utf-8')).hexdigest()
                
                # ì™„ì „íˆ ê²°ì •ì ì¸ ë²¡í„° ìƒì„± (random.seed ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
                vector = self._hash_to_vector(text_hash, 64)
                
                # ì •ê·œí™”
                vector = vector / np.linalg.norm(vector)
                embeddings.append(vector)
            
            logger.info(f"ì™„ì „íˆ ê²°ì •ì ì¸ í•´ì‹œ ê¸°ë°˜ ì„ë² ë”© {len(embeddings)}ê°œ ìƒì„± ì™„ë£Œ")
            return embeddings
            
        except Exception as e:
            logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            # fallback: ê²°ì •ì ì¸ ë²¡í„°
            embeddings = []
            for i, chunk in enumerate(text_chunks):
                text_hash = hashlib.md5(f"fallback_{i}".encode('utf-8')).hexdigest()
                vector = self._hash_to_vector(text_hash, 64)
                vector = vector / np.linalg.norm(vector)
                embeddings.append(vector)
            logger.info(f"ê²°ì •ì ì¸ fallback ë²¡í„° {len(embeddings)}ê°œ ìƒì„± ì™„ë£Œ")
            return embeddings
    
    def _hash_to_vector(self, text_hash, dimensions):
        """í•´ì‹œë¥¼ ê²°ì •ì ì¸ ë²¡í„°ë¡œ ë³€í™˜"""
        vector = np.zeros(dimensions, dtype='float32')
        
        # í•´ì‹œì˜ ê° ë¬¸ìë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ì—¬ ë²¡í„° ìƒì„±
        for i in range(dimensions):
            # í•´ì‹œì—ì„œ ìˆœí™˜í•˜ë©´ì„œ ê°’ì„ ì¶”ì¶œ
            hash_idx = i % len(text_hash)
            char_val = ord(text_hash[hash_idx])
            
            # ë¬¸ì ê°’ì„ -1ì—ì„œ 1 ì‚¬ì´ë¡œ ì •ê·œí™”
            normalized_val = (char_val - 48) / 122.0  # 48(0) ~ 122(z) ë²”ìœ„ë¥¼ -1~1ë¡œ
            vector[i] = normalized_val
        
        return vector
    
    def _build_vector_database(self):
        """ê²½ëŸ‰í™”ëœ Python ê¸°ë°˜ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•"""
        try:
            if len(self.embeddings) == 0:
                logger.warning("ì„ë² ë”©ì´ ì—†ì–´ ë²¡í„° DB êµ¬ì¶• ë¶ˆê°€")
                return
            
            # ë²¡í„° ì°¨ì› í™•ì¸
            vector_dim = self.embeddings[0].shape[0]
            
            # ìˆœìˆ˜ Pythonìœ¼ë¡œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
            self.vector_database = {
                "vectors": np.array(self.embeddings),
                "dimension": vector_dim,
                "count": len(self.embeddings)
            }
            
            logger.info(f"ê²½ëŸ‰í™”ëœ Python ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(self.embeddings)}ê°œ ë²¡í„°, {vector_dim}ì°¨ì›")
            
        except Exception as e:
            logger.error(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨: {e}")
            self.vector_database = None
    
    def create_default_images(self):
        """ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„± (ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)"""
        # ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±í•˜ì§€ ì•ŠìŒ
        # Excelì—ì„œ ì¶”ì¶œëœ ì‹¤ì œ ì´ë¯¸ì§€ë§Œ ì‚¬ìš©
        logger.info("ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„± ë¹„í™œì„±í™” - Excel ì´ë¯¸ì§€ë§Œ ì‚¬ìš©")
        pass
    
    def create_quality_inspection_image(self):
        """í’ˆì§ˆê²€ì‚¬í‘œ ì´ë¯¸ì§€ ìƒì„±"""
        # 400x300 í¬ê¸°ì˜ ì´ë¯¸ì§€ ìƒì„±
        img = Image.new('RGB', (400, 300), color='white')
        
        # ê°„ë‹¨í•œ í’ˆì§ˆê²€ì‚¬í‘œ ê·¸ë¦¬ê¸°
        from PIL import ImageDraw, ImageFont
        
        draw = ImageDraw.Draw(img)
        
        # ì œëª©
        draw.text((20, 20), "í’ˆì§ˆê²€ì‚¬í‘œ", fill='black')
        draw.line([(20, 50), (380, 50)], fill='black', width=2)
        
        # ê²€ì‚¬ í•­ëª©ë“¤
        items = [
            "1. ì™¸ê´€ ê²€ì‚¬",
            "2. ì¹˜ìˆ˜ ê²€ì‚¬", 
            "3. ê¸°ëŠ¥ ê²€ì‚¬",
            "4. ë‚´êµ¬ì„± ê²€ì‚¬"
        ]
        
        y_pos = 70
        for item in items:
            draw.text((30, y_pos), item, fill='blue')
            y_pos += 30
        
        # í•©ê²©/ë¶ˆí•©ê²© ì²´í¬ë°•ìŠ¤
        draw.text((200, 70), "â–¡ í•©ê²©", fill='green')
        draw.text((200, 100), "â–¡ ë¶ˆí•©ê²©", fill='red')
        
        return img
    
    def create_assembly_process_image(self):
        """ì¡°ë¦½ê³µì •ë„ ì´ë¯¸ì§€ ìƒì„±"""
        # 400x300 í¬ê¸°ì˜ ì´ë¯¸ì§€ ìƒì„±
        img = Image.new('RGB', (400, 300), color='lightblue')
        
        from PIL import ImageDraw
        
        draw = ImageDraw.Draw(img)
        
        # ì œëª©
        draw.text((20, 20), "ì¡°ë¦½ê³µì •ë„", fill='darkblue')
        draw.line([(20, 50), (380, 50)], fill='darkblue', width=2)
        
        # ê³µì • íë¦„ë„ ê·¸ë¦¬ê¸°
        processes = [
            "ìˆ˜ì…ê²€ì‚¬",
            "ì´ì˜¤ë‚˜ì´ì €",
            "DINO ê²€ì‚¬", 
            "CU+SPONGE",
            "ë„ì „ TAPE",
            "ì¶œí•˜ê²€ì‚¬",
            "í¬ì¥"
        ]
        
        x_pos = 30
        y_pos = 80
        for i, process in enumerate(processes):
            # ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            draw.rectangle([x_pos, y_pos, x_pos+80, y_pos+40], outline='darkblue', width=2, fill='white')
            draw.text((x_pos+5, y_pos+10), process, fill='darkblue', size=8)
            
            # í™”ì‚´í‘œ ê·¸ë¦¬ê¸° (ë§ˆì§€ë§‰ ì œì™¸)
            if i < len(processes) - 1:
                draw.line([x_pos+80, y_pos+20, x_pos+100, y_pos+20], fill='darkblue', width=2)
                # í™”ì‚´í‘œ ë¨¸ë¦¬
                draw.polygon([(x_pos+100, y_pos+15), (x_pos+100, y_pos+25), (x_pos+110, y_pos+20)], fill='darkblue')
            
            x_pos += 100
            
            # ë‘ ë²ˆì§¸ ì¤„ë¡œ ë„˜ì–´ê°€ê¸°
            if x_pos > 350:
                x_pos = 30
                y_pos += 80
        
        return img
    
    def create_part_drawing_image(self):
        """ë¶€í’ˆë„ë©´ ì´ë¯¸ì§€ ìƒì„±"""
        # 400x300 í¬ê¸°ì˜ ì´ë¯¸ì§€ ìƒì„±
        img = Image.new('RGB', (400, 300), color='lightgreen')
        
        from PIL import ImageDraw
        
        draw = ImageDraw.Draw(img)
        
        # ì œëª©
        draw.text((20, 20), "ë¶€í’ˆë„ë©´ - FRONT DECO SUB", fill='darkgreen')
        draw.line([(20, 50), (380, 50)], fill='darkgreen', width=2)
        
        # ê°„ë‹¨í•œ ë„ë©´ ê·¸ë¦¬ê¸°
        # ì™¸ê³½ì„ 
        draw.rectangle([50, 80, 350, 250], outline='darkgreen', width=3)
        
        # ë‚´ë¶€ êµ¬ì¡°
        draw.rectangle([70, 100, 150, 180], outline='darkgreen', width=2, fill='white')
        draw.text((80, 120), "GATE", fill='darkgreen')
        
        draw.rectangle([170, 100, 250, 180], outline='darkgreen', width=2, fill='white')
        draw.text((180, 120), "SPONGE", fill='darkgreen')
        
        draw.rectangle([270, 100, 330, 180], outline='darkgreen', width=2, fill='white')
        draw.text((280, 120), "TAPE", fill='darkgreen')
        
        # ì¹˜ìˆ˜ì„ 
        draw.line([50, 260, 350, 260], fill='darkgreen', width=1)
        draw.text((200, 270), "300mm", fill='darkgreen')
        
        draw.line([370, 80, 370, 250], fill='darkgreen', width=1)
        draw.text((380, 165), "170mm", fill='darkgreen')
        
        return img
    
    def process_real_excel_data(self):
        """ì‹¤ì œ Excel íŒŒì¼ ë‚´ìš© ê¸°ë°˜ ë°ì´í„° ì²˜ë¦¬ (ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)"""
        # Streamlit Cloudì—ì„œëŠ” ë¡œì»¬ íŒŒì¼ ì ‘ê·¼ ë¶ˆê°€
        # ì—…ë¡œë“œëœ íŒŒì¼ë§Œ ì²˜ë¦¬ ê°€ëŠ¥
        logger.info("ë¡œì»¬ Excel íŒŒì¼ ì ‘ê·¼ ë¶ˆê°€ - ì—…ë¡œë“œëœ íŒŒì¼ë§Œ ì²˜ë¦¬ ê°€ëŠ¥")
        self.processed_data = {
            "ì‹œìŠ¤í…œ ì •ë³´": {
                "type": "system",
                "content": "Excel íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "features": ["íŒŒì¼ ì—…ë¡œë“œ", "ì´ë¯¸ì§€ ì¶”ì¶œ", "ë°ì´í„° ë¶„ì„"]
            }
        }
    
    def query_system(self, query):
        """ê°œì„ ëœ ì¿¼ë¦¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ"""
        query_lower = query.lower()
        
        # 1. ì´ë¯¸ì§€ ê´€ë ¨ ì§ˆë¬¸ (ìµœìš°ì„ )
        if any(keyword in query_lower for keyword in ["ì´ë¯¸ì§€", "ì‚¬ì§„", "ê·¸ë¦¼", "ë³´ì—¬", "ì¶œë ¥", "ì¡°ë¦½ë„", "ë„ë©´"]):
            image_result = self.get_image_data(query)
            if image_result and image_result.get("type") != "no_image":
                return image_result
        
        # 2. Excel íŒŒì¼ ì •ë³´ ìš”ì²­
        if "íŒŒì¼ ì •ë³´" in query_lower or "excel íŒŒì¼" in query_lower:
            return self.get_excel_file_info()
        
        # 3. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ (AI ê¸°ë°˜)
        if self.vector_database is not None and len(self.text_chunks) > 0:
            vector_results = self._vector_search_query(query)
            if vector_results:
                return vector_results
        
        # 4. Excel ë°ì´í„° ì§ì ‘ ê²€ìƒ‰ (fallback)
        excel_results = self._search_excel_data(query)
        if excel_results:
            return excel_results
        
        # 5. ì¼ë°˜ì ì¸ ì‘ë‹µ
        return self.get_general_response(query)
    
    def _search_excel_data(self, query):
        """Excel ë°ì´í„°ì—ì„œ ê²€ìƒ‰ (fallback)"""
        try:
            query_lower = query.lower()
            results = []
            
            for file_name, file_data in self.processed_data.items():
                if file_data.get("type") == "excel_file":
                    # íŒŒì‹±ëœ ë°ì´í„°ì—ì„œ ê²€ìƒ‰
                    if "parsed_data" in file_data:
                        parsed_data = file_data["parsed_data"]
                        for sheet_name, sheet_data in parsed_data.get("sheets", {}).items():
                            if "content" in sheet_data and "text_content" in sheet_data["content"]:
                                for text_line in sheet_data["content"]["text_content"]:
                                    if query_lower in text_line.lower():
                                        results.append({
                                            "file": file_name,
                                            "sheet": sheet_name,
                                            "content": text_line,
                                            "type": "text_match"
                                        })
                    
                    # ê¸°ì¡´ ì‹œíŠ¸ ì •ë³´ì—ì„œ ê²€ìƒ‰ (fallback)
                    for sheet_name, sheet_info in file_data.get("sheets", {}).items():
                        # ìƒ˜í”Œ ë°ì´í„°ì—ì„œ ê²€ìƒ‰
                        for row_data in sheet_info.get("sample_data", []):
                            for key, value in row_data.items():
                                if query_lower in str(value).lower():
                                    results.append({
                                        "file": file_name,
                                        "sheet": sheet_name,
                                        "data": row_data,
                                        "match": f"{key}: {value}",
                                        "type": "data_match"
                                    })
            
            if results:
                # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
                df_data = []
                for result in results:
                    if result.get("type") == "text_match":
                        df_data.append({
                            "íŒŒì¼ëª…": result["file"],
                            "ì‹œíŠ¸ëª…": result["sheet"],
                            "ë§¤ì¹­ ìœ í˜•": "í…ìŠ¤íŠ¸ ë§¤ì¹­",
                            "ë‚´ìš©": result["content"][:100] + "..." if len(result["content"]) > 100 else result["content"]
                        })
                    else:
                        df_data.append({
                            "íŒŒì¼ëª…": result["file"],
                            "ì‹œíŠ¸ëª…": result["sheet"],
                            "ë§¤ì¹­ ìœ í˜•": "ë°ì´í„° ë§¤ì¹­",
                            "ë§¤ì¹­ ë°ì´í„°": result["match"],
                            "ì „ì²´ ë°ì´í„°": str(result["data"])[:100] + "..." if len(str(result["data"])) > 100 else str(result["data"])
                        })
                
                df = pd.DataFrame(df_data)
                
                return {
                    "type": "excel_search",
                    "title": f"ğŸ” '{query}' ê²€ìƒ‰ ê²°ê³¼ (fallback)",
                    "data": df,
                    "summary": f"ì´ {len(results)}ê°œ ê²°ê³¼ ë°œê²¬",
                    "chart_type": "table"
                }
            
            return None
            
        except Exception as e:
            logger.error(f"Excel ë°ì´í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None
    
    def _vector_search_query(self, query):
        """ì™„ì „íˆ ê²°ì •ì ì¸ ë²¡í„° ê²€ìƒ‰ì„ í†µí•œ ì¿¼ë¦¬ ì²˜ë¦¬"""
        try:
            if self.vector_database is None or len(self.text_chunks) == 0:
                return None
            
            # ì¿¼ë¦¬ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜ (ê²°ì •ì )
            query_hash = hashlib.md5(query.encode('utf-8')).hexdigest()
            query_vector = self._hash_to_vector(query_hash, 64)
            query_vector = query_vector / np.linalg.norm(query_vector)
            
            # ìˆœìˆ˜ Pythonìœ¼ë¡œ ìœ ì‚¬ë„ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
            similarities = []
            vectors = self.vector_database["vectors"]
            
            for i, vector in enumerate(vectors):
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))
                similarities.append((similarity, i))
            
            # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬ (ìƒìœ„ 5ê°œ)
            similarities.sort(reverse=True)
            k = min(5, len(self.text_chunks))
            
            # ê²€ìƒ‰ ê²°ê³¼ ì •ë¦¬
            search_results = []
            for i, (similarity, idx) in enumerate(similarities[:k]):
                if idx < len(self.text_chunks):
                    chunk = self.text_chunks[idx]
                    search_results.append({
                        "rank": i + 1,
                        "similarity": float(similarity),
                        "content": chunk["content"],
                        "type": chunk["type"],
                        "sheet_name": chunk.get("sheet_name", "N/A"),
                        "metadata": chunk.get("metadata", {})
                    })
            
            if search_results:
                # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
                df_data = []
                for result in search_results:
                    df_data.append({
                        "ìˆœìœ„": result["rank"],
                        "ìœ ì‚¬ë„": f"{result['similarity']:.3f}",
                        "ì‹œíŠ¸": result["sheet_name"],
                        "ìœ í˜•": result["type"],
                        "ë‚´ìš©": result["content"][:100] + "..." if len(result["content"]) > 100 else result["content"]
                    })
                
                df = pd.DataFrame(df_data)
                
                return {
                    "type": "vector_search",
                    "title": f"ğŸ” AI ë²¡í„° ê²€ìƒ‰ ê²°ê³¼: '{query}'",
                    "data": df,
                    "summary": f"AI ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ {len(search_results)}ê°œ ê²°ê³¼ ë°œê²¬ (ìœ ì‚¬ë„ ê¸°ë°˜)",
                    "chart_type": "table",
                    "raw_results": search_results
                }
            
            return None
            
        except Exception as e:
            logger.error(f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None
    
    def get_excel_file_info(self):
        """Excel íŒŒì¼ ì •ë³´ ë°˜í™˜"""
        try:
            file_info = []
            for file_name, file_data in self.processed_data.items():
                if file_data.get("type") == "excel_file":
                    # íŒŒì‹±ëœ ë°ì´í„° ì •ë³´
                    parsed_info = file_data.get("parsed_data", {})
                    sheets_info = parsed_info.get("sheets", {})
                    
                    info = {
                        "íŒŒì¼ëª…": file_name,
                        "ì‹œíŠ¸ ìˆ˜": len(sheets_info),
                        "í…ìŠ¤íŠ¸ ì²­í¬ ìˆ˜": file_data.get("chunks_count", 0),
                        "ë²¡í„° DB í¬ê¸°": file_data.get("vector_db_size", 0),
                        "íŒŒì¼ í¬ê¸°": f"{file_data.get('file_info', {}).get('size', 0) / 1024:.1f} KB",
                        "ì—…ë¡œë“œ ì‹œê°„": str(file_data.get('file_info', {}).get('uploaded', 'N/A'))
                    }
                    file_info.append(info)
            
            if file_info:
                df = pd.DataFrame(file_info)
                return {
                    "type": "file_info",
                    "title": "ğŸ“ Excel íŒŒì¼ ì •ë³´ (ë²¡í„° DB í¬í•¨)",
                    "data": df,
                    "summary": f"ì´ {len(file_info)}ê°œ Excel íŒŒì¼, ë²¡í„° ê²€ìƒ‰ ê°€ëŠ¥",
                    "chart_type": "table"
                }
            else:
                return {
                    "type": "no_files",
                    "title": "ğŸ“ Excel íŒŒì¼ ì—†ìŒ",
                    "content": "ì²˜ë¦¬ëœ Excel íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
                }
                
        except Exception as e:
            logger.error(f"íŒŒì¼ ì •ë³´ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "type": "error",
                "title": "âŒ ì˜¤ë¥˜",
                "content": f"íŒŒì¼ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }
    
    def get_image_data(self, query):
        """ê°„ë‹¨í•˜ê³  íš¨ê³¼ì ì¸ ì´ë¯¸ì§€ ë§¤ì¹­ ì‹œìŠ¤í…œ"""
        query_lower = query.lower()
        
        # Excelì—ì„œ ì¶”ì¶œëœ ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ì•ˆë‚´
        if not self.extracted_images:
            return {
                "type": "no_image",
                "title": "ğŸ–¼ï¸ ì´ë¯¸ì§€ ì—†ìŒ",
                "content": "Excel íŒŒì¼ì—ì„œ ì¶”ì¶œëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € Excel íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  'ğŸ“¤ ì´ë¯¸ì§€ ì¶”ì¶œ' ë²„íŠ¼ì„ í´ë¦­í•´ì£¼ì„¸ìš”.",
                "available_images": [],
                "suggestions": ["Excel íŒŒì¼ ì—…ë¡œë“œ", "ğŸ“¤ ì´ë¯¸ì§€ ì¶”ì¶œ ë²„íŠ¼ í´ë¦­"]
            }
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ì‹œìŠ¤í…œ
        matched_images = []
        
        logger.info(f"ì´ë¯¸ì§€ ë§¤ì¹­ ì‹œì‘: ì§ˆë¬¸='{query}', ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë¯¸ì§€={list(self.extracted_images.keys())}")
        
        # ì¡°ë¦½ë„ ê´€ë ¨ ì§ˆë¬¸ (ê°€ì¥ êµ¬ì²´ì ì¸ í‚¤ì›Œë“œë¶€í„° ì²´í¬)
        if any(word in query_lower for word in ["ì¡°ë¦½ë„", "ì¡°ë¦½ê³µì •", "ì¡°ë¦½ì‘ì—…", "ì¡°ë¦½ê³¼ì •"]):
            logger.info("ì¡°ë¦½ë„ ê´€ë ¨ ì§ˆë¬¸ ê°ì§€")
            # ì¡°ë¦½ ê´€ë ¨ ì´ë¯¸ì§€ ì°¾ê¸° (image1~30 ìš°ì„ )
            for img_name, img in self.extracted_images.items():
                if "image" in img_name.lower():
                    try:
                        img_num = int(''.join(filter(str.isdigit, img_name)))
                        if img_num <= 30:  # ì¡°ë¦½ë„ëŠ” ë³´í†µ ì•ìª½ ì´ë¯¸ì§€
                            matched_images.append((img_name, img, f"ì¡°ë¦½ ê´€ë ¨ ì´ë¯¸ì§€ (ë²ˆí˜¸: {img_num})", 10))
                            logger.info(f"ì¡°ë¦½ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 10)")
                        else:
                            matched_images.append((img_name, img, f"ì¡°ë¦½ ê´€ë ¨ ì´ë¯¸ì§€ (ë²ˆí˜¸: {img_num})", 5))
                            logger.info(f"ì¡°ë¦½ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 5)")
                    except:
                        matched_images.append((img_name, img, "ì¡°ë¦½ ê´€ë ¨ ì´ë¯¸ì§€", 3))
                        logger.info(f"ì¡°ë¦½ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 3)")
        
        # ê²€ì‚¬ ê´€ë ¨ ì§ˆë¬¸ (í’ˆì§ˆ ê²€ì‚¬ ìš°ì„ )
        elif any(word in query_lower for word in ["ê²€ì‚¬", "í’ˆì§ˆ", "í…ŒìŠ¤íŠ¸", "í™•ì¸", "ê²€ìˆ˜"]):
            logger.info("ê²€ì‚¬ ê´€ë ¨ ì§ˆë¬¸ ê°ì§€")
            for img_name, img in self.extracted_images.items():
                if "image" in img_name.lower():
                    try:
                        img_num = int(''.join(filter(str.isdigit, img_name)))
                        if 20 <= img_num <= 50:  # ê²€ì‚¬ ê´€ë ¨ì€ ì¤‘ê°„ ì´ë¯¸ì§€
                            matched_images.append((img_name, img, f"ê²€ì‚¬ ê´€ë ¨ ì´ë¯¸ì§€ (ë²ˆí˜¸: {img_num})", 10))
                            logger.info(f"ê²€ì‚¬ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 10)")
                        else:
                            matched_images.append((img_name, img, f"ê²€ì‚¬ ê´€ë ¨ ì´ë¯¸ì§€ (ë²ˆí˜¸: {img_num})", 5))
                            logger.info(f"ê²€ì‚¬ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 5)")
                    except:
                        matched_images.append((img_name, img, "ê²€ì‚¬ ê´€ë ¨ ì´ë¯¸ì§€", 3))
                        logger.info(f"ê²€ì‚¬ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 3)")
        
        # ë¶€í’ˆ/ë„ë©´ ê´€ë ¨ ì§ˆë¬¸
        elif any(word in query_lower for word in ["ë¶€í’ˆ", "ë„ë©´", "ì„¤ê³„", "ì¹˜ìˆ˜", "BOM"]):
            logger.info("ë¶€í’ˆ/ë„ë©´ ê´€ë ¨ ì§ˆë¬¸ ê°ì§€")
            for img_name, img in self.extracted_images.items():
                if "image" in img_name.lower():
                    try:
                        img_num = int(''.join(filter(str.isdigit, img_name)))
                        if img_num <= 25:  # ë¶€í’ˆë„ë©´ì€ ì•ìª½ ì´ë¯¸ì§€
                            matched_images.append((img_name, img, f"ë¶€í’ˆ/ë„ë©´ ì´ë¯¸ì§€ (ë²ˆí˜¸: {img_num})", 10))
                            logger.info(f"ë¶€í’ˆ/ë„ë©´ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 10)")
                        else:
                            matched_images.append((img_name, img, f"ë¶€í’ˆ/ë„ë©´ ì´ë¯¸ì§€ (ë²ˆí˜¸: {img_num})", 5))
                            logger.info(f"ë¶€í’ˆ/ë„ë©´ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 5)")
                    except:
                        matched_images.append((img_name, img, "ë¶€í’ˆ/ë„ë©´ ì´ë¯¸ì§€", 3))
                        logger.info(f"ë¶€í’ˆ/ë„ë©´ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 3)")
        
        # ì œí’ˆ ê´€ë ¨ ì§ˆë¬¸
        elif any(word in query_lower for word in ["ì œí’ˆ", "ì•ˆì°©", "ìƒì„¸", "í´ë¡œì¦ˆì—…", "ì™„ì„±"]):
            logger.info("ì œí’ˆ ê´€ë ¨ ì§ˆë¬¸ ê°ì§€")
            # ì œí’ˆ ê´€ë ¨ ì´ë¯¸ì§€ ì°¾ê¸° (image40+ ìš°ì„ )
            for img_name, img in self.extracted_images.items():
                if "image" in img_name.lower():
                    try:
                        img_num = int(''.join(filter(str.isdigit, img_name)))
                        if img_num >= 40:  # ì œí’ˆ ê´€ë ¨ì€ ë’¤ìª½ ì´ë¯¸ì§€
                            matched_images.append((img_name, img, f"ì œí’ˆ ê´€ë ¨ ì´ë¯¸ì§€ (ë²ˆí˜¸: {img_num})", 10))
                            logger.info(f"ì œí’ˆ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 10)")
                        else:
                            matched_images.append((img_name, img, f"ì œí’ˆ ê´€ë ¨ ì´ë¯¸ì§€ (ë²ˆí˜¸: {img_num})", 5))
                            logger.info(f"ì œí’ˆ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 5)")
                    except:
                        matched_images.append((img_name, img, "ì œí’ˆ ê´€ë ¨ ì´ë¯¸ì§€", 3))
                        logger.info(f"ì œí’ˆ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 3)")
        
        # ì¼ë°˜ì ì¸ ì¡°ë¦½ ê´€ë ¨ ì§ˆë¬¸ (ë§ˆì§€ë§‰ì— ì²´í¬)
        elif any(word in query_lower for word in ["ì¡°ë¦½", "ê³µì •", "ì‘ì—…", "ê³¼ì •"]):
            logger.info("ì¼ë°˜ ì¡°ë¦½ ê´€ë ¨ ì§ˆë¬¸ ê°ì§€")
            # ì¡°ë¦½ ê´€ë ¨ ì´ë¯¸ì§€ ì°¾ê¸° (image1~30 ìš°ì„ )
            for img_name, img in self.extracted_images.items():
                if "image" in img_name.lower():
                    try:
                        img_num = int(''.join(filter(str.isdigit, img_name)))
                        if img_num <= 30:  # ì¡°ë¦½ë„ëŠ” ë³´í†µ ì•ìª½ ì´ë¯¸ì§€
                            matched_images.append((img_name, img, f"ì¡°ë¦½ ê´€ë ¨ ì´ë¯¸ì§€ (ë²ˆí˜¸: {img_num})", 10))
                            logger.info(f"ì¡°ë¦½ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 10)")
                        else:
                            matched_images.append((img_name, img, f"ì¡°ë¦½ ê´€ë ¨ ì´ë¯¸ì§€ (ë²ˆí˜¸: {img_num})", 5))
                            logger.info(f"ì¡°ë¦½ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 5)")
                    except:
                        matched_images.append((img_name, img, "ì¡°ë¦½ ê´€ë ¨ ì´ë¯¸ì§€", 3))
                        logger.info(f"ì¡°ë¦½ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 3)")
        

        
        # ì¼ë°˜ì ì¸ ì´ë¯¸ì§€ ìš”ì²­
        else:
            # ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ì ìˆ˜ì™€ í•¨ê»˜ ì¶”ê°€
            for img_name, img in self.extracted_images.items():
                matched_images.append((img_name, img, f"ì´ë¯¸ì§€: {img_name}", 1))
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        matched_images.sort(key=lambda x: x[3], reverse=True)
        
        logger.info(f"ì´ë¯¸ì§€ ë§¤ì¹­ ì™„ë£Œ: ì´ {len(matched_images)}ê°œ ë§¤ì¹­, ìƒìœ„ 3ê°œ: {[(name, score) for name, img, desc, score in matched_images[:3]]}")
        
        # ìµœê³  ì ìˆ˜ ì´ë¯¸ì§€ ë°˜í™˜
        if matched_images:
            best_img_name, best_img, best_desc, best_score = matched_images[0]
            
            logger.info(f"ìµœì  ì´ë¯¸ì§€ ì„ íƒ: {best_img_name} (ì ìˆ˜: {best_score})")
            
            # VLM ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì¶”ê°€ ì •ë³´ ì œê³µ
            vlm_analysis = None
            if hasattr(self, 'image_analysis') and best_img_name in self.image_analysis:
                vlm_analysis = self.image_analysis[best_img_name]
            
            result = {
                "type": "image",
                "title": f"ğŸ–¼ï¸ {best_img_name} - {query}",
                "image": best_img,
                "description": best_desc,
                "all_images": [(name, img, desc) for name, img, desc, score in matched_images[:3]],  # ìƒìœ„ 3ê°œ
                "query_info": f"ì§ˆë¬¸: '{query}'ì— ëŒ€í•œ ìµœì  ë§¤ì¹­ ì´ë¯¸ì§€ (ì ìˆ˜: {best_score})",
                "total_matches": len(matched_images)
            }
            
            # VLM ë¶„ì„ ê²°ê³¼ ì¶”ê°€
            if vlm_analysis and 'error' not in vlm_analysis:
                result["vlm_analysis"] = {
                    "summary": vlm_analysis['summary'],
                    "type": vlm_analysis['type'],
                    "tags": vlm_analysis['tags'],
                    "confidence": vlm_analysis['confidence'],
                    "details": vlm_analysis['details']
                }
            
            return result
        
        # ë§¤ì¹­ë˜ëŠ” ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ëª¨ë“  ì´ë¯¸ì§€ ëª©ë¡ í‘œì‹œ
        return {
            "type": "image_list",
            "title": "ğŸ–¼ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë¯¸ì§€ë“¤",
            "content": f"ì§ˆë¬¸ '{query}'ì— ë§ëŠ” ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒ ì´ë¯¸ì§€ë“¤ì´ ìˆìŠµë‹ˆë‹¤:",
            "available_images": list(self.extracted_images.keys()),
            "all_images": [(name, img, f"ì´ë¯¸ì§€: {name}") for name, img in self.extracted_images.items()],
            "suggestions": [
                "ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”",
                "ì˜ˆ: 'ì¡°ë¦½ ê³µì •ë„ë¥¼ ë³´ì—¬ì¤˜'",
                "ì˜ˆ: 'ì œí’ˆ ì•ˆì°© ì´ë¯¸ì§€ë¥¼ ë³´ì—¬ì¤˜'",
                "ì˜ˆ: 'í’ˆì§ˆ ê²€ì‚¬ ê³¼ì •ì„ ë³´ì—¬ì¤˜'"
            ]
        }
    
    def _analyze_images_with_vlm(self):
        """VLMì„ ì‚¬ìš©í•˜ì—¬ ì¶”ì¶œëœ ì´ë¯¸ì§€ë“¤ì„ ë¶„ì„"""
        try:
            logger.info("VLM ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘")
            
            # ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ ì €ì¥
            self.image_analysis = {}
            
            for img_name, img in self.extracted_images.items():
                try:
                    # VLM ë¶„ì„ ìˆ˜í–‰
                    analysis_result = self._analyze_single_image_with_vlm(img_name, img)
                    self.image_analysis[img_name] = analysis_result
                    
                    logger.info(f"ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ: {img_name} - {analysis_result['summary']}")
                    
                except Exception as e:
                    logger.error(f"ì´ë¯¸ì§€ {img_name} VLM ë¶„ì„ ì‹¤íŒ¨: {e}")
                    self.image_analysis[img_name] = {
                        "error": str(e),
                        "summary": "ë¶„ì„ ì‹¤íŒ¨",
                        "details": [],
                        "tags": []
                    }
            
            logger.info(f"VLM ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ: {len(self.image_analysis)}ê°œ ì´ë¯¸ì§€")
            
        except Exception as e:
            logger.error(f"VLM ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _analyze_single_image_with_vlm(self, img_name, img):
        """ë‹¨ì¼ ì´ë¯¸ì§€ë¥¼ VLMìœ¼ë¡œ ë¶„ì„"""
        try:
            # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ë¶„ì„
            img_info = {
                "name": img_name,
                "size": img.size,
                "mode": img.mode,
                "format": getattr(img, 'format', 'Unknown')
            }
            
            # ì‹¤ì œ LLMì„ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ë¶„ì„ ì‹œë„
            analysis_result = self._analyze_with_real_llm(img_name, img, img_info)
            
            # LLM ë¶„ì„ì´ ì‹¤íŒ¨í•˜ë©´ ì‹œë®¬ë ˆì´ì…˜ ì‚¬ìš©
            if not analysis_result or "error" in analysis_result:
                logger.warning(f"LLM ë¶„ì„ ì‹¤íŒ¨, ì‹œë®¬ë ˆì´ì…˜ ì‚¬ìš©: {img_name}")
                analysis_result = self._simulate_vlm_analysis(img_name, img_info)
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"ë‹¨ì¼ ì´ë¯¸ì§€ VLM ë¶„ì„ ì‹¤íŒ¨ {img_name}: {e}")
            raise
    
    def _simulate_vlm_analysis(self, img_name, img_info):
        """VLM ë¶„ì„ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ VLM ëª¨ë¸ë¡œ ëŒ€ì²´ ê°€ëŠ¥)"""
        try:
            # ì´ë¯¸ì§€ ì´ë¦„ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤ë§ˆíŠ¸ ë¶„ì„
            img_name_lower = img_name.lower()
            
            # ì´ë¯¸ì§€ ë²ˆí˜¸ ì¶”ì¶œ
            img_num = None
            if "image" in img_name_lower:
                try:
                    img_num = int(''.join(filter(str.isdigit, img_name)))
                except:
                    pass
            
            # ì´ë¯¸ì§€ ìœ í˜• ë¶„ë¥˜ ë° ë¶„ì„
            if img_num is not None:
                if img_num <= 30:
                    # ì¡°ë¦½/ê³µì • ê´€ë ¨ ì´ë¯¸ì§€
                    analysis = {
                        "type": "assembly_process",
                        "summary": f"ì¡°ë¦½ ê³µì • ì´ë¯¸ì§€ (ë²ˆí˜¸: {img_num})",
                        "details": [
                            f"ì´ë¯¸ì§€ {img_num}ì€ ì¡°ë¦½ ê³µì •ì˜ {img_num}ë²ˆì§¸ ë‹¨ê³„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤",
                            "ì‘ì—…ìê°€ ë¶€í’ˆì„ ì¡°ë¦½í•˜ëŠ” ê³¼ì •ì´ë‚˜ ê³µì • ë‹¨ê³„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤",
                            "ì¡°ë¦½ ì‘ì—…ì˜ í‘œì¤€í™”ëœ ì ˆì°¨ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤"
                        ],
                        "tags": ["ì¡°ë¦½", "ê³µì •", "ì‘ì—…", "ë‹¨ê³„", f"image{img_num}"],
                        "confidence": 0.85
                    }
                elif 31 <= img_num <= 50:
                    # ê²€ì‚¬/í’ˆì§ˆ ê´€ë ¨ ì´ë¯¸ì§€
                    analysis = {
                        "type": "quality_inspection",
                        "summary": f"í’ˆì§ˆ ê²€ì‚¬ ì´ë¯¸ì§€ (ë²ˆí˜¸: {img_num})",
                        "details": [
                            f"ì´ë¯¸ì§€ {img_num}ì€ í’ˆì§ˆ ê²€ì‚¬ ê³¼ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤",
                            "ì œí’ˆì˜ í’ˆì§ˆì„ í™•ì¸í•˜ê³  ê²€ì¦í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤",
                            "ê²€ì‚¬ ê¸°ì¤€ê³¼ ë°©ë²•ì„ ì‹œê°ì ìœ¼ë¡œ ì œì‹œí•©ë‹ˆë‹¤"
                        ],
                        "tags": ["ê²€ì‚¬", "í’ˆì§ˆ", "í…ŒìŠ¤íŠ¸", "í™•ì¸", f"image{img_num}"],
                        "confidence": 0.80
                    }
                else:
                    # ì œí’ˆ/ì™„ì„± ê´€ë ¨ ì´ë¯¸ì§€
                    analysis = {
                        "type": "product_final",
                        "summary": f"ì œí’ˆ ì™„ì„± ì´ë¯¸ì§€ (ë²ˆí˜¸: {img_num})",
                        "details": [
                            f"ì´ë¯¸ì§€ {img_num}ì€ ì™„ì„±ëœ ì œí’ˆì´ë‚˜ ìµœì¢… ìƒíƒœë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤",
                            "ì œí’ˆì˜ ìµœì¢… í˜•íƒœë‚˜ ì•ˆì°© ìƒíƒœë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤",
                            "ì¶œí•˜ ì „ ìµœì¢… ì ê²€ ê²°ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤"
                        ],
                        "tags": ["ì œí’ˆ", "ì™„ì„±", "ì•ˆì°©", "ìµœì¢…", f"image{img_num}"],
                        "confidence": 0.75
                    }
            else:
                # ì¼ë°˜ ì´ë¯¸ì§€
                analysis = {
                    "type": "general_image",
                    "summary": f"ì¼ë°˜ ì´ë¯¸ì§€: {img_name}",
                    "details": [
                        f"ì´ë¯¸ì§€ {img_name}ì€ ë¬¸ì„œì— í¬í•¨ëœ ì¼ë°˜ì ì¸ ì´ë¯¸ì§€ì…ë‹ˆë‹¤",
                        "êµ¬ì²´ì ì¸ ë‚´ìš©ì€ ì´ë¯¸ì§€ ìì²´ë¥¼ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤"
                    ],
                    "tags": ["ì´ë¯¸ì§€", "ì¼ë°˜", img_name],
                    "confidence": 0.60
                }
            
            # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            analysis["metadata"] = img_info
            analysis["analysis_method"] = "VLM_Simulation"
            
            return analysis
            
        except Exception as e:
            logger.error(f"VLM ë¶„ì„ ì‹œë®¬ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
            return {
                "type": "error",
                "summary": "ë¶„ì„ ì‹¤íŒ¨",
                "details": [f"ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"],
                "tags": ["ì˜¤ë¥˜", "ë¶„ì„ì‹¤íŒ¨"],
                "confidence": 0.0
            }
    
    def _analyze_with_real_llm(self, img_name, img, img_info):
        """ì‹¤ì œ LLMì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë¶„ì„"""
        try:
            # ì´ë¯¸ì§€ ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = f"""
            ì´ ì´ë¯¸ì§€({img_name})ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”. 
            
            ë‹¤ìŒ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”:
            1. ì´ë¯¸ì§€ê°€ ë³´ì—¬ì£¼ëŠ” ë‚´ìš© (ì¡°ë¦½ ê³µì •, í’ˆì§ˆ ê²€ì‚¬, ì œí’ˆ ë“±)
            2. ì´ë¯¸ì§€ì˜ ëª©ì ê³¼ ìš©ë„
            3. ì‘ì—…ìê°€ ì•Œì•„ì•¼ í•  í•µì‹¬ ì •ë³´
            4. ê´€ë ¨ëœ í‚¤ì›Œë“œë‚˜ íƒœê·¸
            
            í•œêµ­ì–´ë¡œ ìƒì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
            """
            
            # GPT OSS 120Bë¡œ ì´ë¯¸ì§€ ë¶„ì„ ì‹œë„
            gpt_result = self.llm_integration.analyze_image_with_gpt_oss(img, prompt)
            
            if gpt_result.get("success"):
                # GPT OSS ë¶„ì„ ê²°ê³¼ íŒŒì‹±
                analysis = self._parse_llm_analysis_result(gpt_result["analysis"], img_name, img_info)
                analysis["llm_model"] = "GPT OSS 120B"
                analysis["analysis_method"] = "Real_LLM"
                return analysis
            
            # GPT OSS ì‹¤íŒ¨ ì‹œ Qwen3 ì‹œë„
            qwen_result = self.llm_integration.analyze_image_with_qwen3(img, prompt)
            
            if qwen_result.get("success"):
                # Qwen3 ë¶„ì„ ê²°ê³¼ íŒŒì‹±
                analysis = self._parse_llm_analysis_result(qwen_result["analysis"], img_name, img_info)
                analysis["llm_model"] = "Qwen3"
                analysis["analysis_method"] = "Real_LLM"
                return analysis
            
            # ëª¨ë“  LLM ë¶„ì„ ì‹¤íŒ¨
            logger.warning(f"ëª¨ë“  LLM ë¶„ì„ ì‹¤íŒ¨: {img_name}")
            return None
            
        except Exception as e:
            logger.error(f"ì‹¤ì œ LLM ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨ {img_name}: {e}")
            return None
    
    def _parse_llm_analysis_result(self, llm_text, img_name, img_info):
        """LLM ë¶„ì„ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ íŒŒì‹±"""
        try:
            # ì´ë¯¸ì§€ ë²ˆí˜¸ ì¶”ì¶œ
            img_num = None
            if "image" in img_name.lower():
                try:
                    img_num = int(''.join(filter(str.isdigit, img_name)))
                except:
                    pass
            
            # LLM í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            text_lower = llm_text.lower()
            
            # ì´ë¯¸ì§€ ìœ í˜• ë¶„ë¥˜
            if any(word in text_lower for word in ["ì¡°ë¦½", "ê³µì •", "ì‘ì—…", "ë‹¨ê³„", "ê³¼ì •"]):
                img_type = "assembly_process"
                confidence = 0.90
            elif any(word in text_lower for word in ["ê²€ì‚¬", "í’ˆì§ˆ", "í…ŒìŠ¤íŠ¸", "í™•ì¸"]):
                img_type = "quality_inspection"
                confidence = 0.90
            elif any(word in text_lower for word in ["ì œí’ˆ", "ì™„ì„±", "ì•ˆì°©", "ìµœì¢…"]):
                img_type = "product_final"
                confidence = 0.90
            else:
                img_type = "general_image"
                confidence = 0.75
            
            # íƒœê·¸ ìƒì„±
            tags = []
            if img_num:
                tags.append(f"image{img_num}")
            
            # LLM í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œí•˜ì—¬ íƒœê·¸ ì¶”ê°€
            keywords = ["ì¡°ë¦½", "ê³µì •", "ì‘ì—…", "ê²€ì‚¬", "í’ˆì§ˆ", "í…ŒìŠ¤íŠ¸", "ì œí’ˆ", "ì™„ì„±", "ë¶€í’ˆ", "ë„ë©´"]
            for keyword in keywords:
                if keyword in text_lower:
                    tags.append(keyword)
            
            # ìƒì„¸ ë¶„ì„ì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
            details = []
            sentences = llm_text.split('.')
            for sentence in sentences:
                sentence = sentence.strip()
                if sentence and len(sentence) > 10:
                    details.append(sentence)
            
            # ìµœëŒ€ 5ê°œ ìƒì„¸ ì„¤ëª…ìœ¼ë¡œ ì œí•œ
            details = details[:5]
            
            analysis = {
                "type": img_type,
                "summary": f"LLM ë¶„ì„: {img_name}",
                "details": details,
                "tags": tags,
                "confidence": confidence,
                "metadata": img_info,
                "llm_raw_text": llm_text
            }
            
            return analysis
            
        except Exception as e:
            logger.error(f"LLM ë¶„ì„ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    def generate_auto_questions(self, excel_file_path):
        """Excel íŒŒì¼ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ìë™ìœ¼ë¡œ ì§ˆë¬¸ ìƒì„±"""
        try:
            # Excel íŒŒì¼ ì½ê¸°
            df = pd.read_excel(excel_file_path, sheet_name=None)
            
            questions = []
            
            # í‚¤ì›Œë“œ ì •ì˜
            process_keywords = {'ì¡°ë¦½', 'ê³µì •', 'ì‘ì—…', 'ê³¼ì •', 'ë‹¨ê³„', 'ìˆœì„œ', 'ì ˆì°¨', 'ë°©ë²•', 'ê¸°ìˆ '}
            quality_keywords = {'ê²€ì‚¬', 'í’ˆì§ˆ', 'í…ŒìŠ¤íŠ¸', 'í™•ì¸', 'ê²€ìˆ˜', 'ì ê²€', 'ì¸¡ì •', 'ê¸°ì¤€'}
            product_keywords = {'ì œí’ˆ', 'ì™„ì„±', 'ì¶œí•˜', 'í¬ì¥', 'ì•ˆì°©', 'ìƒì„¸', 'í´ë¡œì¦ˆì—…'}
            material_keywords = {'ë¶€í’ˆ', 'ìì¬', 'ì†Œì¬', 'ì¬ë£Œ', 'BOM', 'ë„ë©´', 'ì„¤ê³„', 'ì¹˜ìˆ˜'}
            equipment_keywords = {'ì¥ë¹„', 'ê¸°ê³„', 'ë„êµ¬', 'ì§€ê·¸', 'í˜„ë¯¸ê²½', 'ë Œì¦ˆ', 'ì¥ì¹˜'}
            
            # ì‹œíŠ¸ë³„ í‚¤ì›Œë“œ ë¶„ì„
            process_found = False
            quality_found = False
            material_found = False
            product_found = False
            equipment_found = False
            
            for sheet_name, sheet_df in df.items():
                if len(sheet_df) > 0:
                    # ì²˜ìŒ 5í–‰ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰
                    for idx, row in sheet_df.head(5).iterrows():
                        row_text = ' '.join([str(val) for val in row.values if pd.notna(val)]).lower()
                        
                        if not process_found and any(kw in row_text for kw in process_keywords):
                            questions.extend([
                                "ì¡°ë¦½ ê³µì •ë„ë¥¼ ë³´ì—¬ì¤˜",
                                "ì‘ì—… ê³¼ì •ì„ ì„¤ëª…í•´ì¤˜",
                                "ì¡°ë¦½ ë‹¨ê³„ë³„ ì´ë¯¸ì§€ë¥¼ ë³´ì—¬ì¤˜"
                            ])
                            process_found = True
                        
                        if not quality_found and any(kw in row_text for kw in quality_keywords):
                            questions.extend([
                                "í’ˆì§ˆ ê²€ì‚¬ ê³¼ì •ì„ ë³´ì—¬ì¤˜",
                                "ê²€ì‚¬ ê¸°ì¤€ì„ ì•Œë ¤ì¤˜",
                                "í…ŒìŠ¤íŠ¸ ë°©ë²•ì„ ë³´ì—¬ì¤˜"
                            ])
                            quality_found = True
                        
                        if not material_found and any(kw in row_text for kw in material_keywords):
                            questions.extend([
                                "ë¶€í’ˆ ë„ë©´ì„ ë³´ì—¬ì¤˜",
                                "BOM ì •ë³´ë¥¼ ì•Œë ¤ì¤˜",
                                "ìì¬ ëª…ì„¸ë¥¼ ë³´ì—¬ì¤˜"
                            ])
                            material_found = True
                        
                        if not product_found and any(kw in row_text for kw in product_keywords):
                            questions.extend([
                                "ì™„ì„±ëœ ì œí’ˆì„ ë³´ì—¬ì¤˜",
                                "ì œí’ˆ ì•ˆì°© ì´ë¯¸ì§€ë¥¼ ë³´ì—¬ì¤˜",
                                "ì¶œí•˜ ìƒíƒœë¥¼ ë³´ì—¬ì¤˜"
                            ])
                            product_found = True
                        
                        if not equipment_found and any(kw in row_text for kw in equipment_keywords):
                            questions.extend([
                                "ì‚¬ìš© ì¥ë¹„ë¥¼ ë³´ì—¬ì¤˜",
                                "ì‘ì—… ë„êµ¬ë¥¼ ì•Œë ¤ì¤˜",
                                "ì¸¡ì • ì¥ë¹„ë¥¼ ë³´ì—¬ì¤˜"
                            ])
                            equipment_found = True
                        
                        if len(questions) >= 12:  # ìµœëŒ€ 12ê°œ ì§ˆë¬¸
                            break
                    if len(questions) >= 12:
                        break
            
            # ì¼ë°˜ì ì¸ ì§ˆë¬¸ ì¶”ê°€
            questions.extend([
                "íŒŒì¼ ì •ë³´ë¥¼ ì•Œë ¤ì¤˜",
                "ì‹œíŠ¸ êµ¬ì¡°ë¥¼ ì„¤ëª…í•´ì¤˜",
                "ë°ì´í„° ìš”ì•½ì„ ë³´ì—¬ì¤˜"
            ])
            
            logger.info(f"ìë™ ì§ˆë¬¸ ìƒì„± ì™„ë£Œ: {len(questions)}ê°œ")
            return questions[:15]  # ìµœëŒ€ 15ê°œë¡œ ì œí•œ
            
        except Exception as e:
            logger.error(f"ìë™ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return [
                "íŒŒì¼ ì •ë³´ë¥¼ ì•Œë ¤ì¤˜",
                "ì´ë¯¸ì§€ë¥¼ ë³´ì—¬ì¤˜",
                "ë°ì´í„°ë¥¼ ìš”ì•½í•´ì¤˜"
            ]
    
    def get_general_response(self, query):
        """ì¼ë°˜ ì‘ë‹µ"""
        return {
            "type": "general",
            "title": "ğŸ’¡ ì¼ë°˜ ì •ë³´",
            "content": f"'{query}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.",
            "suggestions": [
                "ì¡°ë¦½ ê³µì •ì€ ì–´ë–¤ ê²ƒë“¤ì´ ìˆë‚˜ìš”?",
                "ì œí’ˆ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
                "ERP ì‹œìŠ¤í…œ ê¸°ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "í’ˆì§ˆ ê²€ì‚¬ ê¸°ì¤€ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "ì¡°ë¦½ ê³µì •ë„ ì´ë¯¸ì§€ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”"
            ]
        }

def main():
    st.title("ğŸ­ Manufacturing Excel VLM System - Cloud")
    st.markdown("---")
    
    # Sidebar
    with st.sidebar:
        st.header("ğŸ”§ System Configuration")
        
        # LLM Model Selection
        st.subheader("ğŸ¤– LLM Model Selection")
        available_models = st.session_state.system.llm_integration.get_available_models()
        
        if "selected_llm_model" not in st.session_state:
            st.session_state.selected_llm_model = available_models[0] if available_models else "Simulation"
        
        # Model options configuration
        model_options = []
        if "GPT OSS 120B" in available_models:
            model_options.append("GPT OSS 120B")
        if "Qwen3" in available_models:
            model_options.append("Qwen3")
        model_options.append("Simulation")
        
        selected_model = st.selectbox(
            "LLM Model for Analysis",
            options=model_options,
            index=model_options.index(st.session_state.selected_llm_model) if st.session_state.selected_llm_model in model_options else len(model_options) - 1,
            help="Select LLM model for image analysis"
        )
        
        if selected_model != st.session_state.selected_llm_model:
            st.session_state.selected_llm_model = selected_model
            st.rerun()
        
        # LLM Status Display
        if selected_model == "GPT OSS 120B":
            if not OPENAI_AVAILABLE:
                st.error("âŒ Cannot use OpenAI package. Package installation required.")
            elif st.session_state.system.llm_integration.gpt_oss_client:
                st.success("âœ… GPT OSS 120B model activated (API connected)")
            else:
                st.error("âŒ GPT OSS 120B model deactivated (API connection failed)")
        elif selected_model == "Qwen3":
            st.warning("âš ï¸ Qwen3 model (API connection test required)")
        else:
            st.info("â„¹ï¸ Simulation mode (No actual LLM usage)")
        
        # Package Status Display
        if not OPENAI_AVAILABLE:
            st.error("âŒ OpenAI package not installed. Check requirements.txt.")
        else:
            st.success("âœ… OpenAI package available")
        
        # API Key Status Display
        if GPT_OSS_API_KEY.startswith("sk-or-v1-"):
            st.warning("âš ï¸ Invalid API key format. Set OpenAI API key.")
        elif GPT_OSS_API_KEY:
            st.success("âœ… API key configured")
        else:
            st.error("âŒ API key not configured")
        
        if st.button("ğŸ”„ Reinitialize System", type="primary"):
            st.session_state.system = CloudVLMSystem()
            st.rerun()
        
        st.header("ğŸ“ Excel File Upload")
        st.write("Upload Excel files to extract images.")
        
        uploaded_file = st.file_uploader(
            "Select Excel File (.xlsx)",
            type=['xlsx'],
            help="Upload Excel file containing images"
        )
        
        if uploaded_file is not None:
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button("ğŸ“¤ Extract Images", type="primary"):
                    with st.spinner("Extracting images from Excel file..."):
                        extracted_count = st.session_state.system.extract_images_from_uploaded_file(uploaded_file)
                        if extracted_count > 0:
                            st.success(f"âœ… {extracted_count} images extracted successfully!")
                        else:
                            st.warning("âš ï¸ No images found in Excel file.")
                            st.info("ğŸ’¡ Please upload Excel file containing images.")
                        st.rerun()
            
            with col2:
                if st.button("ğŸ“Š Parse Data", type="secondary"):
                    with st.spinner("Parsing Excel file..."):
                        success = st.session_state.system.process_uploaded_excel_data(uploaded_file)
                        if success:
                            st.success("âœ… Excel data parsing completed!")
                        else:
                            st.warning("âš ï¸ Data parsing failed.")
                        st.rerun()
        
        st.header("ğŸ“Š Excel File Information")
        
        if st.button("ğŸ“ View File Information", key="btn_file_info"):
            st.session_state.query = "Show Excel file information"
            st.rerun()
        
        st.header("ğŸ“ Example Questions")
        
        # Display auto-generated questions
        if uploaded_file is not None and hasattr(st.session_state.system, 'auto_questions'):
            st.subheader("ğŸ¤– AI Auto-Generated Questions")
            for i, question in enumerate(st.session_state.system.auto_questions[:8], 1):  # Show top 8 only
                if st.button(f"{i}. {question}", key=f"btn_auto_{i}"):
                    st.session_state.query = question
                    st.rerun()
            
            if len(st.session_state.system.auto_questions) > 8:
                with st.expander(f"View More Questions ({len(st.session_state.system.auto_questions)})"):
                    for i, question in enumerate(st.session_state.system.auto_questions[8:], 9):
                        if st.button(f"{i}. {question}", key=f"btn_auto_{i}"):
                            st.session_state.query = question
                            st.rerun()
        else:
            # Default example questions
            example_questions = [
                "Show Excel file information",
                "What is BOM information?",
                "What materials are needed for product production?",
                "Show assembly process diagram image",
                "What are the quality inspection standards?"
            ]
            
            for question in example_questions:
                if st.button(question, key=f"btn_{question}"):
                    st.session_state.query = question
                    st.rerun()
    
    # Main Content
    if 'system' not in st.session_state:
        st.session_state.system = CloudVLMSystem()
    
    if 'query' not in st.session_state:
        st.session_state.query = ""
    
    # Display current extracted image information
    if st.session_state.system.extracted_images:
        st.info(f"ğŸ“¸ Currently {len(st.session_state.system.extracted_images)} images are loaded.")
        
        # Display VLM analysis results if available
        if hasattr(st.session_state.system, 'image_analysis') and st.session_state.system.image_analysis:
            st.success("ğŸ¤– VLM Image Analysis Completed!")
            with st.expander("ğŸ” VLM Image Analysis Results"):
                for img_name, analysis in st.session_state.system.image_analysis.items():
                    if 'error' not in analysis:
                        st.markdown(f"**{img_name}**")
                        st.write(f"ğŸ“ **Summary**: {analysis['summary']}")
                        st.write(f"ğŸ·ï¸ **Tags**: {', '.join(analysis['tags'])}")
                        st.write(f"ğŸ“Š **Confidence**: {analysis['confidence']:.2f}")
                        
                        # Display LLM model information
                        if "llm_model" in analysis:
                            st.write(f"ğŸ¤– **LLM Model**: {analysis['llm_model']}")
                            st.write(f"ğŸ”§ **Analysis Method**: {analysis['analysis_method']}")
                        
                        with st.expander("ğŸ“‹ Detailed Analysis"):
                            for detail in analysis['details']:
                                st.write(f"â€¢ {detail}")
                            
                            # Display LLM original text
                            if "llm_raw_text" in analysis:
                                st.write("---")
                                st.write("**ğŸ¤– LLM Original Analysis:**")
                                st.write(analysis['llm_raw_text'])
                        
                        st.divider()
                    else:
                        st.error(f"âŒ {img_name}: {analysis['error']}")
        
        with st.expander("ğŸ“‹ Loaded Image List"):
            for img_name in st.session_state.system.extracted_images.keys():
                st.write(f"- {img_name}")
    
    # Query Input
    query = st.text_input(
        "ğŸ” Enter your question:",
        value=st.session_state.query,
        placeholder="e.g., What assembly processes are there?"
    )
    
    if st.button("ğŸš€ Ask Question", type="primary") or st.session_state.query:
        if query:
            st.session_state.query = query
            with st.spinner("Processing your question..."):
                result = st.session_state.system.query_system(query)
                display_result(result)
        else:
            st.warning("Please enter a question.")

def display_result(result):
    """Display Results"""
    if result["type"] == "assembly":
        st.subheader(result["title"])
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.dataframe(result["data"], width='stretch')
        
        with col2:
            st.metric("Total Processes", result["summary"])
            st.info("SM-F741U Model Assembly Process Procedures")
    
    elif result["type"] == "product":
        st.subheader(result["title"])
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.dataframe(result["data"], width='stretch')
        
        with col2:
            st.metric("Model Name", result["summary"])
            st.info("Product Basic Information")
    
    elif result["type"] == "erp":
        st.subheader(result["title"])
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.dataframe(result["data"], width='stretch')
        
        with col2:
            st.metric("System", result["summary"])
            st.info("ERP System Functions")
    
    elif result["type"] == "quality":
        st.subheader(result["title"])
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.dataframe(result["data"], width='stretch')
        
        with col2:
            st.metric("Quality Management", result["summary"])
            st.info("Quality Inspection Standards and Procedures")
    
    elif result["type"] == "image":
        st.subheader(result["title"])
        
        # ì´ë¯¸ì§€ë¥¼ ë°”ì´íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ í‘œì‹œ
        img_byte_arr = io.BytesIO()
        result["image"].save(img_byte_arr, format='PNG')
        img_byte_arr = img_byte_arr.getvalue()
        
        st.image(img_byte_arr, caption=result["description"], width=400)
        
        # Display image information
        st.info(f"ğŸ“ Image Size: {result['image'].size[0]} x {result['image'].size[1]} pixels")
        
        # Display VLM analysis results
        if "vlm_analysis" in result:
            st.success("ğŸ¤– VLM Image Analysis Results")
            vlm = result["vlm_analysis"]
            st.write(f"**ğŸ“ Summary**: {vlm['summary']}")
            st.write(f"**ğŸ·ï¸ Type**: {vlm['type']}")
            st.write(f"**ğŸ”– Tags**: {', '.join(vlm['tags'])}")
            st.write(f"**ğŸ“Š Confidence**: {vlm['confidence']:.2f}")
            
            # Display LLM model information
            if "llm_model" in vlm:
                st.write(f"**ğŸ¤– LLM Model**: {vlm['llm_model']}")
                st.write(f"**ğŸ”§ Analysis Method**: {vlm['analysis_method']}")
            
            with st.expander("ğŸ“‹ Detailed Analysis"):
                for detail in vlm['details']:
                    st.write(f"â€¢ {detail}")
                
                # Display LLM original text
                if "llm_raw_text" in vlm:
                    st.write("---")
                    st.write("**ğŸ¤– LLM Original Analysis:**")
                    st.write(vlm['llm_raw_text'])
        
        # Display other matched images
        if "all_images" in result and len(result["all_images"]) > 1:
            st.write("ğŸ” Other Related Images:")
            for i, (img_name, img, desc) in enumerate(result["all_images"][1:], 1):
                with st.expander(f"{i}. {img_name}"):
                    img_byte_arr = io.BytesIO()
                    img.save(img_byte_arr, format='PNG')
                    img_byte_arr = img_byte_arr.getvalue()
                    st.image(img_byte_arr, caption=desc, width=300)
    
    elif result["type"] == "image_list":
        st.subheader(result["title"])
        st.write(result["content"])
        
        # Display available image list
        if "available_images" in result:
            st.write("ğŸ“‹ Available Images:")
            for img_name in result["available_images"]:
                st.write(f"- {img_name}")
        
        # Display all images
        if "all_images" in result:
            st.write("ğŸ–¼ï¸ All Images:")
            for i, (img_name, img, desc) in enumerate(result["all_images"], 1):
                with st.expander(f"{i}. {img_name}"):
                    img_byte_arr = io.BytesIO()
                    img.save(img_byte_arr, format='PNG')
                    img_byte_arr = img_byte_arr.getvalue()
                    st.image(img_byte_arr, caption=desc, width=300)
    
    elif result["type"] == "no_image":
        st.subheader(result["title"])
        st.write(result["content"])
        
        if "available_images" in result:
            st.write("ğŸ“‹ Available Images:")
            for img_name in result["available_images"]:
                st.write(f"- {img_name}")
    
    elif result["type"] == "excel_search":
        st.subheader(result["title"])
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.dataframe(result["data"], width='stretch')
        
        with col2:
            st.metric("Search Results", result["summary"])
            st.info("Data found in Excel file")
    
    elif result["type"] == "vector_search":
        st.subheader(result["title"])
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.dataframe(result["data"], width='stretch')
        
        with col2:
            st.metric("Vector Search Results", result["summary"])
            st.info("Similar content found using AI vector search")
            
            # Display detailed information
            if "raw_results" in result:
                st.write("ğŸ” Detailed Search Results:")
                for i, raw_result in enumerate(result["raw_results"][:3], 1):
                    with st.expander(f"Result {i} (Similarity: {raw_result['similarity']:.3f})"):
                        st.write(f"**Sheet**: {raw_result['sheet_name']}")
                        st.write(f"**Type**: {raw_result['type']}")
                        st.write(f"**Content**: {raw_result['content']}")
                        if raw_result.get("metadata"):
                            st.write(f"**Metadata**: {raw_result['metadata']}")
    
    elif result["type"] == "file_info":
        st.subheader(result["title"])
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.dataframe(result["data"], width='stretch')
        
        with col2:
            st.metric("File Count", result["summary"])
            st.info("Processed Excel file information")
    
    elif result["type"] == "no_files":
        st.subheader(result["title"])
        st.write(result["content"])
        st.info("ğŸ“¤ Upload Excel files to process data.")
    
    elif result["type"] == "general":
        st.subheader(result["title"])
        st.write(result["content"])
        
        if "suggestions" in result:
            st.write("ğŸ’¡ Recommended Questions:")
            for suggestion in result["suggestions"]:
                st.write(f"- {suggestion}")

if __name__ == "__main__":
    main()
