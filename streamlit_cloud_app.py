import streamlit as st
import pandas as pd
from pathlib import Path
import zipfile
import os
from PIL import Image
import io
import base64
import json
from datetime import datetime
import logging
import numpy as np
import hashlib
import random
import requests

# OpenAI íŒ¨í‚¤ì§€ import ì‹œë„ (Streamlit Cloud í˜¸í™˜ì„±)
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    openai = None
    OPENAI_AVAILABLE = False
    st.warning("âš ï¸ OpenAI íŒ¨í‚¤ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Test Excels VLM System - Cloud",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# LLM API ì„¤ì •
GPT_OSS_API_KEY = "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"
GPT_OSS_BASE_URL = "https://api.openai.com/v1"  # ì‹¤ì œ API ì—”ë“œí¬ì¸íŠ¸ë¡œ ë³€ê²½ í•„ìš”

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸° (ë³´ì•ˆì„ ìœ„í•´)
import os
if os.getenv("OPENAI_API_KEY"):
    GPT_OSS_API_KEY = os.getenv("OPENAI_API_KEY")

class LLMIntegration:
    """LLM ëª¨ë¸ ì—°ë™ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.gpt_oss_client = None
        self.qwen3_client = None
        self.initialize_llm_clients()
    
    def initialize_llm_clients(self):
        """LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            # OpenAI íŒ¨í‚¤ì§€ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            if not OPENAI_AVAILABLE:
                logger.warning("âš ï¸ OpenAI íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                self.gpt_oss_client = None
                return
            
            # API í‚¤ ìœ íš¨ì„± ê²€ì‚¬
            if not GPT_OSS_API_KEY or GPT_OSS_API_KEY.startswith("sk-or-v1-"):
                logger.warning("âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ API í‚¤ í˜•ì‹. ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                self.gpt_oss_client = None
                return
            
            # GPT OSS 120B í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            self.gpt_oss_client = openai.OpenAI(
                api_key=GPT_OSS_API_KEY,
                base_url=GPT_OSS_BASE_URL
            )
            
            # ê°„ë‹¨í•œ API í…ŒìŠ¤íŠ¸
            try:
                response = self.gpt_oss_client.models.list()
                logger.info("âœ… GPT OSS 120B í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ë° ì—°ê²° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            except Exception as test_error:
                logger.warning(f"âš ï¸ API ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {test_error}")
                self.gpt_oss_client = None
                
        except Exception as e:
            logger.error(f"âŒ GPT OSS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.gpt_oss_client = None
    
    def analyze_image_with_gpt_oss(self, image, prompt):
        """GPT OSS 120Bë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë¶„ì„"""
        try:
            if not self.gpt_oss_client:
                return {"error": "GPT OSS í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
            
            # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
            img_buffer = io.BytesIO()
            image.save(img_buffer, format='PNG')
            img_base64 = base64.b64encode(img_buffer.getvalue()).decode()
            
            # GPT OSS Vision API í˜¸ì¶œ
            response = self.gpt_oss_client.chat.completions.create(
                model="gpt-4o",  # ì‹¤ì œ ëª¨ë¸ëª…ìœ¼ë¡œ ë³€ê²½ í•„ìš”
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{img_base64}"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=1000,
                temperature=0.3
            )
            
            return {
                "success": True,
                "analysis": response.choices[0].message.content,
                "model": "GPT OSS 120B"
            }
            
        except Exception as e:
            logger.error(f"GPT OSS ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def generate_text_with_gpt_oss(self, prompt, context=""):
        """GPT OSS 120Bë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            if not self.gpt_oss_client:
                return {"error": "GPT OSS í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
            
            full_prompt = f"{context}\n\n{prompt}" if context else prompt
            
            response = self.gpt_oss_client.chat.completions.create(
                model="gpt-4o",  # ì‹¤ì œ ëª¨ë¸ëª…ìœ¼ë¡œ ë³€ê²½ í•„ìš”
                messages=[
                    {"role": "user", "content": full_prompt}
                ],
                max_tokens=1000,
                temperature=0.3
            )
            
            return {
                "success": True,
                "text": response.choices[0].message.content,
                "model": "GPT OSS 120B"
            }
            
        except Exception as e:
            logger.error(f"GPT OSS í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def analyze_image_with_qwen3(self, image, prompt):
        """Qwen3 ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë¶„ì„"""
        try:
            # Qwen3 API í˜¸ì¶œ (ì‹¤ì œ ì—”ë“œí¬ì¸íŠ¸ë¡œ ë³€ê²½ í•„ìš”)
            qwen3_url = "https://api.qwen.ai/v1/chat/completions"
            
            # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
            img_buffer = io.BytesIO()
            image.save(img_buffer, format='PNG')
            img_base64 = base64.b64encode(img_buffer.getvalue()).decode()
            
            headers = {
                "Authorization": f"Bearer {GPT_OSS_API_KEY}",  # API í‚¤ ì¬ì‚¬ìš©
                "Content-Type": "application/json"
            }
            
            data = {
                "model": "qwen-vl-plus",  # ì‹¤ì œ ëª¨ë¸ëª…ìœ¼ë¡œ ë³€ê²½ í•„ìš”
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{img_base64}"
                                }
                            }
                        ]
                    }
                ],
                "max_tokens": 1000,
                "temperature": 0.3
            }
            
            response = requests.post(qwen3_url, headers=headers, json=data)
            
            if response.status_code == 200:
                result = response.json()
                return {
                    "success": True,
                    "analysis": result["choices"][0]["message"]["content"],
                    "model": "Qwen3"
                }
            else:
                return {"error": f"Qwen3 API ì˜¤ë¥˜: {response.status_code}"}
                
        except Exception as e:
            logger.error(f"Qwen3 ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def get_available_models(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ LLM ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        models = []
        
        # OpenAI íŒ¨í‚¤ì§€ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        if OPENAI_AVAILABLE and self.gpt_oss_client:
            models.append("GPT OSS 120B")
        
        # Qwen3ëŠ” í•­ìƒ ì‚¬ìš© ê°€ëŠ¥ (API í˜¸ì¶œ ì‹œë„)
        models.append("Qwen3")
        
        # ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì‹œë®¬ë ˆì´ì…˜ë§Œ í‘œì‹œ
        if not models:
            models.append("ì‹œë®¬ë ˆì´ì…˜")
        
        return models

class CloudVLMSystem:
    def __init__(self):
        self.excel_files = []
        self.processed_data = {}
        self.extracted_images = {}
        self.vector_database = None
        self.text_chunks = []
        self.embeddings = []
        self.embedding_model = None
        
        # ìë™ ì§ˆë¬¸ ìƒì„± ê´€ë ¨
        self.auto_questions = []
        
        # VLM ì´ë¯¸ì§€ ë¶„ì„ ê´€ë ¨
        self.image_analysis = {}
        
        # LLM ì—°ë™ ê´€ë ¨
        self.llm_integration = LLMIntegration()
        
        self.initialize_system()
    
    def initialize_system(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            # Streamlit Cloudì—ì„œëŠ” ë¡œì»¬ íŒŒì¼ ì ‘ê·¼ ë¶ˆê°€
            # ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±í•˜ì§€ ì•Šê³  ì—…ë¡œë“œëœ íŒŒì¼ ì²˜ë¦¬ ëŒ€ê¸°
            self.extracted_images = {}  # ë¹ˆ ì´ë¯¸ì§€ ë”•ì…”ë„ˆë¦¬ë¡œ ì´ˆê¸°í™”
            
            # ê¸°ë³¸ ë°ì´í„° ì´ˆê¸°í™”
            self.processed_data = {
                "ì‹œìŠ¤í…œ ì •ë³´": {
                    "type": "system",
                    "content": "Excel íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                    "features": ["íŒŒì¼ ì—…ë¡œë“œ", "ì´ë¯¸ì§€ ì¶”ì¶œ", "ë°ì´í„° ë¶„ì„"]
                }
            }
            
            return True
        except Exception as e:
            st.error(f"âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return False
    
    def extract_images_from_excel(self):
        """Excel íŒŒì¼ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ (ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)"""
        # Streamlit Cloudì—ì„œëŠ” ë¡œì»¬ íŒŒì¼ ì ‘ê·¼ ë¶ˆê°€
        # ì—…ë¡œë“œëœ íŒŒì¼ë§Œ ì²˜ë¦¬ ê°€ëŠ¥
        logger.info("ë¡œì»¬ Excel íŒŒì¼ ì ‘ê·¼ ë¶ˆê°€ - ì—…ë¡œë“œëœ íŒŒì¼ë§Œ ì²˜ë¦¬ ê°€ëŠ¥")
        self.create_default_images()
    
    def extract_images_from_uploaded_file(self, uploaded_file):
        """ì—…ë¡œë“œëœ Excel íŒŒì¼ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ"""
        try:
            # ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œë¡œ ì €ì¥
            with open("temp_excel.xlsx", "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            # Excel íŒŒì¼ì„ ZIPìœ¼ë¡œ ì—´ê¸°
            with zipfile.ZipFile("temp_excel.xlsx", 'r') as zip_file:
                # ì´ë¯¸ì§€ íŒŒì¼ë“¤ ì°¾ê¸°
                image_files = [f for f in zip_file.namelist() if f.startswith('xl/media/')]
                
                extracted_count = 0
                for image_file in image_files:
                    try:
                        # ì´ë¯¸ì§€ íŒŒì¼ ì½ê¸°
                        with zip_file.open(image_file) as img_file:
                            img_data = img_file.read()
                            img = Image.open(io.BytesIO(img_data))
                            
                            # ì´ë¯¸ì§€ ì´ë¦„ ì¶”ì¶œ
                            img_name = os.path.basename(image_file)
                            img_name_without_ext = os.path.splitext(img_name)[0]
                            
                            # ì´ë¯¸ì§€ ì €ì¥
                            self.extracted_images[img_name_without_ext] = img
                            extracted_count += 1
                            
                    except Exception as e:
                        logger.error(f"ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨ {image_file}: {e}")
                
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                if os.path.exists("temp_excel.xlsx"):
                    os.remove("temp_excel.xlsx")
                
                if extracted_count > 0:
                    # VLMì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë¶„ì„
                    logger.info(f"VLM ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘: {extracted_count}ê°œ ì´ë¯¸ì§€")
                    self._analyze_images_with_vlm()
                
                return extracted_count
                
        except Exception as e:
            logger.error(f"ì—…ë¡œë“œëœ Excel ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return 0
    
    def process_uploaded_excel_data(self, uploaded_file):
        """ì—…ë¡œë“œëœ Excel íŒŒì¼ì„ docling ìŠ¤íƒ€ì¼ë¡œ íŒŒì‹±í•˜ê³  ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•"""
        try:
            # ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œë¡œ ì €ì¥
            with open("temp_excel.xlsx", "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            # 1ë‹¨ê³„: Excel íŒŒì¼ì„ docling ìŠ¤íƒ€ì¼ë¡œ íŒŒì‹±
            parsed_data = self._parse_excel_docling_style("temp_excel.xlsx")
            
            # 2ë‹¨ê³„: í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„±
            self.text_chunks = self._create_text_chunks(parsed_data)
            
            # 3ë‹¨ê³„: ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ë° ë²¡í„° ìƒì„±
            self._initialize_embedding_model()
            self.embeddings = self._generate_embeddings(self.text_chunks)
            
            # 4ë‹¨ê³„: FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
            self._build_vector_database()
            
            # 5ë‹¨ê³„: ìë™ ì§ˆë¬¸ ìƒì„±
            self.auto_questions = self.generate_auto_questions("temp_excel.xlsx")
            
            # 6ë‹¨ê³„: ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
            file_name = uploaded_file.name
            self.processed_data[file_name] = {
                "type": "excel_file",
                "content": f"Excel íŒŒì¼: {file_name}",
                "parsed_data": parsed_data,
                "chunks_count": len(self.text_chunks),
                "vector_db_size": len(self.embeddings),
                "auto_questions_count": len(self.auto_questions),
                "file_info": {
                    "name": file_name,
                    "size": len(uploaded_file.getbuffer()),
                    "uploaded": datetime.now()
                }
            }
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists("temp_excel.xlsx"):
                os.remove("temp_excel.xlsx")
            
            logger.info(f"Excel íŒŒì¼ docling íŒŒì‹± ë° ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ: {file_name}")
            return True
            
        except Exception as e:
            logger.error(f"Excel íŒŒì¼ docling íŒŒì‹± ì‹¤íŒ¨: {e}")
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if os.path.exists("temp_excel.xlsx"):
                os.remove("temp_excel.xlsx")
            return False
    
    def _parse_excel_docling_style(self, excel_file_path):
        """Excel íŒŒì¼ì„ docling ìŠ¤íƒ€ì¼ë¡œ íŒŒì‹±"""
        try:
            # Excel íŒŒì¼ ì½ê¸°
            df = pd.read_excel(excel_file_path, sheet_name=None)
            parsed_data = {
                "file_path": excel_file_path,
                "sheets": {},
                "metadata": {
                    "total_sheets": len(df),
                    "parsed_at": datetime.now().isoformat()
                }
            }
            
            for sheet_name, sheet_df in df.items():
                # ì‹œíŠ¸ë³„ ë°ì´í„° íŒŒì‹±
                sheet_data = self._parse_sheet_content(sheet_name, sheet_df)
                parsed_data["sheets"][sheet_name] = sheet_data
            
            return parsed_data
            
        except Exception as e:
            logger.error(f"Excel docling íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    def _parse_sheet_content(self, sheet_name, sheet_df):
        """ì‹œíŠ¸ ë‚´ìš©ì„ docling ìŠ¤íƒ€ì¼ë¡œ íŒŒì‹±"""
        try:
            # ê¸°ë³¸ ì •ë³´
            sheet_info = {
                "name": sheet_name,
                "dimensions": {
                    "rows": len(sheet_df),
                    "columns": len(sheet_df.columns)
                },
                "content": {}
            }
            
            # 1. í—¤ë” ì •ë³´ ì¶”ì¶œ
            if len(sheet_df) > 0:
                headers = sheet_df.columns.tolist()
                sheet_info["content"]["headers"] = headers
                
                # 2. ë°ì´í„° íƒ€ì… ë¶„ì„
                data_types = sheet_df.dtypes.to_dict()
                sheet_info["content"]["data_types"] = {str(k): str(v) for k, v in data_types.items()}
                
                # 3. í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ (docling ìŠ¤íƒ€ì¼)
                text_content = []
                
                # í—¤ë” í…ìŠ¤íŠ¸
                header_text = f"ì‹œíŠ¸ '{sheet_name}'ì˜ ì»¬ëŸ¼: {', '.join(headers)}"
                text_content.append(header_text)
                
                # ë°ì´í„° í–‰ í…ìŠ¤íŠ¸ (ì²˜ìŒ 10í–‰)
                for idx, row in sheet_df.head(10).iterrows():
                    row_text = f"í–‰ {idx+1}: {', '.join([f'{col}={val}' for col, val in row.items() if pd.notna(val)])}"
                    text_content.append(row_text)
                
                # 4. í…Œì´ë¸” êµ¬ì¡° ë¶„ì„
                if len(sheet_df) > 0:
                    # ìˆ«ì ì»¬ëŸ¼ê³¼ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ êµ¬ë¶„
                    numeric_cols = sheet_df.select_dtypes(include=[np.number]).columns.tolist()
                    text_cols = sheet_df.select_dtypes(include=['object']).columns.tolist()
                    
                    sheet_info["content"]["structure"] = {
                        "numeric_columns": numeric_cols,
                        "text_columns": text_cols,
                        "total_records": len(sheet_df)
                    }
                    
                    # ìˆ«ì ì»¬ëŸ¼ í†µê³„
                    if numeric_cols:
                        numeric_stats = {}
                        for col in numeric_cols:
                            col_data = sheet_df[col].dropna()
                            if len(col_data) > 0:
                                numeric_stats[col] = {
                                    "min": float(col_data.min()),
                                    "max": float(col_data.max()),
                                    "mean": float(col_data.mean()),
                                    "count": len(col_data)
                                }
                        sheet_info["content"]["numeric_stats"] = numeric_stats
                
                sheet_info["content"]["text_content"] = text_content
            
            return sheet_info
            
        except Exception as e:
            logger.error(f"ì‹œíŠ¸ íŒŒì‹± ì‹¤íŒ¨ {sheet_name}: {e}")
            return {"name": sheet_name, "error": str(e)}
    
    def _create_text_chunks(self, parsed_data):
        """íŒŒì‹±ëœ ë°ì´í„°ì—ì„œ ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„±"""
        chunks = []
        
        try:
            for sheet_name, sheet_data in parsed_data["sheets"].items():
                if "content" in sheet_data and "text_content" in sheet_data["content"]:
                    # ì‹œíŠ¸ë³„ ì²­í¬ ìƒì„±
                    sheet_chunk = {
                        "type": "sheet_overview",
                        "sheet_name": sheet_name,
                        "content": f"ì‹œíŠ¸ '{sheet_name}': {sheet_data['content']['text_content'][0]}",
                        "metadata": {
                            "rows": sheet_data["dimensions"]["rows"],
                            "columns": sheet_data["dimensions"]["columns"]
                        }
                    }
                    chunks.append(sheet_chunk)
                    
                    # ìƒì„¸ ë°ì´í„° ì²­í¬
                    for text_line in sheet_data["content"]["text_content"][1:]:
                        data_chunk = {
                            "type": "data_row",
                            "sheet_name": sheet_name,
                            "content": text_line,
                            "metadata": {"row_type": "data"}
                        }
                        chunks.append(data_chunk)
                    
                    # êµ¬ì¡° ì •ë³´ ì²­í¬
                    if "structure" in sheet_data["content"]:
                        structure = sheet_data["content"]["structure"]
                        structure_chunk = {
                            "type": "structure_info",
                            "sheet_name": sheet_name,
                            "content": f"ì‹œíŠ¸ '{sheet_name}' êµ¬ì¡°: ìˆ«ì ì»¬ëŸ¼ {len(structure['numeric_columns'])}, í…ìŠ¤íŠ¸ ì»¬ëŸ¼ {len(structure['text_columns'])}, ì´ {structure['total_records']}ê°œ ë ˆì½”ë“œ",
                            "metadata": structure
                        }
                        chunks.append(structure_chunk)
                        
                        # ìˆ«ì í†µê³„ ì²­í¬
                        if "numeric_stats" in sheet_data["content"]:
                            for col, stats in sheet_data["content"]["numeric_stats"].items():
                                stats_chunk = {
                                    "type": "numeric_stats",
                                    "sheet_name": sheet_name,
                                    "content": f"ì»¬ëŸ¼ '{col}' í†µê³„: ìµœì†Œê°’ {stats['min']}, ìµœëŒ€ê°’ {stats['max']}, í‰ê·  {stats['mean']}, ë°ì´í„° ìˆ˜ {stats['count']}",
                                    "metadata": {"column": col, "stats": stats}
                                }
                                chunks.append(stats_chunk)
            
            logger.info(f"ì´ {len(chunks)}ê°œì˜ í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„± ì™„ë£Œ")
            return chunks
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„± ì‹¤íŒ¨: {e}")
            return []
    
    def _initialize_embedding_model(self):
        """ê²½ëŸ‰í™”ëœ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            # Streamlit Cloud í™˜ê²½ì— ë§ê²Œ ê²½ëŸ‰í™”ëœ í•´ì‹œ ê¸°ë°˜ ì„ë² ë”© ì‚¬ìš©
            self.embedding_model = "hash_based"
            logger.info("ê²½ëŸ‰í™”ëœ í•´ì‹œ ê¸°ë°˜ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©")
        except Exception as e:
            logger.error(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.embedding_model = None
    
    def _generate_embeddings(self, text_chunks):
        """ì™„ì „íˆ ê²°ì •ì ì¸ í•´ì‹œ ê¸°ë°˜ ì„ë² ë”© ë²¡í„° ìƒì„±"""
        try:
            embeddings = []
            for chunk in text_chunks:
                # í•´ì‹œ ê¸°ë°˜ ë²¡í„° ìƒì„± (64ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ)
                text_content = chunk["content"]
                text_hash = hashlib.md5(text_content.encode('utf-8')).hexdigest()
                
                # ì™„ì „íˆ ê²°ì •ì ì¸ ë²¡í„° ìƒì„± (random.seed ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
                vector = self._hash_to_vector(text_hash, 64)
                
                # ì •ê·œí™”
                vector = vector / np.linalg.norm(vector)
                embeddings.append(vector)
            
            logger.info(f"ì™„ì „íˆ ê²°ì •ì ì¸ í•´ì‹œ ê¸°ë°˜ ì„ë² ë”© {len(embeddings)}ê°œ ìƒì„± ì™„ë£Œ")
            return embeddings
            
        except Exception as e:
            logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            # fallback: ê²°ì •ì ì¸ ë²¡í„°
            embeddings = []
            for i, chunk in enumerate(text_chunks):
                text_hash = hashlib.md5(f"fallback_{i}".encode('utf-8')).hexdigest()
                vector = self._hash_to_vector(text_hash, 64)
                vector = vector / np.linalg.norm(vector)
                embeddings.append(vector)
            logger.info(f"ê²°ì •ì ì¸ fallback ë²¡í„° {len(embeddings)}ê°œ ìƒì„± ì™„ë£Œ")
            return embeddings
    
    def _hash_to_vector(self, text_hash, dimensions):
        """í•´ì‹œë¥¼ ê²°ì •ì ì¸ ë²¡í„°ë¡œ ë³€í™˜"""
        vector = np.zeros(dimensions, dtype='float32')
        
        # í•´ì‹œì˜ ê° ë¬¸ìë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ì—¬ ë²¡í„° ìƒì„±
        for i in range(dimensions):
            # í•´ì‹œì—ì„œ ìˆœí™˜í•˜ë©´ì„œ ê°’ì„ ì¶”ì¶œ
            hash_idx = i % len(text_hash)
            char_val = ord(text_hash[hash_idx])
            
            # ë¬¸ì ê°’ì„ -1ì—ì„œ 1 ì‚¬ì´ë¡œ ì •ê·œí™”
            normalized_val = (char_val - 48) / 122.0  # 48(0) ~ 122(z) ë²”ìœ„ë¥¼ -1~1ë¡œ
            vector[i] = normalized_val
        
        return vector
    
    def _build_vector_database(self):
        """ê²½ëŸ‰í™”ëœ Python ê¸°ë°˜ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•"""
        try:
            if len(self.embeddings) == 0:
                logger.warning("ì„ë² ë”©ì´ ì—†ì–´ ë²¡í„° DB êµ¬ì¶• ë¶ˆê°€")
                return
            
            # ë²¡í„° ì°¨ì› í™•ì¸
            vector_dim = self.embeddings[0].shape[0]
            
            # ìˆœìˆ˜ Pythonìœ¼ë¡œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
            self.vector_database = {
                "vectors": np.array(self.embeddings),
                "dimension": vector_dim,
                "count": len(self.embeddings)
            }
            
            logger.info(f"ê²½ëŸ‰í™”ëœ Python ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(self.embeddings)}ê°œ ë²¡í„°, {vector_dim}ì°¨ì›")
            
        except Exception as e:
            logger.error(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨: {e}")
            self.vector_database = None
    
    def create_default_images(self):
        """ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„± (ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)"""
        # ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±í•˜ì§€ ì•ŠìŒ
        # Excelì—ì„œ ì¶”ì¶œëœ ì‹¤ì œ ì´ë¯¸ì§€ë§Œ ì‚¬ìš©
        logger.info("ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„± ë¹„í™œì„±í™” - Excel ì´ë¯¸ì§€ë§Œ ì‚¬ìš©")
        pass
    
    def create_quality_inspection_image(self):
        """í’ˆì§ˆê²€ì‚¬í‘œ ì´ë¯¸ì§€ ìƒì„±"""
        # 400x300 í¬ê¸°ì˜ ì´ë¯¸ì§€ ìƒì„±
        img = Image.new('RGB', (400, 300), color='white')
        
        # ê°„ë‹¨í•œ í’ˆì§ˆê²€ì‚¬í‘œ ê·¸ë¦¬ê¸°
        from PIL import ImageDraw, ImageFont
        
        draw = ImageDraw.Draw(img)
        
        # ì œëª©
        draw.text((20, 20), "í’ˆì§ˆê²€ì‚¬í‘œ", fill='black')
        draw.line([(20, 50), (380, 50)], fill='black', width=2)
        
        # ê²€ì‚¬ í•­ëª©ë“¤
        items = [
            "1. ì™¸ê´€ ê²€ì‚¬",
            "2. ì¹˜ìˆ˜ ê²€ì‚¬", 
            "3. ê¸°ëŠ¥ ê²€ì‚¬",
            "4. ë‚´êµ¬ì„± ê²€ì‚¬"
        ]
        
        y_pos = 70
        for item in items:
            draw.text((30, y_pos), item, fill='blue')
            y_pos += 30
        
        # í•©ê²©/ë¶ˆí•©ê²© ì²´í¬ë°•ìŠ¤
        draw.text((200, 70), "â–¡ í•©ê²©", fill='green')
        draw.text((200, 100), "â–¡ ë¶ˆí•©ê²©", fill='red')
        
        return img
    
    def create_assembly_process_image(self):
        """ì¡°ë¦½ê³µì •ë„ ì´ë¯¸ì§€ ìƒì„±"""
        # 400x300 í¬ê¸°ì˜ ì´ë¯¸ì§€ ìƒì„±
        img = Image.new('RGB', (400, 300), color='lightblue')
        
        from PIL import ImageDraw
        
        draw = ImageDraw.Draw(img)
        
        # ì œëª©
        draw.text((20, 20), "ì¡°ë¦½ê³µì •ë„", fill='darkblue')
        draw.line([(20, 50), (380, 50)], fill='darkblue', width=2)
        
        # ê³µì • íë¦„ë„ ê·¸ë¦¬ê¸°
        processes = [
            "ìˆ˜ì…ê²€ì‚¬",
            "ì´ì˜¤ë‚˜ì´ì €",
            "DINO ê²€ì‚¬", 
            "CU+SPONGE",
            "ë„ì „ TAPE",
            "ì¶œí•˜ê²€ì‚¬",
            "í¬ì¥"
        ]
        
        x_pos = 30
        y_pos = 80
        for i, process in enumerate(processes):
            # ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            draw.rectangle([x_pos, y_pos, x_pos+80, y_pos+40], outline='darkblue', width=2, fill='white')
            draw.text((x_pos+5, y_pos+10), process, fill='darkblue', size=8)
            
            # í™”ì‚´í‘œ ê·¸ë¦¬ê¸° (ë§ˆì§€ë§‰ ì œì™¸)
            if i < len(processes) - 1:
                draw.line([x_pos+80, y_pos+20, x_pos+100, y_pos+20], fill='darkblue', width=2)
                # í™”ì‚´í‘œ ë¨¸ë¦¬
                draw.polygon([(x_pos+100, y_pos+15), (x_pos+100, y_pos+25), (x_pos+110, y_pos+20)], fill='darkblue')
            
            x_pos += 100
            
            # ë‘ ë²ˆì§¸ ì¤„ë¡œ ë„˜ì–´ê°€ê¸°
            if x_pos > 350:
                x_pos = 30
                y_pos += 80
        
        return img
    
    def create_part_drawing_image(self):
        """ë¶€í’ˆë„ë©´ ì´ë¯¸ì§€ ìƒì„±"""
        # 400x300 í¬ê¸°ì˜ ì´ë¯¸ì§€ ìƒì„±
        img = Image.new('RGB', (400, 300), color='lightgreen')
        
        from PIL import ImageDraw
        
        draw = ImageDraw.Draw(img)
        
        # ì œëª©
        draw.text((20, 20), "ë¶€í’ˆë„ë©´ - FRONT DECO SUB", fill='darkgreen')
        draw.line([(20, 50), (380, 50)], fill='darkgreen', width=2)
        
        # ê°„ë‹¨í•œ ë„ë©´ ê·¸ë¦¬ê¸°
        # ì™¸ê³½ì„ 
        draw.rectangle([50, 80, 350, 250], outline='darkgreen', width=3)
        
        # ë‚´ë¶€ êµ¬ì¡°
        draw.rectangle([70, 100, 150, 180], outline='darkgreen', width=2, fill='white')
        draw.text((80, 120), "GATE", fill='darkgreen')
        
        draw.rectangle([170, 100, 250, 180], outline='darkgreen', width=2, fill='white')
        draw.text((180, 120), "SPONGE", fill='darkgreen')
        
        draw.rectangle([270, 100, 330, 180], outline='darkgreen', width=2, fill='white')
        draw.text((280, 120), "TAPE", fill='darkgreen')
        
        # ì¹˜ìˆ˜ì„ 
        draw.line([50, 260, 350, 260], fill='darkgreen', width=1)
        draw.text((200, 270), "300mm", fill='darkgreen')
        
        draw.line([370, 80, 370, 250], fill='darkgreen', width=1)
        draw.text((380, 165), "170mm", fill='darkgreen')
        
        return img
    
    def process_real_excel_data(self):
        """ì‹¤ì œ Excel íŒŒì¼ ë‚´ìš© ê¸°ë°˜ ë°ì´í„° ì²˜ë¦¬ (ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)"""
        # Streamlit Cloudì—ì„œëŠ” ë¡œì»¬ íŒŒì¼ ì ‘ê·¼ ë¶ˆê°€
        # ì—…ë¡œë“œëœ íŒŒì¼ë§Œ ì²˜ë¦¬ ê°€ëŠ¥
        logger.info("ë¡œì»¬ Excel íŒŒì¼ ì ‘ê·¼ ë¶ˆê°€ - ì—…ë¡œë“œëœ íŒŒì¼ë§Œ ì²˜ë¦¬ ê°€ëŠ¥")
        self.processed_data = {
            "ì‹œìŠ¤í…œ ì •ë³´": {
                "type": "system",
                "content": "Excel íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "features": ["íŒŒì¼ ì—…ë¡œë“œ", "ì´ë¯¸ì§€ ì¶”ì¶œ", "ë°ì´í„° ë¶„ì„"]
            }
        }
    
    def query_system(self, query):
        """ê°œì„ ëœ ì¿¼ë¦¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ"""
        query_lower = query.lower()
        
        # 1. ì´ë¯¸ì§€ ê´€ë ¨ ì§ˆë¬¸ (ìµœìš°ì„ )
        if any(keyword in query_lower for keyword in ["ì´ë¯¸ì§€", "ì‚¬ì§„", "ê·¸ë¦¼", "ë³´ì—¬", "ì¶œë ¥", "ì¡°ë¦½ë„", "ë„ë©´"]):
            image_result = self.get_image_data(query)
            if image_result and image_result.get("type") != "no_image":
                return image_result
        
        # 2. Excel íŒŒì¼ ì •ë³´ ìš”ì²­
        if "íŒŒì¼ ì •ë³´" in query_lower or "excel íŒŒì¼" in query_lower:
            return self.get_excel_file_info()
        
        # 3. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ (AI ê¸°ë°˜)
        if self.vector_database is not None and len(self.text_chunks) > 0:
            vector_results = self._vector_search_query(query)
            if vector_results:
                return vector_results
        
        # 4. Excel ë°ì´í„° ì§ì ‘ ê²€ìƒ‰ (fallback)
        excel_results = self._search_excel_data(query)
        if excel_results:
            return excel_results
        
        # 5. ì¼ë°˜ì ì¸ ì‘ë‹µ
        return self.get_general_response(query)
    
    def _search_excel_data(self, query):
        """Excel ë°ì´í„°ì—ì„œ ê²€ìƒ‰ (fallback)"""
        try:
            query_lower = query.lower()
            results = []
            
            for file_name, file_data in self.processed_data.items():
                if file_data.get("type") == "excel_file":
                    # íŒŒì‹±ëœ ë°ì´í„°ì—ì„œ ê²€ìƒ‰
                    if "parsed_data" in file_data:
                        parsed_data = file_data["parsed_data"]
                        for sheet_name, sheet_data in parsed_data.get("sheets", {}).items():
                            if "content" in sheet_data and "text_content" in sheet_data["content"]:
                                for text_line in sheet_data["content"]["text_content"]:
                                    if query_lower in text_line.lower():
                                        results.append({
                                            "file": file_name,
                                            "sheet": sheet_name,
                                            "content": text_line,
                                            "type": "text_match"
                                        })
                    
                    # ê¸°ì¡´ ì‹œíŠ¸ ì •ë³´ì—ì„œ ê²€ìƒ‰ (fallback)
                    for sheet_name, sheet_info in file_data.get("sheets", {}).items():
                        # ìƒ˜í”Œ ë°ì´í„°ì—ì„œ ê²€ìƒ‰
                        for row_data in sheet_info.get("sample_data", []):
                            for key, value in row_data.items():
                                if query_lower in str(value).lower():
                                    results.append({
                                        "file": file_name,
                                        "sheet": sheet_name,
                                        "data": row_data,
                                        "match": f"{key}: {value}",
                                        "type": "data_match"
                                    })
            
            if results:
                # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
                df_data = []
                for result in results:
                    if result.get("type") == "text_match":
                        df_data.append({
                            "íŒŒì¼ëª…": result["file"],
                            "ì‹œíŠ¸ëª…": result["sheet"],
                            "ë§¤ì¹­ ìœ í˜•": "í…ìŠ¤íŠ¸ ë§¤ì¹­",
                            "ë‚´ìš©": result["content"][:100] + "..." if len(result["content"]) > 100 else result["content"]
                        })
                    else:
                        df_data.append({
                            "íŒŒì¼ëª…": result["file"],
                            "ì‹œíŠ¸ëª…": result["sheet"],
                            "ë§¤ì¹­ ìœ í˜•": "ë°ì´í„° ë§¤ì¹­",
                            "ë§¤ì¹­ ë°ì´í„°": result["match"],
                            "ì „ì²´ ë°ì´í„°": str(result["data"])[:100] + "..." if len(str(result["data"])) > 100 else str(result["data"])
                        })
                
                df = pd.DataFrame(df_data)
                
                return {
                    "type": "excel_search",
                    "title": f"ğŸ” '{query}' ê²€ìƒ‰ ê²°ê³¼ (fallback)",
                    "data": df,
                    "summary": f"ì´ {len(results)}ê°œ ê²°ê³¼ ë°œê²¬",
                    "chart_type": "table"
                }
            
            return None
            
        except Exception as e:
            logger.error(f"Excel ë°ì´í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None
    
    def _vector_search_query(self, query):
        """ì™„ì „íˆ ê²°ì •ì ì¸ ë²¡í„° ê²€ìƒ‰ì„ í†µí•œ ì¿¼ë¦¬ ì²˜ë¦¬"""
        try:
            if self.vector_database is None or len(self.text_chunks) == 0:
                return None
            
            # ì¿¼ë¦¬ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜ (ê²°ì •ì )
            query_hash = hashlib.md5(query.encode('utf-8')).hexdigest()
            query_vector = self._hash_to_vector(query_hash, 64)
            query_vector = query_vector / np.linalg.norm(query_vector)
            
            # ìˆœìˆ˜ Pythonìœ¼ë¡œ ìœ ì‚¬ë„ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
            similarities = []
            vectors = self.vector_database["vectors"]
            
            for i, vector in enumerate(vectors):
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))
                similarities.append((similarity, i))
            
            # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬ (ìƒìœ„ 5ê°œ)
            similarities.sort(reverse=True)
            k = min(5, len(self.text_chunks))
            
            # ê²€ìƒ‰ ê²°ê³¼ ì •ë¦¬
            search_results = []
            for i, (similarity, idx) in enumerate(similarities[:k]):
                if idx < len(self.text_chunks):
                    chunk = self.text_chunks[idx]
                    search_results.append({
                        "rank": i + 1,
                        "similarity": float(similarity),
                        "content": chunk["content"],
                        "type": chunk["type"],
                        "sheet_name": chunk.get("sheet_name", "N/A"),
                        "metadata": chunk.get("metadata", {})
                    })
            
            if search_results:
                # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
                df_data = []
                for result in search_results:
                    df_data.append({
                        "ìˆœìœ„": result["rank"],
                        "ìœ ì‚¬ë„": f"{result['similarity']:.3f}",
                        "ì‹œíŠ¸": result["sheet_name"],
                        "ìœ í˜•": result["type"],
                        "ë‚´ìš©": result["content"][:100] + "..." if len(result["content"]) > 100 else result["content"]
                    })
                
                df = pd.DataFrame(df_data)
                
                return {
                    "type": "vector_search",
                    "title": f"ğŸ” AI ë²¡í„° ê²€ìƒ‰ ê²°ê³¼: '{query}'",
                    "data": df,
                    "summary": f"AI ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ {len(search_results)}ê°œ ê²°ê³¼ ë°œê²¬ (ìœ ì‚¬ë„ ê¸°ë°˜)",
                    "chart_type": "table",
                    "raw_results": search_results
                }
            
            return None
            
        except Exception as e:
            logger.error(f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None
    
    def get_excel_file_info(self):
        """Excel íŒŒì¼ ì •ë³´ ë°˜í™˜"""
        try:
            file_info = []
            for file_name, file_data in self.processed_data.items():
                if file_data.get("type") == "excel_file":
                    # íŒŒì‹±ëœ ë°ì´í„° ì •ë³´
                    parsed_info = file_data.get("parsed_data", {})
                    sheets_info = parsed_info.get("sheets", {})
                    
                    info = {
                        "íŒŒì¼ëª…": file_name,
                        "ì‹œíŠ¸ ìˆ˜": len(sheets_info),
                        "í…ìŠ¤íŠ¸ ì²­í¬ ìˆ˜": file_data.get("chunks_count", 0),
                        "ë²¡í„° DB í¬ê¸°": file_data.get("vector_db_size", 0),
                        "íŒŒì¼ í¬ê¸°": f"{file_data.get('file_info', {}).get('size', 0) / 1024:.1f} KB",
                        "ì—…ë¡œë“œ ì‹œê°„": str(file_data.get('file_info', {}).get('uploaded', 'N/A'))
                    }
                    file_info.append(info)
            
            if file_info:
                df = pd.DataFrame(file_info)
                return {
                    "type": "file_info",
                    "title": "ğŸ“ Excel íŒŒì¼ ì •ë³´ (ë²¡í„° DB í¬í•¨)",
                    "data": df,
                    "summary": f"ì´ {len(file_info)}ê°œ Excel íŒŒì¼, ë²¡í„° ê²€ìƒ‰ ê°€ëŠ¥",
                    "chart_type": "table"
                }
            else:
                return {
                    "type": "no_files",
                    "title": "ğŸ“ Excel íŒŒì¼ ì—†ìŒ",
                    "content": "ì²˜ë¦¬ëœ Excel íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
                }
                
        except Exception as e:
            logger.error(f"íŒŒì¼ ì •ë³´ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "type": "error",
                "title": "âŒ ì˜¤ë¥˜",
                "content": f"íŒŒì¼ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }
    
    def get_image_data(self, query):
        """ê°„ë‹¨í•˜ê³  íš¨ê³¼ì ì¸ ì´ë¯¸ì§€ ë§¤ì¹­ ì‹œìŠ¤í…œ"""
        query_lower = query.lower()
        
        # Excelì—ì„œ ì¶”ì¶œëœ ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ì•ˆë‚´
        if not self.extracted_images:
            return {
                "type": "no_image",
                "title": "ğŸ–¼ï¸ ì´ë¯¸ì§€ ì—†ìŒ",
                "content": "Excel íŒŒì¼ì—ì„œ ì¶”ì¶œëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € Excel íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  'ğŸ“¤ ì´ë¯¸ì§€ ì¶”ì¶œ' ë²„íŠ¼ì„ í´ë¦­í•´ì£¼ì„¸ìš”.",
                "available_images": [],
                "suggestions": ["Excel íŒŒì¼ ì—…ë¡œë“œ", "ğŸ“¤ ì´ë¯¸ì§€ ì¶”ì¶œ ë²„íŠ¼ í´ë¦­"]
            }
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ì‹œìŠ¤í…œ
        matched_images = []
        
        logger.info(f"ì´ë¯¸ì§€ ë§¤ì¹­ ì‹œì‘: ì§ˆë¬¸='{query}', ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë¯¸ì§€={list(self.extracted_images.keys())}")
        
        # ì¡°ë¦½ë„ ê´€ë ¨ ì§ˆë¬¸ (ê°€ì¥ êµ¬ì²´ì ì¸ í‚¤ì›Œë“œë¶€í„° ì²´í¬)
        if any(word in query_lower for word in ["ì¡°ë¦½ë„", "ì¡°ë¦½ê³µì •", "ì¡°ë¦½ì‘ì—…", "ì¡°ë¦½ê³¼ì •"]):
            logger.info("ì¡°ë¦½ë„ ê´€ë ¨ ì§ˆë¬¸ ê°ì§€")
            # ì¡°ë¦½ ê´€ë ¨ ì´ë¯¸ì§€ ì°¾ê¸° (image1~30 ìš°ì„ )
            for img_name, img in self.extracted_images.items():
                if "image" in img_name.lower():
                    try:
                        img_num = int(''.join(filter(str.isdigit, img_name)))
                        if img_num <= 30:  # ì¡°ë¦½ë„ëŠ” ë³´í†µ ì•ìª½ ì´ë¯¸ì§€
                            matched_images.append((img_name, img, f"ì¡°ë¦½ ê´€ë ¨ ì´ë¯¸ì§€ (ë²ˆí˜¸: {img_num})", 10))
                            logger.info(f"ì¡°ë¦½ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 10)")
                        else:
                            matched_images.append((img_name, img, f"ì¡°ë¦½ ê´€ë ¨ ì´ë¯¸ì§€ (ë²ˆí˜¸: {img_num})", 5))
                            logger.info(f"ì¡°ë¦½ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 5)")
                    except:
                        matched_images.append((img_name, img, "ì¡°ë¦½ ê´€ë ¨ ì´ë¯¸ì§€", 3))
                        logger.info(f"ì¡°ë¦½ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 3)")
        
        # ê²€ì‚¬ ê´€ë ¨ ì§ˆë¬¸ (í’ˆì§ˆ ê²€ì‚¬ ìš°ì„ )
        elif any(word in query_lower for word in ["ê²€ì‚¬", "í’ˆì§ˆ", "í…ŒìŠ¤íŠ¸", "í™•ì¸", "ê²€ìˆ˜"]):
            logger.info("ê²€ì‚¬ ê´€ë ¨ ì§ˆë¬¸ ê°ì§€")
            for img_name, img in self.extracted_images.items():
                if "image" in img_name.lower():
                    try:
                        img_num = int(''.join(filter(str.isdigit, img_name)))
                        if 20 <= img_num <= 50:  # ê²€ì‚¬ ê´€ë ¨ì€ ì¤‘ê°„ ì´ë¯¸ì§€
                            matched_images.append((img_name, img, f"ê²€ì‚¬ ê´€ë ¨ ì´ë¯¸ì§€ (ë²ˆí˜¸: {img_num})", 10))
                            logger.info(f"ê²€ì‚¬ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 10)")
                        else:
                            matched_images.append((img_name, img, f"ê²€ì‚¬ ê´€ë ¨ ì´ë¯¸ì§€ (ë²ˆí˜¸: {img_num})", 5))
                            logger.info(f"ê²€ì‚¬ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 5)")
                    except:
                        matched_images.append((img_name, img, "ê²€ì‚¬ ê´€ë ¨ ì´ë¯¸ì§€", 3))
                        logger.info(f"ê²€ì‚¬ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 3)")
        
        # ë¶€í’ˆ/ë„ë©´ ê´€ë ¨ ì§ˆë¬¸
        elif any(word in query_lower for word in ["ë¶€í’ˆ", "ë„ë©´", "ì„¤ê³„", "ì¹˜ìˆ˜", "BOM"]):
            logger.info("ë¶€í’ˆ/ë„ë©´ ê´€ë ¨ ì§ˆë¬¸ ê°ì§€")
            for img_name, img in self.extracted_images.items():
                if "image" in img_name.lower():
                    try:
                        img_num = int(''.join(filter(str.isdigit, img_name)))
                        if img_num <= 25:  # ë¶€í’ˆë„ë©´ì€ ì•ìª½ ì´ë¯¸ì§€
                            matched_images.append((img_name, img, f"ë¶€í’ˆ/ë„ë©´ ì´ë¯¸ì§€ (ë²ˆí˜¸: {img_num})", 10))
                            logger.info(f"ë¶€í’ˆ/ë„ë©´ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 10)")
                        else:
                            matched_images.append((img_name, img, f"ë¶€í’ˆ/ë„ë©´ ì´ë¯¸ì§€ (ë²ˆí˜¸: {img_num})", 5))
                            logger.info(f"ë¶€í’ˆ/ë„ë©´ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 5)")
                    except:
                        matched_images.append((img_name, img, "ë¶€í’ˆ/ë„ë©´ ì´ë¯¸ì§€", 3))
                        logger.info(f"ë¶€í’ˆ/ë„ë©´ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 3)")
        
        # ì œí’ˆ ê´€ë ¨ ì§ˆë¬¸
        elif any(word in query_lower for word in ["ì œí’ˆ", "ì•ˆì°©", "ìƒì„¸", "í´ë¡œì¦ˆì—…", "ì™„ì„±"]):
            logger.info("ì œí’ˆ ê´€ë ¨ ì§ˆë¬¸ ê°ì§€")
            # ì œí’ˆ ê´€ë ¨ ì´ë¯¸ì§€ ì°¾ê¸° (image40+ ìš°ì„ )
            for img_name, img in self.extracted_images.items():
                if "image" in img_name.lower():
                    try:
                        img_num = int(''.join(filter(str.isdigit, img_name)))
                        if img_num >= 40:  # ì œí’ˆ ê´€ë ¨ì€ ë’¤ìª½ ì´ë¯¸ì§€
                            matched_images.append((img_name, img, f"ì œí’ˆ ê´€ë ¨ ì´ë¯¸ì§€ (ë²ˆí˜¸: {img_num})", 10))
                            logger.info(f"ì œí’ˆ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 10)")
                        else:
                            matched_images.append((img_name, img, f"ì œí’ˆ ê´€ë ¨ ì´ë¯¸ì§€ (ë²ˆí˜¸: {img_num})", 5))
                            logger.info(f"ì œí’ˆ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 5)")
                    except:
                        matched_images.append((img_name, img, "ì œí’ˆ ê´€ë ¨ ì´ë¯¸ì§€", 3))
                        logger.info(f"ì œí’ˆ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 3)")
        
        # ì¼ë°˜ì ì¸ ì¡°ë¦½ ê´€ë ¨ ì§ˆë¬¸ (ë§ˆì§€ë§‰ì— ì²´í¬)
        elif any(word in query_lower for word in ["ì¡°ë¦½", "ê³µì •", "ì‘ì—…", "ê³¼ì •"]):
            logger.info("ì¼ë°˜ ì¡°ë¦½ ê´€ë ¨ ì§ˆë¬¸ ê°ì§€")
            # ì¡°ë¦½ ê´€ë ¨ ì´ë¯¸ì§€ ì°¾ê¸° (image1~30 ìš°ì„ )
            for img_name, img in self.extracted_images.items():
                if "image" in img_name.lower():
                    try:
                        img_num = int(''.join(filter(str.isdigit, img_name)))
                        if img_num <= 30:  # ì¡°ë¦½ë„ëŠ” ë³´í†µ ì•ìª½ ì´ë¯¸ì§€
                            matched_images.append((img_name, img, f"ì¡°ë¦½ ê´€ë ¨ ì´ë¯¸ì§€ (ë²ˆí˜¸: {img_num})", 10))
                            logger.info(f"ì¡°ë¦½ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 10)")
                        else:
                            matched_images.append((img_name, img, f"ì¡°ë¦½ ê´€ë ¨ ì´ë¯¸ì§€ (ë²ˆí˜¸: {img_num})", 5))
                            logger.info(f"ì¡°ë¦½ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 5)")
                    except:
                        matched_images.append((img_name, img, "ì¡°ë¦½ ê´€ë ¨ ì´ë¯¸ì§€", 3))
                        logger.info(f"ì¡°ë¦½ ì´ë¯¸ì§€ ë§¤ì¹­: {img_name} (ì ìˆ˜: 3)")
        

        
        # ì¼ë°˜ì ì¸ ì´ë¯¸ì§€ ìš”ì²­
        else:
            # ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ì ìˆ˜ì™€ í•¨ê»˜ ì¶”ê°€
            for img_name, img in self.extracted_images.items():
                matched_images.append((img_name, img, f"ì´ë¯¸ì§€: {img_name}", 1))
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        matched_images.sort(key=lambda x: x[3], reverse=True)
        
        logger.info(f"ì´ë¯¸ì§€ ë§¤ì¹­ ì™„ë£Œ: ì´ {len(matched_images)}ê°œ ë§¤ì¹­, ìƒìœ„ 3ê°œ: {[(name, score) for name, img, desc, score in matched_images[:3]]}")
        
        # ìµœê³  ì ìˆ˜ ì´ë¯¸ì§€ ë°˜í™˜
        if matched_images:
            best_img_name, best_img, best_desc, best_score = matched_images[0]
            
            logger.info(f"ìµœì  ì´ë¯¸ì§€ ì„ íƒ: {best_img_name} (ì ìˆ˜: {best_score})")
            
            # VLM ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì¶”ê°€ ì •ë³´ ì œê³µ
            vlm_analysis = None
            if hasattr(self, 'image_analysis') and best_img_name in self.image_analysis:
                vlm_analysis = self.image_analysis[best_img_name]
            
            result = {
                "type": "image",
                "title": f"ğŸ–¼ï¸ {best_img_name} - {query}",
                "image": best_img,
                "description": best_desc,
                "all_images": [(name, img, desc) for name, img, desc, score in matched_images[:3]],  # ìƒìœ„ 3ê°œ
                "query_info": f"ì§ˆë¬¸: '{query}'ì— ëŒ€í•œ ìµœì  ë§¤ì¹­ ì´ë¯¸ì§€ (ì ìˆ˜: {best_score})",
                "total_matches": len(matched_images)
            }
            
            # VLM ë¶„ì„ ê²°ê³¼ ì¶”ê°€
            if vlm_analysis and 'error' not in vlm_analysis:
                result["vlm_analysis"] = {
                    "summary": vlm_analysis['summary'],
                    "type": vlm_analysis['type'],
                    "tags": vlm_analysis['tags'],
                    "confidence": vlm_analysis['confidence'],
                    "details": vlm_analysis['details']
                }
            
            return result
        
        # ë§¤ì¹­ë˜ëŠ” ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ëª¨ë“  ì´ë¯¸ì§€ ëª©ë¡ í‘œì‹œ
        return {
            "type": "image_list",
            "title": "ğŸ–¼ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë¯¸ì§€ë“¤",
            "content": f"ì§ˆë¬¸ '{query}'ì— ë§ëŠ” ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒ ì´ë¯¸ì§€ë“¤ì´ ìˆìŠµë‹ˆë‹¤:",
            "available_images": list(self.extracted_images.keys()),
            "all_images": [(name, img, f"ì´ë¯¸ì§€: {name}") for name, img in self.extracted_images.items()],
            "suggestions": [
                "ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”",
                "ì˜ˆ: 'ì¡°ë¦½ ê³µì •ë„ë¥¼ ë³´ì—¬ì¤˜'",
                "ì˜ˆ: 'ì œí’ˆ ì•ˆì°© ì´ë¯¸ì§€ë¥¼ ë³´ì—¬ì¤˜'",
                "ì˜ˆ: 'í’ˆì§ˆ ê²€ì‚¬ ê³¼ì •ì„ ë³´ì—¬ì¤˜'"
            ]
        }
    
    def _analyze_images_with_vlm(self):
        """VLMì„ ì‚¬ìš©í•˜ì—¬ ì¶”ì¶œëœ ì´ë¯¸ì§€ë“¤ì„ ë¶„ì„"""
        try:
            logger.info("VLM ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘")
            
            # ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ ì €ì¥
            self.image_analysis = {}
            
            for img_name, img in self.extracted_images.items():
                try:
                    # VLM ë¶„ì„ ìˆ˜í–‰
                    analysis_result = self._analyze_single_image_with_vlm(img_name, img)
                    self.image_analysis[img_name] = analysis_result
                    
                    logger.info(f"ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ: {img_name} - {analysis_result['summary']}")
                    
                except Exception as e:
                    logger.error(f"ì´ë¯¸ì§€ {img_name} VLM ë¶„ì„ ì‹¤íŒ¨: {e}")
                    self.image_analysis[img_name] = {
                        "error": str(e),
                        "summary": "ë¶„ì„ ì‹¤íŒ¨",
                        "details": [],
                        "tags": []
                    }
            
            logger.info(f"VLM ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ: {len(self.image_analysis)}ê°œ ì´ë¯¸ì§€")
            
        except Exception as e:
            logger.error(f"VLM ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _analyze_single_image_with_vlm(self, img_name, img):
        """ë‹¨ì¼ ì´ë¯¸ì§€ë¥¼ VLMìœ¼ë¡œ ë¶„ì„"""
        try:
            # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ë¶„ì„
            img_info = {
                "name": img_name,
                "size": img.size,
                "mode": img.mode,
                "format": getattr(img, 'format', 'Unknown')
            }
            
            # ì‹¤ì œ LLMì„ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ë¶„ì„ ì‹œë„
            analysis_result = self._analyze_with_real_llm(img_name, img, img_info)
            
            # LLM ë¶„ì„ì´ ì‹¤íŒ¨í•˜ë©´ ì‹œë®¬ë ˆì´ì…˜ ì‚¬ìš©
            if not analysis_result or "error" in analysis_result:
                logger.warning(f"LLM ë¶„ì„ ì‹¤íŒ¨, ì‹œë®¬ë ˆì´ì…˜ ì‚¬ìš©: {img_name}")
                analysis_result = self._simulate_vlm_analysis(img_name, img_info)
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"ë‹¨ì¼ ì´ë¯¸ì§€ VLM ë¶„ì„ ì‹¤íŒ¨ {img_name}: {e}")
            raise
    
    def _simulate_vlm_analysis(self, img_name, img_info):
        """VLM ë¶„ì„ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ VLM ëª¨ë¸ë¡œ ëŒ€ì²´ ê°€ëŠ¥)"""
        try:
            # ì´ë¯¸ì§€ ì´ë¦„ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤ë§ˆíŠ¸ ë¶„ì„
            img_name_lower = img_name.lower()
            
            # ì´ë¯¸ì§€ ë²ˆí˜¸ ì¶”ì¶œ
            img_num = None
            if "image" in img_name_lower:
                try:
                    img_num = int(''.join(filter(str.isdigit, img_name)))
                except:
                    pass
            
            # ì´ë¯¸ì§€ ìœ í˜• ë¶„ë¥˜ ë° ë¶„ì„
            if img_num is not None:
                if img_num <= 30:
                    # ì¡°ë¦½/ê³µì • ê´€ë ¨ ì´ë¯¸ì§€
                    analysis = {
                        "type": "assembly_process",
                        "summary": f"ì¡°ë¦½ ê³µì • ì´ë¯¸ì§€ (ë²ˆí˜¸: {img_num})",
                        "details": [
                            f"ì´ë¯¸ì§€ {img_num}ì€ ì¡°ë¦½ ê³µì •ì˜ {img_num}ë²ˆì§¸ ë‹¨ê³„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤",
                            "ì‘ì—…ìê°€ ë¶€í’ˆì„ ì¡°ë¦½í•˜ëŠ” ê³¼ì •ì´ë‚˜ ê³µì • ë‹¨ê³„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤",
                            "ì¡°ë¦½ ì‘ì—…ì˜ í‘œì¤€í™”ëœ ì ˆì°¨ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤"
                        ],
                        "tags": ["ì¡°ë¦½", "ê³µì •", "ì‘ì—…", "ë‹¨ê³„", f"image{img_num}"],
                        "confidence": 0.85
                    }
                elif 31 <= img_num <= 50:
                    # ê²€ì‚¬/í’ˆì§ˆ ê´€ë ¨ ì´ë¯¸ì§€
                    analysis = {
                        "type": "quality_inspection",
                        "summary": f"í’ˆì§ˆ ê²€ì‚¬ ì´ë¯¸ì§€ (ë²ˆí˜¸: {img_num})",
                        "details": [
                            f"ì´ë¯¸ì§€ {img_num}ì€ í’ˆì§ˆ ê²€ì‚¬ ê³¼ì •ì„ ë³´ì—¬ì¤ë‹ˆë‹¤",
                            "ì œí’ˆì˜ í’ˆì§ˆì„ í™•ì¸í•˜ê³  ê²€ì¦í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤",
                            "ê²€ì‚¬ ê¸°ì¤€ê³¼ ë°©ë²•ì„ ì‹œê°ì ìœ¼ë¡œ ì œì‹œí•©ë‹ˆë‹¤"
                        ],
                        "tags": ["ê²€ì‚¬", "í’ˆì§ˆ", "í…ŒìŠ¤íŠ¸", "í™•ì¸", f"image{img_num}"],
                        "confidence": 0.80
                    }
                else:
                    # ì œí’ˆ/ì™„ì„± ê´€ë ¨ ì´ë¯¸ì§€
                    analysis = {
                        "type": "product_final",
                        "summary": f"ì œí’ˆ ì™„ì„± ì´ë¯¸ì§€ (ë²ˆí˜¸: {img_num})",
                        "details": [
                            f"ì´ë¯¸ì§€ {img_num}ì€ ì™„ì„±ëœ ì œí’ˆì´ë‚˜ ìµœì¢… ìƒíƒœë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤",
                            "ì œí’ˆì˜ ìµœì¢… í˜•íƒœë‚˜ ì•ˆì°© ìƒíƒœë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤",
                            "ì¶œí•˜ ì „ ìµœì¢… ì ê²€ ê²°ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤"
                        ],
                        "tags": ["ì œí’ˆ", "ì™„ì„±", "ì•ˆì°©", "ìµœì¢…", f"image{img_num}"],
                        "confidence": 0.75
                    }
            else:
                # ì¼ë°˜ ì´ë¯¸ì§€
                analysis = {
                    "type": "general_image",
                    "summary": f"ì¼ë°˜ ì´ë¯¸ì§€: {img_name}",
                    "details": [
                        f"ì´ë¯¸ì§€ {img_name}ì€ ë¬¸ì„œì— í¬í•¨ëœ ì¼ë°˜ì ì¸ ì´ë¯¸ì§€ì…ë‹ˆë‹¤",
                        "êµ¬ì²´ì ì¸ ë‚´ìš©ì€ ì´ë¯¸ì§€ ìì²´ë¥¼ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤"
                    ],
                    "tags": ["ì´ë¯¸ì§€", "ì¼ë°˜", img_name],
                    "confidence": 0.60
                }
            
            # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            analysis["metadata"] = img_info
            analysis["analysis_method"] = "VLM_Simulation"
            
            return analysis
            
        except Exception as e:
            logger.error(f"VLM ë¶„ì„ ì‹œë®¬ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
            return {
                "type": "error",
                "summary": "ë¶„ì„ ì‹¤íŒ¨",
                "details": [f"ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"],
                "tags": ["ì˜¤ë¥˜", "ë¶„ì„ì‹¤íŒ¨"],
                "confidence": 0.0
            }
    
    def _analyze_with_real_llm(self, img_name, img, img_info):
        """ì‹¤ì œ LLMì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë¶„ì„"""
        try:
            # ì´ë¯¸ì§€ ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = f"""
            ì´ ì´ë¯¸ì§€({img_name})ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”. 
            
            ë‹¤ìŒ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”:
            1. ì´ë¯¸ì§€ê°€ ë³´ì—¬ì£¼ëŠ” ë‚´ìš© (ì¡°ë¦½ ê³µì •, í’ˆì§ˆ ê²€ì‚¬, ì œí’ˆ ë“±)
            2. ì´ë¯¸ì§€ì˜ ëª©ì ê³¼ ìš©ë„
            3. ì‘ì—…ìê°€ ì•Œì•„ì•¼ í•  í•µì‹¬ ì •ë³´
            4. ê´€ë ¨ëœ í‚¤ì›Œë“œë‚˜ íƒœê·¸
            
            í•œêµ­ì–´ë¡œ ìƒì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
            """
            
            # GPT OSS 120Bë¡œ ì´ë¯¸ì§€ ë¶„ì„ ì‹œë„
            gpt_result = self.llm_integration.analyze_image_with_gpt_oss(img, prompt)
            
            if gpt_result.get("success"):
                # GPT OSS ë¶„ì„ ê²°ê³¼ íŒŒì‹±
                analysis = self._parse_llm_analysis_result(gpt_result["analysis"], img_name, img_info)
                analysis["llm_model"] = "GPT OSS 120B"
                analysis["analysis_method"] = "Real_LLM"
                return analysis
            
            # GPT OSS ì‹¤íŒ¨ ì‹œ Qwen3 ì‹œë„
            qwen_result = self.llm_integration.analyze_image_with_qwen3(img, prompt)
            
            if qwen_result.get("success"):
                # Qwen3 ë¶„ì„ ê²°ê³¼ íŒŒì‹±
                analysis = self._parse_llm_analysis_result(qwen_result["analysis"], img_name, img_info)
                analysis["llm_model"] = "Qwen3"
                analysis["analysis_method"] = "Real_LLM"
                return analysis
            
            # ëª¨ë“  LLM ë¶„ì„ ì‹¤íŒ¨
            logger.warning(f"ëª¨ë“  LLM ë¶„ì„ ì‹¤íŒ¨: {img_name}")
            return None
            
        except Exception as e:
            logger.error(f"ì‹¤ì œ LLM ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨ {img_name}: {e}")
            return None
    
    def _parse_llm_analysis_result(self, llm_text, img_name, img_info):
        """LLM ë¶„ì„ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ íŒŒì‹±"""
        try:
            # ì´ë¯¸ì§€ ë²ˆí˜¸ ì¶”ì¶œ
            img_num = None
            if "image" in img_name.lower():
                try:
                    img_num = int(''.join(filter(str.isdigit, img_name)))
                except:
                    pass
            
            # LLM í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            text_lower = llm_text.lower()
            
            # ì´ë¯¸ì§€ ìœ í˜• ë¶„ë¥˜
            if any(word in text_lower for word in ["ì¡°ë¦½", "ê³µì •", "ì‘ì—…", "ë‹¨ê³„", "ê³¼ì •"]):
                img_type = "assembly_process"
                confidence = 0.90
            elif any(word in text_lower for word in ["ê²€ì‚¬", "í’ˆì§ˆ", "í…ŒìŠ¤íŠ¸", "í™•ì¸"]):
                img_type = "quality_inspection"
                confidence = 0.90
            elif any(word in text_lower for word in ["ì œí’ˆ", "ì™„ì„±", "ì•ˆì°©", "ìµœì¢…"]):
                img_type = "product_final"
                confidence = 0.90
            else:
                img_type = "general_image"
                confidence = 0.75
            
            # íƒœê·¸ ìƒì„±
            tags = []
            if img_num:
                tags.append(f"image{img_num}")
            
            # LLM í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œí•˜ì—¬ íƒœê·¸ ì¶”ê°€
            keywords = ["ì¡°ë¦½", "ê³µì •", "ì‘ì—…", "ê²€ì‚¬", "í’ˆì§ˆ", "í…ŒìŠ¤íŠ¸", "ì œí’ˆ", "ì™„ì„±", "ë¶€í’ˆ", "ë„ë©´"]
            for keyword in keywords:
                if keyword in text_lower:
                    tags.append(keyword)
            
            # ìƒì„¸ ë¶„ì„ì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
            details = []
            sentences = llm_text.split('.')
            for sentence in sentences:
                sentence = sentence.strip()
                if sentence and len(sentence) > 10:
                    details.append(sentence)
            
            # ìµœëŒ€ 5ê°œ ìƒì„¸ ì„¤ëª…ìœ¼ë¡œ ì œí•œ
            details = details[:5]
            
            analysis = {
                "type": img_type,
                "summary": f"LLM ë¶„ì„: {img_name}",
                "details": details,
                "tags": tags,
                "confidence": confidence,
                "metadata": img_info,
                "llm_raw_text": llm_text
            }
            
            return analysis
            
        except Exception as e:
            logger.error(f"LLM ë¶„ì„ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    def generate_auto_questions(self, excel_file_path):
        """Excel íŒŒì¼ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ìë™ìœ¼ë¡œ ì§ˆë¬¸ ìƒì„±"""
        try:
            # Excel íŒŒì¼ ì½ê¸°
            df = pd.read_excel(excel_file_path, sheet_name=None)
            
            questions = []
            
            # í‚¤ì›Œë“œ ì •ì˜
            process_keywords = {'ì¡°ë¦½', 'ê³µì •', 'ì‘ì—…', 'ê³¼ì •', 'ë‹¨ê³„', 'ìˆœì„œ', 'ì ˆì°¨', 'ë°©ë²•', 'ê¸°ìˆ '}
            quality_keywords = {'ê²€ì‚¬', 'í’ˆì§ˆ', 'í…ŒìŠ¤íŠ¸', 'í™•ì¸', 'ê²€ìˆ˜', 'ì ê²€', 'ì¸¡ì •', 'ê¸°ì¤€'}
            product_keywords = {'ì œí’ˆ', 'ì™„ì„±', 'ì¶œí•˜', 'í¬ì¥', 'ì•ˆì°©', 'ìƒì„¸', 'í´ë¡œì¦ˆì—…'}
            material_keywords = {'ë¶€í’ˆ', 'ìì¬', 'ì†Œì¬', 'ì¬ë£Œ', 'BOM', 'ë„ë©´', 'ì„¤ê³„', 'ì¹˜ìˆ˜'}
            equipment_keywords = {'ì¥ë¹„', 'ê¸°ê³„', 'ë„êµ¬', 'ì§€ê·¸', 'í˜„ë¯¸ê²½', 'ë Œì¦ˆ', 'ì¥ì¹˜'}
            
            # ì‹œíŠ¸ë³„ í‚¤ì›Œë“œ ë¶„ì„
            process_found = False
            quality_found = False
            material_found = False
            product_found = False
            equipment_found = False
            
            for sheet_name, sheet_df in df.items():
                if len(sheet_df) > 0:
                    # ì²˜ìŒ 5í–‰ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰
                    for idx, row in sheet_df.head(5).iterrows():
                        row_text = ' '.join([str(val) for val in row.values if pd.notna(val)]).lower()
                        
                        if not process_found and any(kw in row_text for kw in process_keywords):
                            questions.extend([
                                "ì¡°ë¦½ ê³µì •ë„ë¥¼ ë³´ì—¬ì¤˜",
                                "ì‘ì—… ê³¼ì •ì„ ì„¤ëª…í•´ì¤˜",
                                "ì¡°ë¦½ ë‹¨ê³„ë³„ ì´ë¯¸ì§€ë¥¼ ë³´ì—¬ì¤˜"
                            ])
                            process_found = True
                        
                        if not quality_found and any(kw in row_text for kw in quality_keywords):
                            questions.extend([
                                "í’ˆì§ˆ ê²€ì‚¬ ê³¼ì •ì„ ë³´ì—¬ì¤˜",
                                "ê²€ì‚¬ ê¸°ì¤€ì„ ì•Œë ¤ì¤˜",
                                "í…ŒìŠ¤íŠ¸ ë°©ë²•ì„ ë³´ì—¬ì¤˜"
                            ])
                            quality_found = True
                        
                        if not material_found and any(kw in row_text for kw in material_keywords):
                            questions.extend([
                                "ë¶€í’ˆ ë„ë©´ì„ ë³´ì—¬ì¤˜",
                                "BOM ì •ë³´ë¥¼ ì•Œë ¤ì¤˜",
                                "ìì¬ ëª…ì„¸ë¥¼ ë³´ì—¬ì¤˜"
                            ])
                            material_found = True
                        
                        if not product_found and any(kw in row_text for kw in product_keywords):
                            questions.extend([
                                "ì™„ì„±ëœ ì œí’ˆì„ ë³´ì—¬ì¤˜",
                                "ì œí’ˆ ì•ˆì°© ì´ë¯¸ì§€ë¥¼ ë³´ì—¬ì¤˜",
                                "ì¶œí•˜ ìƒíƒœë¥¼ ë³´ì—¬ì¤˜"
                            ])
                            product_found = True
                        
                        if not equipment_found and any(kw in row_text for kw in equipment_keywords):
                            questions.extend([
                                "ì‚¬ìš© ì¥ë¹„ë¥¼ ë³´ì—¬ì¤˜",
                                "ì‘ì—… ë„êµ¬ë¥¼ ì•Œë ¤ì¤˜",
                                "ì¸¡ì • ì¥ë¹„ë¥¼ ë³´ì—¬ì¤˜"
                            ])
                            equipment_found = True
                        
                        if len(questions) >= 12:  # ìµœëŒ€ 12ê°œ ì§ˆë¬¸
                            break
                    if len(questions) >= 12:
                        break
            
            # ì¼ë°˜ì ì¸ ì§ˆë¬¸ ì¶”ê°€
            questions.extend([
                "íŒŒì¼ ì •ë³´ë¥¼ ì•Œë ¤ì¤˜",
                "ì‹œíŠ¸ êµ¬ì¡°ë¥¼ ì„¤ëª…í•´ì¤˜",
                "ë°ì´í„° ìš”ì•½ì„ ë³´ì—¬ì¤˜"
            ])
            
            logger.info(f"ìë™ ì§ˆë¬¸ ìƒì„± ì™„ë£Œ: {len(questions)}ê°œ")
            return questions[:15]  # ìµœëŒ€ 15ê°œë¡œ ì œí•œ
            
        except Exception as e:
            logger.error(f"ìë™ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return [
                "íŒŒì¼ ì •ë³´ë¥¼ ì•Œë ¤ì¤˜",
                "ì´ë¯¸ì§€ë¥¼ ë³´ì—¬ì¤˜",
                "ë°ì´í„°ë¥¼ ìš”ì•½í•´ì¤˜"
            ]
    
    def get_general_response(self, query):
        """ì¼ë°˜ ì‘ë‹µ"""
        return {
            "type": "general",
            "title": "ğŸ’¡ ì¼ë°˜ ì •ë³´",
            "content": f"'{query}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.",
            "suggestions": [
                "ì¡°ë¦½ ê³µì •ì€ ì–´ë–¤ ê²ƒë“¤ì´ ìˆë‚˜ìš”?",
                "ì œí’ˆ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
                "ERP ì‹œìŠ¤í…œ ê¸°ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "í’ˆì§ˆ ê²€ì‚¬ ê¸°ì¤€ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "ì¡°ë¦½ ê³µì •ë„ ì´ë¯¸ì§€ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”"
            ]
        }

def main():
    st.title("ğŸ“Š Test Excels VLM System - Cloud")
    st.markdown("---")
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("ğŸ”§ ì‹œìŠ¤í…œ ì„¤ì •")
        
        # LLM ëª¨ë¸ ì„ íƒ
        st.subheader("ğŸ¤– LLM ëª¨ë¸ ì„ íƒ")
        available_models = st.session_state.system.llm_integration.get_available_models()
        
        if "selected_llm_model" not in st.session_state:
            st.session_state.selected_llm_model = available_models[0] if available_models else "ì‹œë®¬ë ˆì´ì…˜"
        
        # ëª¨ë¸ ì˜µì…˜ êµ¬ì„±
        model_options = []
        if "GPT OSS 120B" in available_models:
            model_options.append("GPT OSS 120B")
        if "Qwen3" in available_models:
            model_options.append("Qwen3")
        model_options.append("ì‹œë®¬ë ˆì´ì…˜")
        
        selected_model = st.selectbox(
            "ë¶„ì„ì— ì‚¬ìš©í•  LLM ëª¨ë¸",
            options=model_options,
            index=model_options.index(st.session_state.selected_llm_model) if st.session_state.selected_llm_model in model_options else len(model_options) - 1,
            help="ì´ë¯¸ì§€ ë¶„ì„ì— ì‚¬ìš©í•  LLM ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”"
        )
        
        if selected_model != st.session_state.selected_llm_model:
            st.session_state.selected_llm_model = selected_model
            st.rerun()
        
        # LLM ìƒíƒœ í‘œì‹œ
        if selected_model == "GPT OSS 120B":
            if not OPENAI_AVAILABLE:
                st.error("âŒ OpenAI íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒ¨í‚¤ì§€ ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            elif st.session_state.system.llm_integration.gpt_oss_client:
                st.success("âœ… GPT OSS 120B ëª¨ë¸ í™œì„±í™” (API ì—°ê²°ë¨)")
            else:
                st.error("âŒ GPT OSS 120B ëª¨ë¸ ë¹„í™œì„±í™” (API ì—°ê²° ì‹¤íŒ¨)")
        elif selected_model == "Qwen3":
            st.warning("âš ï¸ Qwen3 ëª¨ë¸ (API ì—°ê²° í…ŒìŠ¤íŠ¸ í•„ìš”)")
        else:
            st.info("â„¹ï¸ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ (ì‹¤ì œ LLM ì‚¬ìš© ì•ˆí•¨)")
        
        # íŒ¨í‚¤ì§€ ìƒíƒœ í‘œì‹œ
        if not OPENAI_AVAILABLE:
            st.error("âŒ OpenAI íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. requirements.txtë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        else:
            st.success("âœ… OpenAI íŒ¨í‚¤ì§€ ì‚¬ìš© ê°€ëŠ¥")
        
        # API í‚¤ ìƒíƒœ í‘œì‹œ
        if GPT_OSS_API_KEY.startswith("sk-or-v1-"):
            st.warning("âš ï¸ API í‚¤ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. OpenAI API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        elif GPT_OSS_API_KEY:
            st.success("âœ… API í‚¤ ì„¤ì •ë¨")
        else:
            st.error("âŒ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if st.button("ğŸ”„ ì‹œìŠ¤í…œ ì¬ì´ˆê¸°í™”", type="primary"):
            st.session_state.system = CloudVLMSystem()
            st.rerun()
        
        st.header("ğŸ“ Excel íŒŒì¼ ì—…ë¡œë“œ")
        st.write("Excel íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        uploaded_file = st.file_uploader(
            "Excel íŒŒì¼ ì„ íƒ (.xlsx)",
            type=['xlsx'],
            help="ì´ë¯¸ì§€ê°€ í¬í•¨ëœ Excel íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”"
        )
        
        if uploaded_file is not None:
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button("ğŸ“¤ ì´ë¯¸ì§€ ì¶”ì¶œ", type="primary"):
                    with st.spinner("Excel íŒŒì¼ì—ì„œ ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                        extracted_count = st.session_state.system.extract_images_from_uploaded_file(uploaded_file)
                        if extracted_count > 0:
                            st.success(f"âœ… {extracted_count}ê°œ ì´ë¯¸ì§€ ì¶”ì¶œ ì™„ë£Œ!")
                        else:
                            st.warning("âš ï¸ Excel íŒŒì¼ì—ì„œ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            st.info("ğŸ’¡ ì´ë¯¸ì§€ê°€ í¬í•¨ëœ Excel íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
                        st.rerun()
            
            with col2:
                if st.button("ğŸ“Š ë°ì´í„° íŒŒì‹±", type="secondary"):
                    with st.spinner("Excel íŒŒì¼ì„ íŒŒì‹±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                        success = st.session_state.system.process_uploaded_excel_data(uploaded_file)
                        if success:
                            st.success("âœ… Excel ë°ì´í„° íŒŒì‹± ì™„ë£Œ!")
                        else:
                            st.warning("âš ï¸ ë°ì´í„° íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                        st.rerun()
        
        st.header("ğŸ“Š Excel íŒŒì¼ ì •ë³´")
        
        if st.button("ğŸ“ íŒŒì¼ ì •ë³´ ë³´ê¸°", key="btn_file_info"):
            st.session_state.query = "Excel íŒŒì¼ ì •ë³´ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”"
            st.rerun()
        
        st.header("ğŸ“ ì˜ˆì‹œ ì§ˆë¬¸ë“¤")
        
        # ìë™ ìƒì„±ëœ ì§ˆë¬¸ë“¤ í‘œì‹œ
        if uploaded_file is not None and hasattr(st.session_state.system, 'auto_questions'):
            st.subheader("ğŸ¤– AI ìë™ ìƒì„± ì§ˆë¬¸")
            for i, question in enumerate(st.session_state.system.auto_questions[:8], 1):  # ìƒìœ„ 8ê°œë§Œ í‘œì‹œ
                if st.button(f"{i}. {question}", key=f"btn_auto_{i}"):
                    st.session_state.query = question
                    st.rerun()
            
            if len(st.session_state.system.auto_questions) > 8:
                with st.expander(f"ë” ë§ì€ ì§ˆë¬¸ ë³´ê¸° ({len(st.session_state.system.auto_questions)}ê°œ)"):
                    for i, question in enumerate(st.session_state.system.auto_questions[8:], 9):
                        if st.button(f"{i}. {question}", key=f"btn_auto_{i}"):
                            st.session_state.query = question
                            st.rerun()
        else:
            # ê¸°ë³¸ ì˜ˆì‹œ ì§ˆë¬¸ë“¤
            example_questions = [
                "Excel íŒŒì¼ ì •ë³´ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”",
                "BOM ì •ë³´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
                "ì œí’ˆ ìƒì‚°ì— í•„ìš”í•œ ìì¬ëŠ”?",
                "ì¡°ë¦½ ê³µì •ë„ ì´ë¯¸ì§€ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”",
                "í’ˆì§ˆê²€ì‚¬ ê¸°ì¤€ì€ ë¬´ì—‡ì¸ê°€ìš”?"
            ]
            
            for question in example_questions:
                if st.button(question, key=f"btn_{question}"):
                    st.session_state.query = question
                    st.rerun()
    
    # ë©”ì¸ ì»¨í…ì¸ 
    if 'system' not in st.session_state:
        st.session_state.system = CloudVLMSystem()
    
    if 'query' not in st.session_state:
        st.session_state.query = ""
    
    # í˜„ì¬ ì¶”ì¶œëœ ì´ë¯¸ì§€ ì •ë³´ í‘œì‹œ
    if st.session_state.system.extracted_images:
        st.info(f"ğŸ“¸ í˜„ì¬ {len(st.session_state.system.extracted_images)}ê°œ ì´ë¯¸ì§€ê°€ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        
        # VLM ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ í‘œì‹œ
        if hasattr(st.session_state.system, 'image_analysis') and st.session_state.system.image_analysis:
            st.success("ğŸ¤– VLM ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ!")
            with st.expander("ğŸ” VLM ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼"):
                for img_name, analysis in st.session_state.system.image_analysis.items():
                    if 'error' not in analysis:
                        st.markdown(f"**{img_name}**")
                        st.write(f"ğŸ“ **ìš”ì•½**: {analysis['summary']}")
                        st.write(f"ğŸ·ï¸ **íƒœê·¸**: {', '.join(analysis['tags'])}")
                        st.write(f"ğŸ“Š **ì‹ ë¢°ë„**: {analysis['confidence']:.2f}")
                        
                        # LLM ëª¨ë¸ ì •ë³´ í‘œì‹œ
                        if "llm_model" in analysis:
                            st.write(f"ğŸ¤– **LLM ëª¨ë¸**: {analysis['llm_model']}")
                            st.write(f"ğŸ”§ **ë¶„ì„ ë°©ë²•**: {analysis['analysis_method']}")
                        
                        with st.expander("ğŸ“‹ ìƒì„¸ ë¶„ì„"):
                            for detail in analysis['details']:
                                st.write(f"â€¢ {detail}")
                            
                            # LLM ì›ë³¸ í…ìŠ¤íŠ¸ í‘œì‹œ
                            if "llm_raw_text" in analysis:
                                st.write("---")
                                st.write("**ğŸ¤– LLM ì›ë³¸ ë¶„ì„:**")
                                st.write(analysis['llm_raw_text'])
                        
                        st.divider()
                    else:
                        st.error(f"âŒ {img_name}: {analysis['error']}")
        
        with st.expander("ğŸ“‹ ë¡œë“œëœ ì´ë¯¸ì§€ ëª©ë¡"):
            for img_name in st.session_state.system.extracted_images.keys():
                st.write(f"- {img_name}")
    
    # ì¿¼ë¦¬ ì…ë ¥
    query = st.text_input(
        "ğŸ” ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
        value=st.session_state.query,
        placeholder="ì˜ˆ: ì¡°ë¦½ ê³µì •ì€ ì–´ë–¤ ê²ƒë“¤ì´ ìˆë‚˜ìš”?"
    )
    
    if st.button("ğŸš€ ì§ˆë¬¸í•˜ê¸°", type="primary") or st.session_state.query:
        if query:
            st.session_state.query = query
            with st.spinner("ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                result = st.session_state.system.query_system(query)
                display_result(result)
        else:
            st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

def display_result(result):
    """ê²°ê³¼ í‘œì‹œ"""
    if result["type"] == "assembly":
        st.subheader(result["title"])
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.dataframe(result["data"], width='stretch')
        
        with col2:
            st.metric("ì´ ê³µì • ìˆ˜", result["summary"])
            st.info("SM-F741U ëª¨ë¸ì˜ ì¡°ë¦½ ê³µì • ì ˆì°¨")
    
    elif result["type"] == "product":
        st.subheader(result["title"])
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.dataframe(result["data"], width='stretch')
        
        with col2:
            st.metric("ëª¨ë¸ëª…", result["summary"])
            st.info("ì œí’ˆ ê¸°ë³¸ ì •ë³´")
    
    elif result["type"] == "erp":
        st.subheader(result["title"])
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.dataframe(result["data"], width='stretch')
        
        with col2:
            st.metric("ì‹œìŠ¤í…œ", result["summary"])
            st.info("ERP ì‹œìŠ¤í…œ ê¸°ëŠ¥")
    
    elif result["type"] == "quality":
        st.subheader(result["title"])
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.dataframe(result["data"], width='stretch')
        
        with col2:
            st.metric("í’ˆì§ˆ ê´€ë¦¬", result["summary"])
            st.info("í’ˆì§ˆ ê²€ì‚¬ ê¸°ì¤€ ë° ì ˆì°¨")
    
    elif result["type"] == "image":
        st.subheader(result["title"])
        
        # ì´ë¯¸ì§€ë¥¼ ë°”ì´íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ í‘œì‹œ
        img_byte_arr = io.BytesIO()
        result["image"].save(img_byte_arr, format='PNG')
        img_byte_arr = img_byte_arr.getvalue()
        
        st.image(img_byte_arr, caption=result["description"], width=400)
        
        # ì´ë¯¸ì§€ ì •ë³´ í‘œì‹œ
        st.info(f"ğŸ“ ì´ë¯¸ì§€ í¬ê¸°: {result['image'].size[0]} x {result['image'].size[1]} í”½ì…€")
        
        # VLM ë¶„ì„ ê²°ê³¼ í‘œì‹œ
        if "vlm_analysis" in result:
            st.success("ğŸ¤– VLM ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼")
            vlm = result["vlm_analysis"]
            st.write(f"**ğŸ“ ìš”ì•½**: {vlm['summary']}")
            st.write(f"**ğŸ·ï¸ ìœ í˜•**: {vlm['type']}")
            st.write(f"**ğŸ”– íƒœê·¸**: {', '.join(vlm['tags'])}")
            st.write(f"**ğŸ“Š ì‹ ë¢°ë„**: {vlm['confidence']:.2f}")
            
            # LLM ëª¨ë¸ ì •ë³´ í‘œì‹œ
            if "llm_model" in vlm:
                st.write(f"**ğŸ¤– LLM ëª¨ë¸**: {vlm['llm_model']}")
                st.write(f"**ğŸ”§ ë¶„ì„ ë°©ë²•**: {vlm['analysis_method']}")
            
            with st.expander("ğŸ“‹ ìƒì„¸ ë¶„ì„"):
                for detail in vlm['details']:
                    st.write(f"â€¢ {detail}")
                
                # LLM ì›ë³¸ í…ìŠ¤íŠ¸ í‘œì‹œ
                if "llm_raw_text" in vlm:
                    st.write("---")
                    st.write("**ğŸ¤– LLM ì›ë³¸ ë¶„ì„:**")
                    st.write(vlm['llm_raw_text'])
        
        # ë‹¤ë¥¸ ë§¤ì¹­ëœ ì´ë¯¸ì§€ë“¤ë„ í‘œì‹œ
        if "all_images" in result and len(result["all_images"]) > 1:
            st.write("ğŸ” ë‹¤ë¥¸ ê´€ë ¨ ì´ë¯¸ì§€ë“¤:")
            for i, (img_name, img, desc) in enumerate(result["all_images"][1:], 1):
                with st.expander(f"{i}. {img_name}"):
                    img_byte_arr = io.BytesIO()
                    img.save(img_byte_arr, format='PNG')
                    img_byte_arr = img_byte_arr.getvalue()
                    st.image(img_byte_arr, caption=desc, width=300)
    
    elif result["type"] == "image_list":
        st.subheader(result["title"])
        st.write(result["content"])
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë¯¸ì§€ ëª©ë¡ í‘œì‹œ
        if "available_images" in result:
            st.write("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë¯¸ì§€:")
            for img_name in result["available_images"]:
                st.write(f"- {img_name}")
        
        # ëª¨ë“  ì´ë¯¸ì§€ í‘œì‹œ
        if "all_images" in result:
            st.write("ğŸ–¼ï¸ ëª¨ë“  ì´ë¯¸ì§€:")
            for i, (img_name, img, desc) in enumerate(result["all_images"], 1):
                with st.expander(f"{i}. {img_name}"):
                    img_byte_arr = io.BytesIO()
                    img.save(img_byte_arr, format='PNG')
                    img_byte_arr = img_byte_arr.getvalue()
                    st.image(img_byte_arr, caption=desc, width=300)
    
    elif result["type"] == "no_image":
        st.subheader(result["title"])
        st.write(result["content"])
        
        if "available_images" in result:
            st.write("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë¯¸ì§€:")
            for img_name in result["available_images"]:
                st.write(f"- {img_name}")
    
    elif result["type"] == "excel_search":
        st.subheader(result["title"])
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.dataframe(result["data"], width='stretch')
        
        with col2:
            st.metric("ê²€ìƒ‰ ê²°ê³¼", result["summary"])
            st.info("Excel íŒŒì¼ì—ì„œ ì°¾ì€ ë°ì´í„°")
    
    elif result["type"] == "vector_search":
        st.subheader(result["title"])
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.dataframe(result["data"], width='stretch')
        
        with col2:
            st.metric("ë²¡í„° ê²€ìƒ‰ ê²°ê³¼", result["summary"])
            st.info("AI ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ì°¾ì€ ìœ ì‚¬í•œ ë‚´ìš©")
            
            # ìƒì„¸ ì •ë³´ í‘œì‹œ
            if "raw_results" in result:
                st.write("ğŸ” ìƒì„¸ ê²€ìƒ‰ ê²°ê³¼:")
                for i, raw_result in enumerate(result["raw_results"][:3], 1):
                    with st.expander(f"ê²°ê³¼ {i} (ìœ ì‚¬ë„: {raw_result['similarity']:.3f})"):
                        st.write(f"**ì‹œíŠ¸**: {raw_result['sheet_name']}")
                        st.write(f"**ìœ í˜•**: {raw_result['type']}")
                        st.write(f"**ë‚´ìš©**: {raw_result['content']}")
                        if raw_result.get("metadata"):
                            st.write(f"**ë©”íƒ€ë°ì´í„°**: {raw_result['metadata']}")
    
    elif result["type"] == "file_info":
        st.subheader(result["title"])
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.dataframe(result["data"], width='stretch')
        
        with col2:
            st.metric("íŒŒì¼ ìˆ˜", result["summary"])
            st.info("ì²˜ë¦¬ëœ Excel íŒŒì¼ ì •ë³´")
    
    elif result["type"] == "no_files":
        st.subheader(result["title"])
        st.write(result["content"])
        st.info("ğŸ“¤ Excel íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    elif result["type"] == "general":
        st.subheader(result["title"])
        st.write(result["content"])
        
        if "suggestions" in result:
            st.write("ğŸ’¡ ì¶”ì²œ ì§ˆë¬¸:")
            for suggestion in result["suggestions"]:
                st.write(f"- {suggestion}")

if __name__ == "__main__":
    main()
